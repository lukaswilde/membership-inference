{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f5f9f45-39b7-492b-ad2e-123b9b9040e6",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This project was created during the course of the \"Attacks Against Machine Learning Models\" lecture at Saarland University. The goal of this project is to attack a target model and find out, whether certain images where used as training data. This constitutes a Privacy Attack called Membership Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "772cdd96-1210-4220-b6e1-b2992dcaf442",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukas/miniconda3/envs/ml/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/lukas/miniconda3/envs/ml/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowExxb\n",
      "  Referenced from: <6E6BE615-472A-3225-994B-C7BC27D09EAE> /Users/lukas/miniconda3/envs/ml/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <41E520E5-65A7-36E9-935B-4BE4C8F20783> /Users/lukas/miniconda3/envs/ml/lib/python3.10/site-packages/torch/lib/libc10.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "random.seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "163c1680-ceab-45f4-8faa-ea7a819e9e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device mps\n",
      "Shadow dataset contains 30000 images\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'pickle/cifar10/resnet34/shadow.p'\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "with open(DATA_PATH, \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "print(f\"Shadow dataset contains {len(dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ee2551-49d5-44db-9ca3-7c63340a3757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target model resnet34 on cifar10 was trained for 200 epochs with final accuracy of 68.49%\n"
     ]
    }
   ],
   "source": [
    "model_str = \"resnet34\"\n",
    "task_dataset = \"cifar10\"\n",
    "\n",
    "MODEL_PATH = f'models/{model_str}_{task_dataset}.pth'\n",
    "\n",
    "target_model = torchvision.models.resnet34(num_classes=10).to(device)\n",
    "# Change num_classes to 200 when you use the Tiny ImageNet dataset\n",
    "\n",
    "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "target_model.load_state_dict(state_dict['net'])\n",
    "# Test accuracy\n",
    "ACC = state_dict['acc']\n",
    "# Training epoch (start from 0)\n",
    "EPOCH = state_dict['epoch']\n",
    "\n",
    "print(f\"Target model {model_str} on {task_dataset} was trained for {EPOCH + 1} epochs with final accuracy of {ACC:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002415db-4401-45e6-a3b5-a7bc048a5b08",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "As we want to replicate the target model with our own model (therefore called shadow model), we train it in a similar way to the target model. Fortunately, we have access to the number of epochs the training model was trained, so we have two possibilities moving forward:\n",
    "\n",
    "1. We can use the number of epochs obtained to train our shadow model for the same amount\n",
    "2. We can train our shadow model until it converges\n",
    "\n",
    "We will do both in the following and compare the attack performance on `eval.p`.\n",
    "\n",
    "First, we will need to split our shadow dataset, to obtain a portion of the data that will not be used for training. This test data contains only non-members of the shadow model and will later be used for training the attack model.\n",
    "\n",
    "For the first approach, we just need the train-test-split because we know the number of epochs.\n",
    "\n",
    "For the second approach, we add an additional validation-split to determine when the training converges on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2041d4b6-9e7e-4cc2-800b-92c666b95565",
   "metadata": {},
   "source": [
    "## 1. Fixed number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43eb5441-5c3a-4216-b8a4-aa175b6e9cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train-test-split with training containing 21000 samples and test containing 9000\n"
     ]
    }
   ],
   "source": [
    "shuffled_data = random.sample(dataset, len(dataset))\n",
    "split = int(0.7 * len(shuffled_data))\n",
    "train_dataset = shuffled_data[:split]\n",
    "test_dataset = shuffled_data[split:]\n",
    "\n",
    "print(f\"Created train-test-split with training containing {len(train_dataset)} samples and test containing {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af1325-82ea-4307-b5d6-999253f4fb42",
   "metadata": {},
   "source": [
    "## Data Inspection\n",
    "\n",
    "Before going to the training, let's analyse the data a bit. Especially important is the distribution of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c992a477-38ce-4a55-8eec-96fdcd456afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[tensor(0.7373), tensor(0.6980), tensor(0.63...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[tensor(0.2863), tensor(0.2000), tensor(0.13...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[tensor(0.9725), tensor(0.9608), tensor(0.96...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[tensor(0.9961), tensor(0.9804), tensor(0.98...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[tensor(0.7294), tensor(0.7255), tensor(0.72...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  label\n",
       "0  [[[tensor(0.7373), tensor(0.6980), tensor(0.63...      6\n",
       "1  [[[tensor(0.2863), tensor(0.2000), tensor(0.13...      5\n",
       "2  [[[tensor(0.9725), tensor(0.9608), tensor(0.96...      6\n",
       "3  [[[tensor(0.9961), tensor(0.9804), tensor(0.98...      3\n",
       "4  [[[tensor(0.7294), tensor(0.7255), tensor(0.72...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(train_dataset, columns=[\"image\", \"label\"])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94d10172-bcb4-4704-a510-2ea7eb6dd82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df[\"label\"].value_counts().loc[range(10)].plot(kind='bar')\n",
    "plt.title(\"Distribution of Classes in Training Dataset\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d967eb-8f02-4981-994d-988a39420fbe",
   "metadata": {},
   "source": [
    "We can see that the distribution across the classes is pretty even, so we continue with the training of the shadow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed8da7fe-29ca-42f7-bf03-fea8c747ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf13d1-61c8-45a0-98f4-68656e2ae8f3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89dc224d-7e73-40dc-b225-15313d81d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_model = torchvision.models.resnet34(num_classes=10).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(shadow_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65fdbc30-8777-40e3-9b10-375fa4b9d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for batch_idx, data in enumerate(train_dataloader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 50 == 49:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch [{}/{}] loss: {}'.format(batch_idx + 1, len(train_dataloader) + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18470af3-6151-4c32-8235-b5348b33289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(model, dataset):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "    correct = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(dataloader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            correct += (predicted == labels).float().sum()\n",
    "    model.train()\n",
    "    return 100 * correct/len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4ff453c6-ceed-422c-805f-75791711152d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "  batch [50/330] loss: 0.1068658356666565\n",
      "  batch [100/330] loss: 0.08759368586540223\n",
      "  batch [150/330] loss: 0.08197230792045593\n",
      "  batch [200/330] loss: 0.07861152398586273\n",
      "  batch [250/330] loss: 0.07530254852771759\n",
      "  batch [300/330] loss: 0.07363667953014373\n",
      "=> epoch 0, training loss: 0.07363667953014373, training accuracy: 48.11428451538086\n",
      "  batch [50/330] loss: 0.06933662116527557\n",
      "  batch [100/330] loss: 0.06879744338989258\n",
      "  batch [150/330] loss: 0.06835883331298828\n",
      "  batch [200/330] loss: 0.06506826114654542\n",
      "  batch [250/330] loss: 0.0622162309885025\n",
      "  batch [300/330] loss: 0.06394648241996766\n",
      "=> epoch 1, training loss: 0.06394648241996766, training accuracy: 49.652381896972656\n",
      "  batch [50/330] loss: 0.05879727125167847\n",
      "  batch [100/330] loss: 0.06049016612768173\n",
      "  batch [150/330] loss: 0.056367367327213286\n",
      "  batch [200/330] loss: 0.05578539550304413\n",
      "  batch [250/330] loss: 0.05530743956565857\n",
      "  batch [300/330] loss: 0.05693060773611069\n",
      "=> epoch 2, training loss: 0.05693060773611069, training accuracy: 48.53809356689453\n",
      "  batch [50/330] loss: 0.05138816624879837\n",
      "  batch [100/330] loss: 0.0490004261136055\n",
      "  batch [150/330] loss: 0.04934092497825623\n",
      "  batch [200/330] loss: 0.050534088313579556\n",
      "  batch [250/330] loss: 0.04946613323688507\n",
      "  batch [300/330] loss: 0.04618237471580505\n",
      "=> epoch 3, training loss: 0.04618237471580505, training accuracy: 61.82381057739258\n",
      "  batch [50/330] loss: 0.06359453839063645\n",
      "  batch [100/330] loss: 0.052779612123966216\n",
      "  batch [150/330] loss: 0.0478846914768219\n",
      "  batch [200/330] loss: 0.04389829248189926\n",
      "  batch [250/330] loss: 0.044836941242218015\n",
      "  batch [300/330] loss: 0.04477424049377442\n",
      "=> epoch 4, training loss: 0.04477424049377442, training accuracy: 68.83809661865234\n",
      "  batch [50/330] loss: 0.04396295535564423\n",
      "  batch [100/330] loss: 0.039438320338726046\n",
      "  batch [150/330] loss: 0.041137365341186526\n",
      "  batch [200/330] loss: 0.04040877431631088\n",
      "  batch [250/330] loss: 0.040551473140716554\n",
      "  batch [300/330] loss: 0.03989031171798706\n",
      "=> epoch 5, training loss: 0.03989031171798706, training accuracy: 66.67142486572266\n",
      "  batch [50/330] loss: 0.037703517764806745\n",
      "  batch [100/330] loss: 0.03453251785039902\n",
      "  batch [150/330] loss: 0.03576798474788666\n",
      "  batch [200/330] loss: 0.032742463201284405\n",
      "  batch [250/330] loss: 0.03465905743837357\n",
      "  batch [300/330] loss: 0.03629852735996247\n",
      "=> epoch 6, training loss: 0.03629852735996247, training accuracy: 70.86190795898438\n",
      "  batch [50/330] loss: 0.033709623157978055\n",
      "  batch [100/330] loss: 0.03230057549476623\n",
      "  batch [150/330] loss: 0.030574890553951264\n",
      "  batch [200/330] loss: 0.03191977715492249\n",
      "  batch [250/330] loss: 0.032054860562086104\n",
      "  batch [300/330] loss: 0.02870264646410942\n",
      "=> epoch 7, training loss: 0.02870264646410942, training accuracy: 76.19999694824219\n",
      "  batch [50/330] loss: 0.028597040832042696\n",
      "  batch [100/330] loss: 0.02552327737212181\n",
      "  batch [150/330] loss: 0.026907452642917634\n",
      "  batch [200/330] loss: 0.028003647685050963\n",
      "  batch [250/330] loss: 0.02831116518378258\n",
      "  batch [300/330] loss: 0.024923843324184417\n",
      "=> epoch 8, training loss: 0.024923843324184417, training accuracy: 83.88571166992188\n",
      "  batch [50/330] loss: 0.02108789899945259\n",
      "  batch [100/330] loss: 0.019965058743953705\n",
      "  batch [150/330] loss: 0.022355159878730775\n",
      "  batch [200/330] loss: 0.020340451419353485\n",
      "  batch [250/330] loss: 0.02063937497138977\n",
      "  batch [300/330] loss: 0.024233102440834046\n",
      "=> epoch 9, training loss: 0.024233102440834046, training accuracy: 86.12857055664062\n",
      "  batch [50/330] loss: 0.018888762071728706\n",
      "  batch [100/330] loss: 0.016688043117523192\n",
      "  batch [150/330] loss: 0.01847303380072117\n",
      "  batch [200/330] loss: 0.01866377992928028\n",
      "  batch [250/330] loss: 0.0206982903778553\n",
      "  batch [300/330] loss: 0.020351059705018996\n",
      "=> epoch 10, training loss: 0.020351059705018996, training accuracy: 79.61904907226562\n",
      "  batch [50/330] loss: 0.012509813264012336\n",
      "  batch [100/330] loss: 0.013110561482608318\n",
      "  batch [150/330] loss: 0.014071465909481048\n",
      "  batch [200/330] loss: 0.015619287326931953\n",
      "  batch [250/330] loss: 0.01518468114733696\n",
      "  batch [300/330] loss: 0.015569860875606537\n",
      "=> epoch 11, training loss: 0.015569860875606537, training accuracy: 86.79523468017578\n",
      "  batch [50/330] loss: 0.011734618663787841\n",
      "  batch [100/330] loss: 0.011966076612472534\n",
      "  batch [150/330] loss: 0.015589226588606835\n",
      "  batch [200/330] loss: 0.014250730976462364\n",
      "  batch [250/330] loss: 0.012454441480338573\n",
      "  batch [300/330] loss: 0.013527733728289604\n",
      "=> epoch 12, training loss: 0.013527733728289604, training accuracy: 93.13333129882812\n",
      "  batch [50/330] loss: 0.007306047270074487\n",
      "  batch [100/330] loss: 0.010297546237707138\n",
      "  batch [150/330] loss: 0.011166468907147646\n",
      "  batch [200/330] loss: 0.011597271248698235\n",
      "  batch [250/330] loss: 0.011669527687132359\n",
      "  batch [300/330] loss: 0.013534554176032543\n",
      "=> epoch 13, training loss: 0.013534554176032543, training accuracy: 93.57618713378906\n",
      "  batch [50/330] loss: 0.009484473705291749\n",
      "  batch [100/330] loss: 0.008032210510224104\n",
      "  batch [150/330] loss: 0.008829221427440644\n",
      "  batch [200/330] loss: 0.008019113548099994\n",
      "  batch [250/330] loss: 0.010035360980778932\n",
      "  batch [300/330] loss: 0.010876055728644132\n",
      "=> epoch 14, training loss: 0.010876055728644132, training accuracy: 88.0142822265625\n",
      "  batch [50/330] loss: 0.0094937851652503\n",
      "  batch [100/330] loss: 0.022126705810427666\n",
      "  batch [150/330] loss: 0.02012892486155033\n",
      "  batch [200/330] loss: 0.013679249197244643\n",
      "  batch [250/330] loss: 0.011479354977607726\n",
      "  batch [300/330] loss: 0.008637605175375939\n",
      "=> epoch 15, training loss: 0.008637605175375939, training accuracy: 94.88095092773438\n",
      "  batch [50/330] loss: 0.01347693607956171\n",
      "  batch [100/330] loss: 0.007905754096806049\n",
      "  batch [150/330] loss: 0.0070462141297757625\n",
      "  batch [200/330] loss: 0.00746669302508235\n",
      "  batch [250/330] loss: 0.0062078734338283535\n",
      "  batch [300/330] loss: 0.010229815863072872\n",
      "=> epoch 16, training loss: 0.010229815863072872, training accuracy: 95.66666412353516\n",
      "  batch [50/330] loss: 0.006584717556834221\n",
      "  batch [100/330] loss: 0.005743696466088295\n",
      "  batch [150/330] loss: 0.004614121423102915\n",
      "  batch [200/330] loss: 0.005607094099745155\n",
      "  batch [250/330] loss: 0.006464300986379385\n",
      "  batch [300/330] loss: 0.00681824279949069\n",
      "=> epoch 17, training loss: 0.00681824279949069, training accuracy: 97.14286041259766\n",
      "  batch [50/330] loss: 0.005607432117685675\n",
      "  batch [100/330] loss: 0.004899291796144098\n",
      "  batch [150/330] loss: 0.005035794982686639\n",
      "  batch [200/330] loss: 0.006779435534030199\n",
      "  batch [250/330] loss: 0.008022978778928519\n",
      "  batch [300/330] loss: 0.006551807675510645\n",
      "=> epoch 18, training loss: 0.006551807675510645, training accuracy: 94.48094940185547\n",
      "  batch [50/330] loss: 0.003546012019738555\n",
      "  batch [100/330] loss: 0.004808508172631264\n",
      "  batch [150/330] loss: 0.003829120517708361\n",
      "  batch [200/330] loss: 0.00476404383033514\n",
      "  batch [250/330] loss: 0.004477831498719752\n",
      "  batch [300/330] loss: 0.011134718112647534\n",
      "=> epoch 19, training loss: 0.011134718112647534, training accuracy: 89.21428680419922\n",
      "  batch [50/330] loss: 0.010491737201809882\n",
      "  batch [100/330] loss: 0.006014281179755926\n",
      "  batch [150/330] loss: 0.006258222753182054\n",
      "  batch [200/330] loss: 0.006492763213813305\n",
      "  batch [250/330] loss: 0.004571698807179928\n",
      "  batch [300/330] loss: 0.006835734952241182\n",
      "=> epoch 20, training loss: 0.006835734952241182, training accuracy: 91.21904754638672\n",
      "  batch [50/330] loss: 0.015039225876331328\n",
      "  batch [100/330] loss: 0.008769127402454615\n",
      "  batch [150/330] loss: 0.005189692234620452\n",
      "  batch [200/330] loss: 0.005317723784595728\n",
      "  batch [250/330] loss: 0.0068278321661055085\n",
      "  batch [300/330] loss: 0.005365192249417305\n",
      "=> epoch 21, training loss: 0.005365192249417305, training accuracy: 96.509521484375\n",
      "  batch [50/330] loss: 0.006069052617996931\n",
      "  batch [100/330] loss: 0.0032887900797650216\n",
      "  batch [150/330] loss: 0.0029532416965812446\n",
      "  batch [200/330] loss: 0.0034454249106347563\n",
      "  batch [250/330] loss: 0.004265726431272924\n",
      "  batch [300/330] loss: 0.00446393215842545\n",
      "=> epoch 22, training loss: 0.00446393215842545, training accuracy: 94.5142822265625\n",
      "  batch [50/330] loss: 0.002829887719359249\n",
      "  batch [100/330] loss: 0.0021835880475118757\n",
      "  batch [150/330] loss: 0.002965758586768061\n",
      "  batch [200/330] loss: 0.0030933451633900406\n",
      "  batch [250/330] loss: 0.003950256848707795\n",
      "  batch [300/330] loss: 0.003111949955113232\n",
      "=> epoch 23, training loss: 0.003111949955113232, training accuracy: 95.64286041259766\n",
      "  batch [50/330] loss: 0.003655091875232756\n",
      "  batch [100/330] loss: 0.0033922966215759517\n",
      "  batch [150/330] loss: 0.003720174180343747\n",
      "  batch [200/330] loss: 0.0036162828528322278\n",
      "  batch [250/330] loss: 0.003821400315500796\n",
      "  batch [300/330] loss: 0.0050577730583027\n",
      "=> epoch 24, training loss: 0.0050577730583027, training accuracy: 94.78571319580078\n",
      "  batch [50/330] loss: 0.00945577908307314\n",
      "  batch [100/330] loss: 0.007027962097898126\n",
      "  batch [150/330] loss: 0.009507956862449646\n",
      "  batch [200/330] loss: 0.011686986215412616\n",
      "  batch [250/330] loss: 0.015014622509479523\n",
      "  batch [300/330] loss: 0.0071828761100769046\n",
      "=> epoch 25, training loss: 0.0071828761100769046, training accuracy: 71.93809509277344\n",
      "  batch [50/330] loss: 0.0682521307617426\n",
      "  batch [100/330] loss: 0.04178256124258042\n",
      "  batch [150/330] loss: 0.02818274062871933\n",
      "  batch [200/330] loss: 0.02136816483736038\n",
      "  batch [250/330] loss: 0.016729588344693184\n",
      "  batch [300/330] loss: 0.012652731209993362\n",
      "=> epoch 26, training loss: 0.012652731209993362, training accuracy: 94.24285888671875\n",
      "  batch [50/330] loss: 0.006918908715248108\n",
      "  batch [100/330] loss: 0.0049151411857455965\n",
      "  batch [150/330] loss: 0.005052358612418175\n",
      "  batch [200/330] loss: 0.00524057499691844\n",
      "  batch [250/330] loss: 0.005560717405751347\n",
      "  batch [300/330] loss: 0.004926783565431833\n",
      "=> epoch 27, training loss: 0.004926783565431833, training accuracy: 98.24285888671875\n",
      "  batch [50/330] loss: 0.004033873639069497\n",
      "  batch [100/330] loss: 0.002233353864401579\n",
      "  batch [150/330] loss: 0.0020569491509813817\n",
      "  batch [200/330] loss: 0.002408091572346166\n",
      "  batch [250/330] loss: 0.0028872651825658977\n",
      "  batch [300/330] loss: 0.0031082605081610383\n",
      "=> epoch 28, training loss: 0.0031082605081610383, training accuracy: 98.47142791748047\n",
      "  batch [50/330] loss: 0.002507049816194922\n",
      "  batch [100/330] loss: 0.0023441079584881662\n",
      "  batch [150/330] loss: 0.0025985207261983306\n",
      "  batch [200/330] loss: 0.0018856782610528172\n",
      "  batch [250/330] loss: 0.0020597255425527693\n",
      "  batch [300/330] loss: 0.0031319829113781454\n",
      "=> epoch 29, training loss: 0.0031319829113781454, training accuracy: 97.28571319580078\n",
      "  batch [50/330] loss: 0.00284370387904346\n",
      "  batch [100/330] loss: 0.0022909946490544827\n",
      "  batch [150/330] loss: 0.0029841051178518684\n",
      "  batch [200/330] loss: 0.0029107087501324715\n",
      "  batch [250/330] loss: 0.0020342694432474674\n",
      "  batch [300/330] loss: 0.002119807165348902\n",
      "=> epoch 30, training loss: 0.002119807165348902, training accuracy: 99.33809661865234\n",
      "  batch [50/330] loss: 0.006572709208354354\n",
      "  batch [100/330] loss: 0.00373558396846056\n",
      "  batch [150/330] loss: 0.004968596022576093\n",
      "  batch [200/330] loss: 0.0036069871354848146\n",
      "  batch [250/330] loss: 0.0030547293424606323\n",
      "  batch [300/330] loss: 0.0031065269401296973\n",
      "=> epoch 31, training loss: 0.0031065269401296973, training accuracy: 98.70952606201172\n",
      "  batch [50/330] loss: 0.004465340764261782\n",
      "  batch [100/330] loss: 0.002421722520608455\n",
      "  batch [150/330] loss: 0.002378082883078605\n",
      "  batch [200/330] loss: 0.002749831959139556\n",
      "  batch [250/330] loss: 0.00233195785805583\n",
      "  batch [300/330] loss: 0.002367912301328033\n",
      "=> epoch 32, training loss: 0.002367912301328033, training accuracy: 97.92857360839844\n",
      "  batch [50/330] loss: 0.007751646351069212\n",
      "  batch [100/330] loss: 0.0043624722277745605\n",
      "  batch [150/330] loss: 0.003178980654105544\n",
      "  batch [200/330] loss: 0.002081324426457286\n",
      "  batch [250/330] loss: 0.0024163020998239516\n",
      "  batch [300/330] loss: 0.002585888607427478\n",
      "=> epoch 33, training loss: 0.002585888607427478, training accuracy: 97.76190185546875\n",
      "  batch [50/330] loss: 0.004226043497212231\n",
      "  batch [100/330] loss: 0.004752209606580436\n",
      "  batch [150/330] loss: 0.003394491285085678\n",
      "  batch [200/330] loss: 0.0025788262858986856\n",
      "  batch [250/330] loss: 0.0018298223677556962\n",
      "  batch [300/330] loss: 0.0026387329921126364\n",
      "=> epoch 34, training loss: 0.0026387329921126364, training accuracy: 96.42381286621094\n",
      "  batch [50/330] loss: 0.0017424547316040844\n",
      "  batch [100/330] loss: 0.0015834452679846435\n",
      "  batch [150/330] loss: 0.0017678858216386287\n",
      "  batch [200/330] loss: 0.001812621854711324\n",
      "  batch [250/330] loss: 0.00282151795970276\n",
      "  batch [300/330] loss: 0.0023535399900283666\n",
      "=> epoch 35, training loss: 0.0023535399900283666, training accuracy: 96.85237884521484\n",
      "  batch [50/330] loss: 0.0026416849615052344\n",
      "  batch [100/330] loss: 0.0020767738056601957\n",
      "  batch [150/330] loss: 0.0011607038556830957\n",
      "  batch [200/330] loss: 0.00198103828006424\n",
      "  batch [250/330] loss: 0.0018441424327902496\n",
      "  batch [300/330] loss: 0.0020533403144218027\n",
      "=> epoch 36, training loss: 0.0020533403144218027, training accuracy: 98.88095092773438\n",
      "  batch [50/330] loss: 0.004625035649165511\n",
      "  batch [100/330] loss: 0.0028994836402125657\n",
      "  batch [150/330] loss: 0.0027573818538803607\n",
      "  batch [200/330] loss: 0.003893324631266296\n",
      "  batch [250/330] loss: 0.003834719629958272\n",
      "  batch [300/330] loss: 0.004176603294909\n",
      "=> epoch 37, training loss: 0.004176603294909, training accuracy: 88.70952606201172\n",
      "  batch [50/330] loss: 0.005210862895473838\n",
      "  batch [100/330] loss: 0.0026464329631999137\n",
      "  batch [150/330] loss: 0.001780247096437961\n",
      "  batch [200/330] loss: 0.0020522647676989437\n",
      "  batch [250/330] loss: 0.002392956501338631\n",
      "  batch [300/330] loss: 0.002438034411869012\n",
      "=> epoch 38, training loss: 0.002438034411869012, training accuracy: 97.18095397949219\n",
      "  batch [50/330] loss: 0.0025076040237327108\n",
      "  batch [100/330] loss: 0.0016974081587977708\n",
      "  batch [150/330] loss: 0.0014640179226407782\n",
      "  batch [200/330] loss: 0.0014692590312333777\n",
      "  batch [250/330] loss: 0.002048501542885788\n",
      "  batch [300/330] loss: 0.0024860223605064676\n",
      "=> epoch 39, training loss: 0.0024860223605064676, training accuracy: 97.8952407836914\n",
      "  batch [50/330] loss: 0.008159696540795267\n",
      "  batch [100/330] loss: 0.004064383201301098\n",
      "  batch [150/330] loss: 0.003069250841625035\n",
      "  batch [200/330] loss: 0.0022912798966281114\n",
      "  batch [250/330] loss: 0.0026245924392715096\n",
      "  batch [300/330] loss: 0.0032316863844171167\n",
      "=> epoch 40, training loss: 0.0032316863844171167, training accuracy: 97.63333129882812\n",
      "  batch [50/330] loss: 0.001780652389745228\n",
      "  batch [100/330] loss: 0.0010631203112425284\n",
      "  batch [150/330] loss: 0.001829521250561811\n",
      "  batch [200/330] loss: 0.0022699299613013865\n",
      "  batch [250/330] loss: 0.0027810078526381405\n",
      "  batch [300/330] loss: 0.0027232181578874587\n",
      "=> epoch 41, training loss: 0.0027232181578874587, training accuracy: 98.9000015258789\n",
      "  batch [50/330] loss: 0.0015342740959022195\n",
      "  batch [100/330] loss: 0.0014949278079438954\n",
      "  batch [150/330] loss: 0.0016803474050248043\n",
      "  batch [200/330] loss: 0.0031003452339209616\n",
      "  batch [250/330] loss: 0.003135752192698419\n",
      "  batch [300/330] loss: 0.0021710391973610967\n",
      "=> epoch 42, training loss: 0.0021710391973610967, training accuracy: 98.26190185546875\n",
      "  batch [50/330] loss: 0.002399658486363478\n",
      "  batch [100/330] loss: 0.0019071154303383082\n",
      "  batch [150/330] loss: 0.003000176284229383\n",
      "  batch [200/330] loss: 0.003231816722545773\n",
      "  batch [250/330] loss: 0.0031863515274599195\n",
      "  batch [300/330] loss: 0.002830772242275998\n",
      "=> epoch 43, training loss: 0.002830772242275998, training accuracy: 98.69999694824219\n",
      "  batch [50/330] loss: 0.0021224486569408328\n",
      "  batch [100/330] loss: 0.001397406010190025\n",
      "  batch [150/330] loss: 0.0016584411058574915\n",
      "  batch [200/330] loss: 0.0013313912641024217\n",
      "  batch [250/330] loss: 0.002168934141052887\n",
      "  batch [300/330] loss: 0.0022542253094725313\n",
      "=> epoch 44, training loss: 0.0022542253094725313, training accuracy: 96.0952377319336\n",
      "  batch [50/330] loss: 0.00817171373963356\n",
      "  batch [100/330] loss: 0.005689126715064049\n",
      "  batch [150/330] loss: 0.004197895322926343\n",
      "  batch [200/330] loss: 0.0035142947239801287\n",
      "  batch [250/330] loss: 0.0024207565225660803\n",
      "  batch [300/330] loss: 0.0019436845349846408\n",
      "=> epoch 45, training loss: 0.0019436845349846408, training accuracy: 99.02381134033203\n",
      "  batch [50/330] loss: 0.004224956031423062\n",
      "  batch [100/330] loss: 0.004358329738490284\n",
      "  batch [150/330] loss: 0.0032074384950101374\n",
      "  batch [200/330] loss: 0.0023069155712146312\n",
      "  batch [250/330] loss: 0.002174739340785891\n",
      "  batch [300/330] loss: 0.002096493787597865\n",
      "=> epoch 46, training loss: 0.002096493787597865, training accuracy: 98.61904907226562\n",
      "  batch [50/330] loss: 0.003515187081415206\n",
      "  batch [100/330] loss: 0.0023634870022069664\n",
      "  batch [150/330] loss: 0.0018725011525675655\n",
      "  batch [200/330] loss: 0.0021694674679310993\n",
      "  batch [250/330] loss: 0.0020594324889825657\n",
      "  batch [300/330] loss: 0.0015234012540895493\n",
      "=> epoch 47, training loss: 0.0015234012540895493, training accuracy: 98.55238342285156\n",
      "  batch [50/330] loss: 0.004884824972134084\n",
      "  batch [100/330] loss: 0.0034685931843705473\n",
      "  batch [150/330] loss: 0.0017208130325889214\n",
      "  batch [200/330] loss: 0.002599804265424609\n",
      "  batch [250/330] loss: 0.007084481002762913\n",
      "  batch [300/330] loss: 0.007459332421422005\n",
      "=> epoch 48, training loss: 0.007459332421422005, training accuracy: 96.9047622680664\n",
      "  batch [50/330] loss: 0.005029535876587033\n",
      "  batch [100/330] loss: 0.002070495740976185\n",
      "  batch [150/330] loss: 0.001868723139166832\n",
      "  batch [200/330] loss: 0.0013550588546786457\n",
      "  batch [250/330] loss: 0.0013474612577701918\n",
      "  batch [300/330] loss: 0.0020350448851240797\n",
      "=> epoch 49, training loss: 0.0020350448851240797, training accuracy: 98.22856903076172\n",
      "  batch [50/330] loss: 0.0025380930779501797\n",
      "  batch [100/330] loss: 0.0016535458972211927\n",
      "  batch [150/330] loss: 0.0017960704010911285\n",
      "  batch [200/330] loss: 0.0015869650347158312\n",
      "  batch [250/330] loss: 0.0014400110074784607\n",
      "  batch [300/330] loss: 0.001115419679088518\n",
      "=> epoch 50, training loss: 0.001115419679088518, training accuracy: 98.20952606201172\n",
      "  batch [50/330] loss: 0.009481573726981878\n",
      "  batch [100/330] loss: 0.003300030724145472\n",
      "  batch [150/330] loss: 0.003017258538864553\n",
      "  batch [200/330] loss: 0.0019463374332990497\n",
      "  batch [250/330] loss: 0.0020708599602803587\n",
      "  batch [300/330] loss: 0.002385911616962403\n",
      "=> epoch 51, training loss: 0.002385911616962403, training accuracy: 97.64286041259766\n",
      "  batch [50/330] loss: 0.013282940881326795\n",
      "  batch [100/330] loss: 0.005986835774034261\n",
      "  batch [150/330] loss: 0.003896306292619556\n",
      "  batch [200/330] loss: 0.0022488636025227607\n",
      "  batch [250/330] loss: 0.0017566905128769577\n",
      "  batch [300/330] loss: 0.0014320025413762777\n",
      "=> epoch 52, training loss: 0.0014320025413762777, training accuracy: 98.24285888671875\n",
      "  batch [50/330] loss: 0.004014007972436957\n",
      "  batch [100/330] loss: 0.001886365389218554\n",
      "  batch [150/330] loss: 0.0020202977678272873\n",
      "  batch [200/330] loss: 0.011932221722090616\n",
      "  batch [250/330] loss: 0.009923344824463129\n",
      "  batch [300/330] loss: 0.0043586853640154005\n",
      "=> epoch 53, training loss: 0.0043586853640154005, training accuracy: 99.25714111328125\n",
      "  batch [50/330] loss: 0.0011662146360613406\n",
      "  batch [100/330] loss: 0.0009390599072212353\n",
      "  batch [150/330] loss: 0.0007216783041367307\n",
      "  batch [200/330] loss: 0.001099569218873512\n",
      "  batch [250/330] loss: 0.0009362112933886238\n",
      "  batch [300/330] loss: 0.000681293690111488\n",
      "=> epoch 54, training loss: 0.000681293690111488, training accuracy: 99.80476379394531\n",
      "  batch [50/330] loss: 0.0003614515707886312\n",
      "  batch [100/330] loss: 0.0004353806613944471\n",
      "  batch [150/330] loss: 0.0005818055578274652\n",
      "  batch [200/330] loss: 0.0005097631733224262\n",
      "  batch [250/330] loss: 0.0007781556755435304\n",
      "  batch [300/330] loss: 0.0012057157280505634\n",
      "=> epoch 55, training loss: 0.0012057157280505634, training accuracy: 99.17619323730469\n",
      "  batch [50/330] loss: 0.0012558656201581472\n",
      "  batch [100/330] loss: 0.001654505893267924\n",
      "  batch [150/330] loss: 0.001061454948503524\n",
      "  batch [200/330] loss: 0.0011550678812200204\n",
      "  batch [250/330] loss: 0.0012989773413573857\n",
      "  batch [300/330] loss: 0.0015158152042422444\n",
      "=> epoch 56, training loss: 0.0015158152042422444, training accuracy: 99.21428680419922\n",
      "  batch [50/330] loss: 0.0019468987950240263\n",
      "  batch [100/330] loss: 0.001248397598043084\n",
      "  batch [150/330] loss: 0.0011964373383671046\n",
      "  batch [200/330] loss: 0.0017574016983853654\n",
      "  batch [250/330] loss: 0.001738380187889561\n",
      "  batch [300/330] loss: 0.001364336423925124\n",
      "=> epoch 57, training loss: 0.001364336423925124, training accuracy: 99.37619018554688\n",
      "  batch [50/330] loss: 0.0007145001656317617\n",
      "  batch [100/330] loss: 0.0009458526100788731\n",
      "  batch [150/330] loss: 0.0014440536290640012\n",
      "  batch [200/330] loss: 0.001798681520158425\n",
      "  batch [250/330] loss: 0.0013265382973477245\n",
      "  batch [300/330] loss: 0.0015477468585595488\n",
      "=> epoch 58, training loss: 0.0015477468585595488, training accuracy: 99.54762268066406\n",
      "  batch [50/330] loss: 0.0017010202602250502\n",
      "  batch [100/330] loss: 0.0018621052247472108\n",
      "  batch [150/330] loss: 0.0017655035740463062\n",
      "  batch [200/330] loss: 0.0016666048200568184\n",
      "  batch [250/330] loss: 0.001500687616528012\n",
      "  batch [300/330] loss: 0.00266254425549414\n",
      "=> epoch 59, training loss: 0.00266254425549414, training accuracy: 97.11904907226562\n",
      "  batch [50/330] loss: 0.008707614608108998\n",
      "  batch [100/330] loss: 0.0037788487384095787\n",
      "  batch [150/330] loss: 0.0024877079750876873\n",
      "  batch [200/330] loss: 0.0014400940951891244\n",
      "  batch [250/330] loss: 0.0011146337273530663\n",
      "  batch [300/330] loss: 0.001485673470655456\n",
      "=> epoch 60, training loss: 0.001485673470655456, training accuracy: 99.19999694824219\n",
      "  batch [50/330] loss: 0.002993270789505914\n",
      "  batch [100/330] loss: 0.0019316122836899013\n",
      "  batch [150/330] loss: 0.0013901038595940918\n",
      "  batch [200/330] loss: 0.0011877762014046312\n",
      "  batch [250/330] loss: 0.0011322584877489134\n",
      "  batch [300/330] loss: 0.0016496671591885387\n",
      "=> epoch 61, training loss: 0.0016496671591885387, training accuracy: 99.15238189697266\n",
      "  batch [50/330] loss: 0.0009059828364406712\n",
      "  batch [100/330] loss: 0.0011136979375150986\n",
      "  batch [150/330] loss: 0.0011926120666903444\n",
      "  batch [200/330] loss: 0.0009142550169490278\n",
      "  batch [250/330] loss: 0.0013520000965800136\n",
      "  batch [300/330] loss: 0.001286052291863598\n",
      "=> epoch 62, training loss: 0.001286052291863598, training accuracy: 98.97142791748047\n",
      "  batch [50/330] loss: 0.006743125919718295\n",
      "  batch [100/330] loss: 0.0074180048797279595\n",
      "  batch [150/330] loss: 0.0031864054407924414\n",
      "  batch [200/330] loss: 0.0026742044957354665\n",
      "  batch [250/330] loss: 0.0013366830018348992\n",
      "  batch [300/330] loss: 0.0015998670966364443\n",
      "=> epoch 63, training loss: 0.0015998670966364443, training accuracy: 99.65714263916016\n",
      "  batch [50/330] loss: 0.0008821608644211665\n",
      "  batch [100/330] loss: 0.0009743389254435896\n",
      "  batch [150/330] loss: 0.0009322330408613197\n",
      "  batch [200/330] loss: 0.0008271526138996705\n",
      "  batch [250/330] loss: 0.0005074956095195375\n",
      "  batch [300/330] loss: 0.0005918498444370925\n",
      "=> epoch 64, training loss: 0.0005918498444370925, training accuracy: 99.93333435058594\n",
      "  batch [50/330] loss: 0.0003697795952757588\n",
      "  batch [100/330] loss: 0.0004026512437849306\n",
      "  batch [150/330] loss: 0.0007047556900215568\n",
      "  batch [200/330] loss: 0.0006080623189627658\n",
      "  batch [250/330] loss: 0.0007190591730322921\n",
      "  batch [300/330] loss: 0.001546727578228456\n",
      "=> epoch 65, training loss: 0.001546727578228456, training accuracy: 97.66190338134766\n",
      "  batch [50/330] loss: 0.0031595442284597084\n",
      "  batch [100/330] loss: 0.0028137128793168814\n",
      "  batch [150/330] loss: 0.001926350940950215\n",
      "  batch [200/330] loss: 0.001822140010073781\n",
      "  batch [250/330] loss: 0.0011206199614098296\n",
      "  batch [300/330] loss: 0.0018017873247154056\n",
      "=> epoch 66, training loss: 0.0018017873247154056, training accuracy: 97.93333435058594\n",
      "  batch [50/330] loss: 0.002160288042854518\n",
      "  batch [100/330] loss: 0.0010997504352708348\n",
      "  batch [150/330] loss: 0.0013993739961297251\n",
      "  batch [200/330] loss: 0.0013592736831633374\n",
      "  batch [250/330] loss: 0.0013663161190925167\n",
      "  batch [300/330] loss: 0.002229471306549385\n",
      "=> epoch 67, training loss: 0.002229471306549385, training accuracy: 99.16666412353516\n",
      "  batch [50/330] loss: 0.0026032689425628634\n",
      "  batch [100/330] loss: 0.0012616578403394669\n",
      "  batch [150/330] loss: 0.002117937953909859\n",
      "  batch [200/330] loss: 0.0053108006431721155\n",
      "  batch [250/330] loss: 0.002770206743385643\n",
      "  batch [300/330] loss: 0.005616678100195714\n",
      "=> epoch 68, training loss: 0.005616678100195714, training accuracy: 96.53333282470703\n",
      "  batch [50/330] loss: 0.0038627976402640343\n",
      "  batch [100/330] loss: 0.0013161147674545645\n",
      "  batch [150/330] loss: 0.001335071542998776\n",
      "  batch [200/330] loss: 0.0008386821238673293\n",
      "  batch [250/330] loss: 0.0009625026754802093\n",
      "  batch [300/330] loss: 0.001545577054959722\n",
      "=> epoch 69, training loss: 0.001545577054959722, training accuracy: 99.18095397949219\n",
      "  batch [50/330] loss: 0.0007675844387849792\n",
      "  batch [100/330] loss: 0.0009197872466174885\n",
      "  batch [150/330] loss: 0.0007645914071472362\n",
      "  batch [200/330] loss: 0.0008091296838829294\n",
      "  batch [250/330] loss: 0.00115747451983043\n",
      "  batch [300/330] loss: 0.0010334563897922634\n",
      "=> epoch 70, training loss: 0.0010334563897922634, training accuracy: 99.28571319580078\n",
      "  batch [50/330] loss: 0.0006252868046285584\n",
      "  batch [100/330] loss: 0.00033326808056153824\n",
      "  batch [150/330] loss: 0.0007412602718104608\n",
      "  batch [200/330] loss: 0.0006068227060895879\n",
      "  batch [250/330] loss: 0.0012175863837182988\n",
      "  batch [300/330] loss: 0.0008918355263449484\n",
      "=> epoch 71, training loss: 0.0008918355263449484, training accuracy: 99.06666564941406\n",
      "  batch [50/330] loss: 0.004864887590520084\n",
      "  batch [100/330] loss: 0.0021796593475155532\n",
      "  batch [150/330] loss: 0.0019541293112561105\n",
      "  batch [200/330] loss: 0.0012369577911449596\n",
      "  batch [250/330] loss: 0.001243347575655207\n",
      "  batch [300/330] loss: 0.0013383436902659014\n",
      "=> epoch 72, training loss: 0.0013383436902659014, training accuracy: 99.4000015258789\n",
      "  batch [50/330] loss: 0.0029710585884749887\n",
      "  batch [100/330] loss: 0.0017757929890649393\n",
      "  batch [150/330] loss: 0.001867117666406557\n",
      "  batch [200/330] loss: 0.001067609837395139\n",
      "  batch [250/330] loss: 0.0012098641432821751\n",
      "  batch [300/330] loss: 0.0010846209303708746\n",
      "=> epoch 73, training loss: 0.0010846209303708746, training accuracy: 99.31904602050781\n",
      "  batch [50/330] loss: 0.0009102993989363313\n",
      "  batch [100/330] loss: 0.0008782968408195302\n",
      "  batch [150/330] loss: 0.000740793934717658\n",
      "  batch [200/330] loss: 0.0006400395961245522\n",
      "  batch [250/330] loss: 0.0006425977494072867\n",
      "  batch [300/330] loss: 0.0008585203496622853\n",
      "=> epoch 74, training loss: 0.0008585203496622853, training accuracy: 98.78095245361328\n",
      "  batch [50/330] loss: 0.001472517447982682\n",
      "  batch [100/330] loss: 0.0014316055952222086\n",
      "  batch [150/330] loss: 0.001225676070112968\n",
      "  batch [200/330] loss: 0.0020253443817491645\n",
      "  batch [250/330] loss: 0.0026901070361491294\n",
      "  batch [300/330] loss: 0.0027094546970911322\n",
      "=> epoch 75, training loss: 0.0027094546970911322, training accuracy: 99.33333587646484\n",
      "  batch [50/330] loss: 0.001193612761911936\n",
      "  batch [100/330] loss: 0.001043388359452365\n",
      "  batch [150/330] loss: 0.0008247765076812356\n",
      "  batch [200/330] loss: 0.0012009858943638392\n",
      "  batch [250/330] loss: 0.0010162824694416486\n",
      "  batch [300/330] loss: 0.0009203393954085186\n",
      "=> epoch 76, training loss: 0.0009203393954085186, training accuracy: 99.56666564941406\n",
      "  batch [50/330] loss: 0.0008282523712841794\n",
      "  batch [100/330] loss: 0.0010159008020418697\n",
      "  batch [150/330] loss: 0.0005571675489190966\n",
      "  batch [200/330] loss: 0.001053836729901377\n",
      "  batch [250/330] loss: 0.001103022764174966\n",
      "  batch [300/330] loss: 0.001489249982056208\n",
      "=> epoch 77, training loss: 0.001489249982056208, training accuracy: 99.74761962890625\n",
      "  batch [50/330] loss: 0.0011676081859041006\n",
      "  batch [100/330] loss: 0.0011067238458781503\n",
      "  batch [150/330] loss: 0.0010486378573114053\n",
      "  batch [200/330] loss: 0.0009682465493388009\n",
      "  batch [250/330] loss: 0.0018083089931751602\n",
      "  batch [300/330] loss: 0.0008579697526874952\n",
      "=> epoch 78, training loss: 0.0008579697526874952, training accuracy: 98.14762115478516\n",
      "  batch [50/330] loss: 0.0009408959557767958\n",
      "  batch [100/330] loss: 0.0012532000144710764\n",
      "  batch [150/330] loss: 0.0016394653966417536\n",
      "  batch [200/330] loss: 0.0015544867244898341\n",
      "  batch [250/330] loss: 0.002341193795495201\n",
      "  batch [300/330] loss: 0.0028204980525188147\n",
      "=> epoch 79, training loss: 0.0028204980525188147, training accuracy: 99.29523468017578\n",
      "  batch [50/330] loss: 0.0009016939852735959\n",
      "  batch [100/330] loss: 0.0007073385971598327\n",
      "  batch [150/330] loss: 0.0007381865865900181\n",
      "  batch [200/330] loss: 0.0011418098153662868\n",
      "  batch [250/330] loss: 0.0009452899415337015\n",
      "  batch [300/330] loss: 0.0009810017383133527\n",
      "=> epoch 80, training loss: 0.0009810017383133527, training accuracy: 99.28571319580078\n",
      "  batch [50/330] loss: 0.001169810019840952\n",
      "  batch [100/330] loss: 0.0007044430321257096\n",
      "  batch [150/330] loss: 0.0006676217588828877\n",
      "  batch [200/330] loss: 0.0007649097237444948\n",
      "  batch [250/330] loss: 0.0012593801697657909\n",
      "  batch [300/330] loss: 0.0013966832245932892\n",
      "=> epoch 81, training loss: 0.0013966832245932892, training accuracy: 97.27143096923828\n",
      "  batch [50/330] loss: 0.006589070523157716\n",
      "  batch [100/330] loss: 0.003226822213502601\n",
      "  batch [150/330] loss: 0.0035773283925373107\n",
      "  batch [200/330] loss: 0.001951928341994062\n",
      "  batch [250/330] loss: 0.001860408281441778\n",
      "  batch [300/330] loss: 0.004827047587838024\n",
      "=> epoch 82, training loss: 0.004827047587838024, training accuracy: 95.86190795898438\n",
      "  batch [50/330] loss: 0.004815606890246272\n",
      "  batch [100/330] loss: 0.0022733555731829255\n",
      "  batch [150/330] loss: 0.002098707067780197\n",
      "  batch [200/330] loss: 0.001469305400038138\n",
      "  batch [250/330] loss: 0.0018742046495899558\n",
      "  batch [300/330] loss: 0.001218035715748556\n",
      "=> epoch 83, training loss: 0.001218035715748556, training accuracy: 99.80000305175781\n",
      "  batch [50/330] loss: 0.000870636148378253\n",
      "  batch [100/330] loss: 0.000576802135357866\n",
      "  batch [150/330] loss: 0.0008553171702078544\n",
      "  batch [200/330] loss: 0.0007101036258973181\n",
      "  batch [250/330] loss: 0.00048589629742491525\n",
      "  batch [300/330] loss: 0.0006274645570956636\n",
      "=> epoch 84, training loss: 0.0006274645570956636, training accuracy: 99.80000305175781\n",
      "  batch [50/330] loss: 0.0037761160280788317\n",
      "  batch [100/330] loss: 0.0011068764328374526\n",
      "  batch [150/330] loss: 0.0012261490120436065\n",
      "  batch [200/330] loss: 0.0012965318255737657\n",
      "  batch [250/330] loss: 0.0012477831557334866\n",
      "  batch [300/330] loss: 0.0011180254660430363\n",
      "=> epoch 85, training loss: 0.0011180254660430363, training accuracy: 99.12857055664062\n",
      "  batch [50/330] loss: 0.003043668414640706\n",
      "  batch [100/330] loss: 0.0017567546747159213\n",
      "  batch [150/330] loss: 0.0008987712058005855\n",
      "  batch [200/330] loss: 0.0006006864994415082\n",
      "  batch [250/330] loss: 0.0006022149133204949\n",
      "  batch [300/330] loss: 0.000760071033728309\n",
      "=> epoch 86, training loss: 0.000760071033728309, training accuracy: 99.84761810302734\n",
      "  batch [50/330] loss: 0.0002738605499325786\n",
      "  batch [100/330] loss: 0.00031466497985093155\n",
      "  batch [150/330] loss: 0.00013893820668454282\n",
      "  batch [200/330] loss: 0.0009858257339074043\n",
      "  batch [250/330] loss: 0.01176741613075137\n",
      "  batch [300/330] loss: 0.005921488264575601\n",
      "=> epoch 87, training loss: 0.005921488264575601, training accuracy: 99.0952377319336\n",
      "  batch [50/330] loss: 0.0050489202523604036\n",
      "  batch [100/330] loss: 0.0025293541974388063\n",
      "  batch [150/330] loss: 0.002202414468396455\n",
      "  batch [200/330] loss: 0.0011735104331746697\n",
      "  batch [250/330] loss: 0.0011886358760530129\n",
      "  batch [300/330] loss: 0.0014311471534892916\n",
      "=> epoch 88, training loss: 0.0014311471534892916, training accuracy: 99.9047622680664\n",
      "  batch [50/330] loss: 0.0004338165868248325\n",
      "  batch [100/330] loss: 0.0005340113732963801\n",
      "  batch [150/330] loss: 0.00023529925849288702\n",
      "  batch [200/330] loss: 0.00011022898546070791\n",
      "  batch [250/330] loss: 0.00012879073061048985\n",
      "  batch [300/330] loss: 0.00040541588394262364\n",
      "=> epoch 89, training loss: 0.00040541588394262364, training accuracy: 99.990478515625\n",
      "  batch [50/330] loss: 0.00013404805873869918\n",
      "  batch [100/330] loss: 0.00014354998756607528\n",
      "  batch [150/330] loss: 0.0002179489595000632\n",
      "  batch [200/330] loss: 0.0005518164663444623\n",
      "  batch [250/330] loss: 0.0008189538904116489\n",
      "  batch [300/330] loss: 0.0004578104800893925\n",
      "=> epoch 90, training loss: 0.0004578104800893925, training accuracy: 99.89047241210938\n",
      "  batch [50/330] loss: 0.0020502597882295958\n",
      "  batch [100/330] loss: 0.0016366137514123693\n",
      "  batch [150/330] loss: 0.0013342960504814983\n",
      "  batch [200/330] loss: 0.0012475829292670825\n",
      "  batch [250/330] loss: 0.000835430096834898\n",
      "  batch [300/330] loss: 0.0008232224179664626\n",
      "=> epoch 91, training loss: 0.0008232224179664626, training accuracy: 99.71904754638672\n",
      "  batch [50/330] loss: 0.00046568702776858116\n",
      "  batch [100/330] loss: 0.0004587686164304614\n",
      "  batch [150/330] loss: 0.0003552099204971455\n",
      "  batch [200/330] loss: 0.00041448500809201504\n",
      "  batch [250/330] loss: 0.0009420929236803203\n",
      "  batch [300/330] loss: 0.0008024190120340791\n",
      "=> epoch 92, training loss: 0.0008024190120340791, training accuracy: 99.63809204101562\n",
      "  batch [50/330] loss: 0.003753795584430918\n",
      "  batch [100/330] loss: 0.0018053173472872004\n",
      "  batch [150/330] loss: 0.0016849670362425967\n",
      "  batch [200/330] loss: 0.0014223419686313719\n",
      "  batch [250/330] loss: 0.000954925203230232\n",
      "  batch [300/330] loss: 0.001294822003808804\n",
      "=> epoch 93, training loss: 0.001294822003808804, training accuracy: 99.66666412353516\n",
      "  batch [50/330] loss: 0.0016029183660866693\n",
      "  batch [100/330] loss: 0.0014748948737978934\n",
      "  batch [150/330] loss: 0.0008495015782827977\n",
      "  batch [200/330] loss: 0.000643102818983607\n",
      "  batch [250/330] loss: 0.0007623988286359236\n",
      "  batch [300/330] loss: 0.0006183129946002737\n",
      "=> epoch 94, training loss: 0.0006183129946002737, training accuracy: 99.5142822265625\n",
      "  batch [50/330] loss: 0.003197992372326553\n",
      "  batch [100/330] loss: 0.0013153844062471763\n",
      "  batch [150/330] loss: 0.0009352261751191691\n",
      "  batch [200/330] loss: 0.0010115508248563856\n",
      "  batch [250/330] loss: 0.000702400096168276\n",
      "  batch [300/330] loss: 0.0007917690825706813\n",
      "=> epoch 95, training loss: 0.0007917690825706813, training accuracy: 99.56666564941406\n",
      "  batch [50/330] loss: 0.0006130213539581746\n",
      "  batch [100/330] loss: 0.0003270765386841958\n",
      "  batch [150/330] loss: 0.0005906991699594073\n",
      "  batch [200/330] loss: 0.0006368349225376733\n",
      "  batch [250/330] loss: 0.0004954627907063695\n",
      "  batch [300/330] loss: 0.0007717914357053815\n",
      "=> epoch 96, training loss: 0.0007717914357053815, training accuracy: 99.81904602050781\n",
      "  batch [50/330] loss: 0.00039276517734106165\n",
      "  batch [100/330] loss: 0.00045366406909306536\n",
      "  batch [150/330] loss: 0.0002587419411720475\n",
      "  batch [200/330] loss: 0.000451804396128864\n",
      "  batch [250/330] loss: 0.0005426718378075747\n",
      "  batch [300/330] loss: 0.0008930941344078747\n",
      "=> epoch 97, training loss: 0.0008930941344078747, training accuracy: 99.70476531982422\n",
      "  batch [50/330] loss: 0.007274139190092683\n",
      "  batch [100/330] loss: 0.002297937859548256\n",
      "  batch [150/330] loss: 0.006391171363182366\n",
      "  batch [200/330] loss: 0.0027359479567967354\n",
      "  batch [250/330] loss: 0.0014972092140233145\n",
      "  batch [300/330] loss: 0.001728698020800948\n",
      "=> epoch 98, training loss: 0.001728698020800948, training accuracy: 99.81428527832031\n",
      "  batch [50/330] loss: 0.0003576887761882972\n",
      "  batch [100/330] loss: 0.00042334377707447856\n",
      "  batch [150/330] loss: 0.0006926994837122038\n",
      "  batch [200/330] loss: 0.00044448120100423695\n",
      "  batch [250/330] loss: 0.000494783642119728\n",
      "  batch [300/330] loss: 0.0003731769782898482\n",
      "=> epoch 99, training loss: 0.0003731769782898482, training accuracy: 99.68095397949219\n",
      "  batch [50/330] loss: 0.002642061845166609\n",
      "  batch [100/330] loss: 0.0007919619627064094\n",
      "  batch [150/330] loss: 0.0008086924636154435\n",
      "  batch [200/330] loss: 0.0008310998554807156\n",
      "  batch [250/330] loss: 0.0008575540416059085\n",
      "  batch [300/330] loss: 0.000507152108475566\n",
      "=> epoch 100, training loss: 0.000507152108475566, training accuracy: 99.78571319580078\n",
      "  batch [50/330] loss: 0.006465865449281409\n",
      "  batch [100/330] loss: 0.0031908558355644347\n",
      "  batch [150/330] loss: 0.0011673588952980935\n",
      "  batch [200/330] loss: 0.0008320263121277093\n",
      "  batch [250/330] loss: 0.0006478171249618753\n",
      "  batch [300/330] loss: 0.00036213329317979516\n",
      "=> epoch 101, training loss: 0.00036213329317979516, training accuracy: 99.94761657714844\n",
      "  batch [50/330] loss: 0.0002470345489709871\n",
      "  batch [100/330] loss: 0.000244828076189151\n",
      "  batch [150/330] loss: 0.00012757294622133485\n",
      "  batch [200/330] loss: 0.00031061058201885315\n",
      "  batch [250/330] loss: 0.0003206173455255339\n",
      "  batch [300/330] loss: 0.00013890984592580935\n",
      "=> epoch 102, training loss: 0.00013890984592580935, training accuracy: 99.97142791748047\n",
      "  batch [50/330] loss: 0.0003009921239354298\n",
      "  batch [100/330] loss: 0.00014787728265218902\n",
      "  batch [150/330] loss: 0.0002984925592572836\n",
      "  batch [200/330] loss: 0.0008119526652517379\n",
      "  batch [250/330] loss: 0.0009845926398847952\n",
      "  batch [300/330] loss: 0.000954918145405827\n",
      "=> epoch 103, training loss: 0.000954918145405827, training accuracy: 99.05238342285156\n",
      "  batch [50/330] loss: 0.007975089858751745\n",
      "  batch [100/330] loss: 0.00353136295103468\n",
      "  batch [150/330] loss: 0.0018574168661143631\n",
      "  batch [200/330] loss: 0.0014439144738717005\n",
      "  batch [250/330] loss: 0.0012207211265922524\n",
      "  batch [300/330] loss: 0.001654876881861128\n",
      "=> epoch 104, training loss: 0.001654876881861128, training accuracy: 99.80000305175781\n",
      "  batch [50/330] loss: 0.0005818031841772609\n",
      "  batch [100/330] loss: 0.0005559910107986071\n",
      "  batch [150/330] loss: 0.00039661073428578673\n",
      "  batch [200/330] loss: 0.0002889142649946734\n",
      "  batch [250/330] loss: 0.0002527049279888161\n",
      "  batch [300/330] loss: 0.00022508697013836354\n",
      "=> epoch 105, training loss: 0.00022508697013836354, training accuracy: 99.95714569091797\n",
      "  batch [50/330] loss: 0.00024563699030841235\n",
      "  batch [100/330] loss: 0.00021702610312058823\n",
      "  batch [150/330] loss: 0.00021143343809671932\n",
      "  batch [200/330] loss: 7.570212714199443e-05\n",
      "  batch [250/330] loss: 0.0007046913985541323\n",
      "  batch [300/330] loss: 0.0007695032057308709\n",
      "=> epoch 106, training loss: 0.0007695032057308709, training accuracy: 99.72380828857422\n",
      "  batch [50/330] loss: 0.00031291695345862536\n",
      "  batch [100/330] loss: 0.00045973982669238466\n",
      "  batch [150/330] loss: 0.0005760863739560591\n",
      "  batch [200/330] loss: 0.0006042701506521553\n",
      "  batch [250/330] loss: 0.000935273354902165\n",
      "  batch [300/330] loss: 0.0009862134918221272\n",
      "=> epoch 107, training loss: 0.0009862134918221272, training accuracy: 98.21904754638672\n",
      "  batch [50/330] loss: 0.006966651436756365\n",
      "  batch [100/330] loss: 0.002811925797490403\n",
      "  batch [150/330] loss: 0.0021334794922731816\n",
      "  batch [200/330] loss: 0.0011271599375177175\n",
      "  batch [250/330] loss: 0.0011419291514903308\n",
      "  batch [300/330] loss: 0.001261545445478987\n",
      "=> epoch 108, training loss: 0.001261545445478987, training accuracy: 99.80952453613281\n",
      "  batch [50/330] loss: 0.0013340229811146856\n",
      "  batch [100/330] loss: 0.0007781569435901474\n",
      "  batch [150/330] loss: 0.0008259218584280461\n",
      "  batch [200/330] loss: 0.0006528238950413652\n",
      "  batch [250/330] loss: 0.00041420840207138097\n",
      "  batch [300/330] loss: 0.0007234643980045803\n",
      "=> epoch 109, training loss: 0.0007234643980045803, training accuracy: 99.79047393798828\n",
      "  batch [50/330] loss: 0.0014701311978860758\n",
      "  batch [100/330] loss: 0.0005910159819322871\n",
      "  batch [150/330] loss: 0.0005303345303982497\n",
      "  batch [200/330] loss: 0.0017465001873497386\n",
      "  batch [250/330] loss: 0.0014935883813595864\n",
      "  batch [300/330] loss: 0.0011073931207647547\n",
      "=> epoch 110, training loss: 0.0011073931207647547, training accuracy: 99.35237884521484\n",
      "  batch [50/330] loss: 0.02632403766736388\n",
      "  batch [100/330] loss: 0.009875307010486722\n",
      "  batch [150/330] loss: 0.005612274570390582\n",
      "  batch [200/330] loss: 0.003998267884366215\n",
      "  batch [250/330] loss: 0.003213088784366846\n",
      "  batch [300/330] loss: 0.002565135981887579\n",
      "=> epoch 111, training loss: 0.002565135981887579, training accuracy: 99.66666412353516\n",
      "  batch [50/330] loss: 0.002701636300422251\n",
      "  batch [100/330] loss: 0.0021443314899224786\n",
      "  batch [150/330] loss: 0.0016333538730395958\n",
      "  batch [200/330] loss: 0.0008706726444652304\n",
      "  batch [250/330] loss: 0.0007161125945276581\n",
      "  batch [300/330] loss: 0.0014637793654110282\n",
      "=> epoch 112, training loss: 0.0014637793654110282, training accuracy: 99.94285583496094\n",
      "  batch [50/330] loss: 0.0002491546881792601\n",
      "  batch [100/330] loss: 0.0003033795374503825\n",
      "  batch [150/330] loss: 0.0003550299529451877\n",
      "  batch [200/330] loss: 0.0001556635680317413\n",
      "  batch [250/330] loss: 0.0002324561826972058\n",
      "  batch [300/330] loss: 0.00045317981104017236\n",
      "=> epoch 113, training loss: 0.00045317981104017236, training accuracy: 99.9857177734375\n",
      "  batch [50/330] loss: 0.0001976698286671308\n",
      "  batch [100/330] loss: 0.0002135672892909497\n",
      "  batch [150/330] loss: 8.486382186674746e-05\n",
      "  batch [200/330] loss: 0.0004433460144282435\n",
      "  batch [250/330] loss: 0.00043322805899515514\n",
      "  batch [300/330] loss: 0.00010249863000353798\n",
      "=> epoch 114, training loss: 0.00010249863000353798, training accuracy: 99.98094940185547\n",
      "  batch [50/330] loss: 0.00011061422625425621\n",
      "  batch [100/330] loss: 4.0032515564234926e-05\n",
      "  batch [150/330] loss: 5.027309206343489e-05\n",
      "  batch [200/330] loss: 0.00012868980401253794\n",
      "  batch [250/330] loss: 7.803062823586515e-05\n",
      "  batch [300/330] loss: 0.00011443246445560362\n",
      "=> epoch 115, training loss: 0.00011443246445560362, training accuracy: 99.9857177734375\n",
      "  batch [50/330] loss: 8.077546895947307e-05\n",
      "  batch [100/330] loss: 7.22171757806791e-05\n",
      "  batch [150/330] loss: 0.00018055879367602756\n",
      "  batch [200/330] loss: 0.0001568229721087846\n",
      "  batch [250/330] loss: 0.0002129673921954236\n",
      "  batch [300/330] loss: 9.296953821467469e-05\n",
      "=> epoch 116, training loss: 9.296953821467469e-05, training accuracy: 100.0\n",
      "  batch [50/330] loss: 0.0001645642076182412\n",
      "  batch [100/330] loss: 6.973581216152524e-05\n",
      "  batch [150/330] loss: 5.708045705978293e-05\n",
      "  batch [200/330] loss: 0.00010835696854337584\n",
      "  batch [250/330] loss: 0.0001287435195154103\n",
      "  batch [300/330] loss: 0.0002162419129126647\n",
      "=> epoch 117, training loss: 0.0002162419129126647, training accuracy: 99.85713958740234\n",
      "  batch [50/330] loss: 0.002070131636703081\n",
      "  batch [100/330] loss: 0.0018268054135260172\n",
      "  batch [150/330] loss: 0.0011195447803474964\n",
      "  batch [200/330] loss: 0.000914149060336058\n",
      "  batch [250/330] loss: 0.0010290356582554523\n",
      "  batch [300/330] loss: 0.001318322720879223\n",
      "=> epoch 118, training loss: 0.001318322720879223, training accuracy: 98.84761810302734\n",
      "  batch [50/330] loss: 0.004011035418952816\n",
      "  batch [100/330] loss: 0.0014933508191024884\n",
      "  batch [150/330] loss: 0.0011325014849426225\n",
      "  batch [200/330] loss: 0.0010189944439189275\n",
      "  batch [250/330] loss: 0.0008418567729968345\n",
      "  batch [300/330] loss: 0.0008488261077436619\n",
      "=> epoch 119, training loss: 0.0008488261077436619, training accuracy: 99.71428680419922\n",
      "  batch [50/330] loss: 0.0005668954608845524\n",
      "  batch [100/330] loss: 0.0002646273735444993\n",
      "  batch [150/330] loss: 0.0005674990504630841\n",
      "  batch [200/330] loss: 0.00029027017028420234\n",
      "  batch [250/330] loss: 0.0005831932142245932\n",
      "  batch [300/330] loss: 0.0005943978232462541\n",
      "=> epoch 120, training loss: 0.0005943978232462541, training accuracy: 99.82857513427734\n",
      "  batch [50/330] loss: 0.0002865092034844565\n",
      "  batch [100/330] loss: 0.00019573914563807192\n",
      "  batch [150/330] loss: 0.00017114626043621684\n",
      "  batch [200/330] loss: 6.167943612672389e-05\n",
      "  batch [250/330] loss: 7.394234113235143e-05\n",
      "  batch [300/330] loss: 0.00018087515246952536\n",
      "=> epoch 121, training loss: 0.00018087515246952536, training accuracy: 99.95714569091797\n",
      "  batch [50/330] loss: 9.965779744379688e-05\n",
      "  batch [100/330] loss: 0.00017114284170020256\n",
      "  batch [150/330] loss: 0.00030301664519720363\n",
      "  batch [200/330] loss: 5.179007200968044e-05\n",
      "  batch [250/330] loss: 0.00022660923275179813\n",
      "  batch [300/330] loss: 0.0009414370173726638\n",
      "=> epoch 122, training loss: 0.0009414370173726638, training accuracy: 94.53809356689453\n",
      "  batch [50/330] loss: 0.0062250382844358685\n",
      "  batch [100/330] loss: 0.003380429410841316\n",
      "  batch [150/330] loss: 0.0021379654083866625\n",
      "  batch [200/330] loss: 0.0014280821605352685\n",
      "  batch [250/330] loss: 0.0009432309629046358\n",
      "  batch [300/330] loss: 0.0009103096096077934\n",
      "=> epoch 123, training loss: 0.0009103096096077934, training accuracy: 99.46666717529297\n",
      "  batch [50/330] loss: 0.001214377993310336\n",
      "  batch [100/330] loss: 0.0007139246704464312\n",
      "  batch [150/330] loss: 0.000604367299209116\n",
      "  batch [200/330] loss: 0.0004705879702814855\n",
      "  batch [250/330] loss: 0.0005514326562260976\n",
      "  batch [300/330] loss: 0.0002947390479384921\n",
      "=> epoch 124, training loss: 0.0002947390479384921, training accuracy: 99.89047241210938\n",
      "  batch [50/330] loss: 0.00019235213422507514\n",
      "  batch [100/330] loss: 0.00034248692404071333\n",
      "  batch [150/330] loss: 0.00038818980089126855\n",
      "  batch [200/330] loss: 0.0006101945228438126\n",
      "  batch [250/330] loss: 0.00039259084365039597\n",
      "  batch [300/330] loss: 0.0003621878809062764\n",
      "=> epoch 125, training loss: 0.0003621878809062764, training accuracy: 99.87142944335938\n",
      "  batch [50/330] loss: 0.00047918042493256505\n",
      "  batch [100/330] loss: 0.0002688782974291826\n",
      "  batch [150/330] loss: 0.0003127507957706257\n",
      "  batch [200/330] loss: 0.0003011687265789078\n",
      "  batch [250/330] loss: 0.0012196745380570065\n",
      "  batch [300/330] loss: 0.0005897713932208717\n",
      "=> epoch 126, training loss: 0.0005897713932208717, training accuracy: 99.88571166992188\n",
      "  batch [50/330] loss: 0.00021191126445046393\n",
      "  batch [100/330] loss: 0.0004714830642860761\n",
      "  batch [150/330] loss: 0.0006010016910586273\n",
      "  batch [200/330] loss: 0.0009871300574668567\n",
      "  batch [250/330] loss: 0.0015531814954883884\n",
      "  batch [300/330] loss: 0.0013809060860075987\n",
      "=> epoch 127, training loss: 0.0013809060860075987, training accuracy: 99.20476531982422\n",
      "  batch [50/330] loss: 0.0008437544065382098\n",
      "  batch [100/330] loss: 0.001487358975748066\n",
      "  batch [150/330] loss: 0.0009834204861545004\n",
      "  batch [200/330] loss: 0.0005006844694435131\n",
      "  batch [250/330] loss: 0.000804829324624734\n",
      "  batch [300/330] loss: 0.0010280062365345657\n",
      "=> epoch 128, training loss: 0.0010280062365345657, training accuracy: 98.30000305175781\n",
      "  batch [50/330] loss: 0.015258517479989677\n",
      "  batch [100/330] loss: 0.004683951018378139\n",
      "  batch [150/330] loss: 0.0030396654368378224\n",
      "  batch [200/330] loss: 0.0020501379172783345\n",
      "  batch [250/330] loss: 0.0022766293139429763\n",
      "  batch [300/330] loss: 0.0016542054950259627\n",
      "=> epoch 129, training loss: 0.0016542054950259627, training accuracy: 99.9000015258789\n",
      "  batch [50/330] loss: 0.0003152288380661048\n",
      "  batch [100/330] loss: 0.00036391151329735294\n",
      "  batch [150/330] loss: 0.00027040666405810045\n",
      "  batch [200/330] loss: 0.000391547161154449\n",
      "  batch [250/330] loss: 0.00024555684404913334\n",
      "  batch [300/330] loss: 0.00017890207027085125\n",
      "=> epoch 130, training loss: 0.00017890207027085125, training accuracy: 100.0\n",
      "  batch [50/330] loss: 0.00031837291263218503\n",
      "  batch [100/330] loss: 0.00024427704777917823\n",
      "  batch [150/330] loss: 0.000295101813680958\n",
      "  batch [200/330] loss: 8.579301563440822e-05\n",
      "  batch [250/330] loss: 0.0001683332119791885\n",
      "  batch [300/330] loss: 8.205793683009687e-05\n",
      "=> epoch 131, training loss: 8.205793683009687e-05, training accuracy: 99.97618865966797\n",
      "  batch [50/330] loss: 0.00021868182346224786\n",
      "  batch [100/330] loss: 0.00022507667092577322\n",
      "  batch [150/330] loss: 0.0003027038930922572\n",
      "  batch [200/330] loss: 0.00041614335175836457\n",
      "  batch [250/330] loss: 0.00039450531142938414\n",
      "  batch [300/330] loss: 0.00028869881713035285\n",
      "=> epoch 132, training loss: 0.00028869881713035285, training accuracy: 99.94761657714844\n",
      "  batch [50/330] loss: 0.00108561945004476\n",
      "  batch [100/330] loss: 0.0008519787726108916\n",
      "  batch [150/330] loss: 0.000691370424348861\n",
      "  batch [200/330] loss: 0.0007459626851632492\n",
      "  batch [250/330] loss: 0.0009612744953192305\n",
      "  batch [300/330] loss: 0.0005290007227449678\n",
      "=> epoch 133, training loss: 0.0005290007227449678, training accuracy: 99.80952453613281\n",
      "  batch [50/330] loss: 0.0002750420365773607\n",
      "  batch [100/330] loss: 0.00022034662844816922\n",
      "  batch [150/330] loss: 0.0005103385507609346\n",
      "  batch [200/330] loss: 0.0006003969107259763\n",
      "  batch [250/330] loss: 0.0005546855692373356\n",
      "  batch [300/330] loss: 0.0006271346136636567\n",
      "=> epoch 134, training loss: 0.0006271346136636567, training accuracy: 99.69523620605469\n",
      "  batch [50/330] loss: 0.000571718872837664\n",
      "  batch [100/330] loss: 0.0005516327700388501\n",
      "  batch [150/330] loss: 0.0006655796132981777\n",
      "  batch [200/330] loss: 0.0011492583919098251\n",
      "  batch [250/330] loss: 0.001405583524290705\n",
      "  batch [300/330] loss: 0.0007390059565368574\n",
      "=> epoch 135, training loss: 0.0007390059565368574, training accuracy: 99.60952758789062\n",
      "  batch [50/330] loss: 0.0006640335877018514\n",
      "  batch [100/330] loss: 0.0007833297277538804\n",
      "  batch [150/330] loss: 0.0008830625080736354\n",
      "  batch [200/330] loss: 0.00044180602001142686\n",
      "  batch [250/330] loss: 0.0009847438072029036\n",
      "  batch [300/330] loss: 0.0007160149952687789\n",
      "=> epoch 136, training loss: 0.0007160149952687789, training accuracy: 99.4190444946289\n",
      "  batch [50/330] loss: 0.00043871116594527846\n",
      "  batch [100/330] loss: 0.0004428483044321183\n",
      "  batch [150/330] loss: 0.0003605239512980916\n",
      "  batch [200/330] loss: 0.00047725789828109553\n",
      "  batch [250/330] loss: 0.00029495363626119795\n",
      "  batch [300/330] loss: 0.0010365542166255182\n",
      "=> epoch 137, training loss: 0.0010365542166255182, training accuracy: 99.78571319580078\n",
      "  batch [50/330] loss: 0.0004660003775425139\n",
      "  batch [100/330] loss: 0.0003094899854659161\n",
      "  batch [150/330] loss: 0.00043821877367736304\n",
      "  batch [200/330] loss: 0.000582109208957263\n",
      "  batch [250/330] loss: 0.0006067686116221012\n",
      "  batch [300/330] loss: 0.0006911108982458245\n",
      "=> epoch 138, training loss: 0.0006911108982458245, training accuracy: 99.67619323730469\n",
      "  batch [50/330] loss: 0.0004185354014225595\n",
      "  batch [100/330] loss: 0.00013763450329133775\n",
      "  batch [150/330] loss: 0.0006762671192736888\n",
      "  batch [200/330] loss: 0.0008328354150726227\n",
      "  batch [250/330] loss: 0.0003692975871017552\n",
      "  batch [300/330] loss: 0.0006412055076361867\n",
      "=> epoch 139, training loss: 0.0006412055076361867, training accuracy: 99.76190185546875\n",
      "  batch [50/330] loss: 0.0020581752643338406\n",
      "  batch [100/330] loss: 0.0009583645344973774\n",
      "  batch [150/330] loss: 0.0007361764480156125\n",
      "  batch [200/330] loss: 0.0004965234404808143\n",
      "  batch [250/330] loss: 0.0008114127578446642\n",
      "  batch [300/330] loss: 0.000548775831295643\n",
      "=> epoch 140, training loss: 0.000548775831295643, training accuracy: 99.81904602050781\n",
      "  batch [50/330] loss: 0.00046477630178560505\n",
      "  batch [100/330] loss: 0.0003153769836644642\n",
      "  batch [150/330] loss: 0.000308407518259628\n",
      "  batch [200/330] loss: 0.0003449954570241971\n",
      "  batch [250/330] loss: 0.0004853573747095652\n",
      "  batch [300/330] loss: 0.0006898485240526498\n",
      "=> epoch 141, training loss: 0.0006898485240526498, training accuracy: 97.51905059814453\n",
      "  batch [50/330] loss: 0.00592640656628646\n",
      "  batch [100/330] loss: 0.0017365511177340523\n",
      "  batch [150/330] loss: 0.0016294997166842221\n",
      "  batch [200/330] loss: 0.002623942847363651\n",
      "  batch [250/330] loss: 0.004122824450023472\n",
      "  batch [300/330] loss: 0.0012876000972464681\n",
      "=> epoch 142, training loss: 0.0012876000972464681, training accuracy: 99.5809555053711\n",
      "  batch [50/330] loss: 0.0012536751619772986\n",
      "  batch [100/330] loss: 0.0009601290000136941\n",
      "  batch [150/330] loss: 0.0004293104503012728\n",
      "  batch [200/330] loss: 0.00036220591886376496\n",
      "  batch [250/330] loss: 0.00045907757338136437\n",
      "  batch [300/330] loss: 0.0005024106995842885\n",
      "=> epoch 143, training loss: 0.0005024106995842885, training accuracy: 99.96190643310547\n",
      "  batch [50/330] loss: 0.0002714918015517469\n",
      "  batch [100/330] loss: 0.00016753319971030578\n",
      "  batch [150/330] loss: 0.0001684227694713627\n",
      "  batch [200/330] loss: 0.00014576912486518267\n",
      "  batch [250/330] loss: 0.0001169490634565591\n",
      "  batch [300/330] loss: 0.00019954993260034825\n",
      "=> epoch 144, training loss: 0.00019954993260034825, training accuracy: 99.98094940185547\n",
      "  batch [50/330] loss: 0.00015323246105981524\n",
      "  batch [100/330] loss: 0.0001662308292943635\n",
      "  batch [150/330] loss: 0.00011139892826759024\n",
      "  batch [200/330] loss: 0.00022220786546677117\n",
      "  batch [250/330] loss: 0.0001011785119844717\n",
      "  batch [300/330] loss: 0.0001530534091507434\n",
      "=> epoch 145, training loss: 0.0001530534091507434, training accuracy: 99.95714569091797\n",
      "  batch [50/330] loss: 0.0032623663218109868\n",
      "  batch [100/330] loss: 0.001448550439963583\n",
      "  batch [150/330] loss: 0.0014221683717914857\n",
      "  batch [200/330] loss: 0.0010632236681412906\n",
      "  batch [250/330] loss: 0.0007824475436354987\n",
      "  batch [300/330] loss: 0.000748367116379086\n",
      "=> epoch 146, training loss: 0.000748367116379086, training accuracy: 99.63809204101562\n",
      "  batch [50/330] loss: 0.002805602937238291\n",
      "  batch [100/330] loss: 0.0014606240353314205\n",
      "  batch [150/330] loss: 0.001061702572624199\n",
      "  batch [200/330] loss: 0.0009187116235261783\n",
      "  batch [250/330] loss: 0.0006797741962654982\n",
      "  batch [300/330] loss: 0.0003485738764720736\n",
      "=> epoch 147, training loss: 0.0003485738764720736, training accuracy: 99.88571166992188\n",
      "  batch [50/330] loss: 0.00022339831083081663\n",
      "  batch [100/330] loss: 0.00030406631268851925\n",
      "  batch [150/330] loss: 0.000301689335530682\n",
      "  batch [200/330] loss: 0.00016516220175981288\n",
      "  batch [250/330] loss: 0.00010877722671284573\n",
      "  batch [300/330] loss: 0.00022440769067179646\n",
      "=> epoch 148, training loss: 0.00022440769067179646, training accuracy: 99.9952392578125\n",
      "  batch [50/330] loss: 9.692874673783081e-05\n",
      "  batch [100/330] loss: 0.00012446063785318985\n",
      "  batch [150/330] loss: 0.0006630253480470855\n",
      "  batch [200/330] loss: 0.00042676347748783885\n",
      "  batch [250/330] loss: 0.0002692590963051771\n",
      "  batch [300/330] loss: 0.00028548276406218065\n",
      "=> epoch 149, training loss: 0.00028548276406218065, training accuracy: 99.93333435058594\n",
      "  batch [50/330] loss: 0.00025158941479458006\n",
      "  batch [100/330] loss: 0.0007553310154617066\n",
      "  batch [150/330] loss: 0.0009008071491480223\n",
      "  batch [200/330] loss: 0.0007474796146561858\n",
      "  batch [250/330] loss: 0.000684694346273318\n",
      "  batch [300/330] loss: 0.0006135385853849584\n",
      "=> epoch 150, training loss: 0.0006135385853849584, training accuracy: 99.74285888671875\n",
      "  batch [50/330] loss: 0.000828251970859128\n",
      "  batch [100/330] loss: 0.0005689151987753575\n",
      "  batch [150/330] loss: 0.0005441091798275011\n",
      "  batch [200/330] loss: 0.00035304515379539224\n",
      "  batch [250/330] loss: 0.0007477810511263669\n",
      "  batch [300/330] loss: 0.0011287399945504148\n",
      "=> epoch 151, training loss: 0.0011287399945504148, training accuracy: 98.6047592163086\n",
      "  batch [50/330] loss: 0.005310002009151504\n",
      "  batch [100/330] loss: inf\n",
      "  batch [150/330] loss: 0.0015040454387199133\n",
      "  batch [200/330] loss: 0.00412711764650885\n",
      "  batch [250/330] loss: 0.0017852587935049088\n",
      "  batch [300/330] loss: 0.0011347664800705388\n",
      "=> epoch 152, training loss: 0.0011347664800705388, training accuracy: 98.95714569091797\n",
      "  batch [50/330] loss: 0.0009053631597780623\n",
      "  batch [100/330] loss: 0.00040392876451369374\n",
      "  batch [150/330] loss: 0.0005707957900012844\n",
      "  batch [200/330] loss: 0.00035043351080094\n",
      "  batch [250/330] loss: 0.000455528703489108\n",
      "  batch [300/330] loss: 0.0004925513917041826\n",
      "=> epoch 153, training loss: 0.0004925513917041826, training accuracy: 99.990478515625\n",
      "  batch [50/330] loss: 0.00025318000707193276\n",
      "  batch [100/330] loss: 5.969854402792407e-05\n",
      "  batch [150/330] loss: 8.525758179166587e-05\n",
      "  batch [200/330] loss: 0.00015188279779977166\n",
      "  batch [250/330] loss: 0.00012117298783596198\n",
      "  batch [300/330] loss: 8.194412705779541e-05\n",
      "=> epoch 154, training loss: 8.194412705779541e-05, training accuracy: 99.9952392578125\n",
      "  batch [50/330] loss: 8.189566127839499e-05\n",
      "  batch [100/330] loss: 3.720801378949545e-05\n",
      "  batch [150/330] loss: 0.0002007542827486759\n",
      "  batch [200/330] loss: 8.230360813467996e-05\n",
      "  batch [250/330] loss: 3.799221207736991e-05\n",
      "  batch [300/330] loss: 4.4412523155187954e-05\n",
      "=> epoch 155, training loss: 4.4412523155187954e-05, training accuracy: 99.98094940185547\n",
      "  batch [50/330] loss: 1.824590008800442e-05\n",
      "  batch [100/330] loss: 2.1107856859089226e-05\n",
      "  batch [150/330] loss: 9.763418402326351e-05\n",
      "  batch [200/330] loss: 5.0773234324879016e-05\n",
      "  batch [250/330] loss: 4.2592690409946954e-05\n",
      "  batch [300/330] loss: 0.0005340036388151929\n",
      "=> epoch 156, training loss: 0.0005340036388151929, training accuracy: 99.69047546386719\n",
      "  batch [50/330] loss: 0.005967470138013596\n",
      "  batch [100/330] loss: 0.002829689856036566\n",
      "  batch [150/330] loss: 0.0020989414660143664\n",
      "  batch [200/330] loss: 0.0012542550785001366\n",
      "  batch [250/330] loss: 0.0009534546056820545\n",
      "  batch [300/330] loss: 0.001030333313945448\n",
      "=> epoch 157, training loss: 0.001030333313945448, training accuracy: 99.81428527832031\n",
      "  batch [50/330] loss: 0.001103018425143091\n",
      "  batch [100/330] loss: 0.0008367046685307287\n",
      "  batch [150/330] loss: 0.0006575652378087398\n",
      "  batch [200/330] loss: 0.00038271378744684625\n",
      "  batch [250/330] loss: 0.0004017193731924635\n",
      "  batch [300/330] loss: 0.0004018058232686599\n",
      "=> epoch 158, training loss: 0.0004018058232686599, training accuracy: 99.95237731933594\n",
      "  batch [50/330] loss: 7.002196665780503e-05\n",
      "  batch [100/330] loss: 0.00015164473951153923\n",
      "  batch [150/330] loss: 0.0002603880555834621\n",
      "  batch [200/330] loss: 0.000313330589873658\n",
      "  batch [250/330] loss: 0.00016362793078587856\n",
      "  batch [300/330] loss: 0.0002160862225937308\n",
      "=> epoch 159, training loss: 0.0002160862225937308, training accuracy: 99.87619018554688\n",
      "  batch [50/330] loss: 7.563415286131203e-05\n",
      "  batch [100/330] loss: 7.592419732372946e-05\n",
      "  batch [150/330] loss: 5.22715176721249e-05\n",
      "  batch [200/330] loss: 0.00018492605704886956\n",
      "  batch [250/330] loss: 5.7109199231490495e-05\n",
      "  batch [300/330] loss: 0.00010746498682783567\n",
      "=> epoch 160, training loss: 0.00010746498682783567, training accuracy: 99.98094940185547\n",
      "  batch [50/330] loss: 0.0008307627618632978\n",
      "  batch [100/330] loss: 0.001874745136243291\n",
      "  batch [150/330] loss: 0.0012625427034654421\n",
      "  batch [200/330] loss: 0.0025902557946683373\n",
      "  batch [250/330] loss: 0.0024361255781841463\n",
      "  batch [300/330] loss: 0.0014279701861960347\n",
      "=> epoch 161, training loss: 0.0014279701861960347, training accuracy: 99.52381134033203\n",
      "  batch [50/330] loss: 0.00036888068707776256\n",
      "  batch [100/330] loss: 0.0006928628727255273\n",
      "  batch [150/330] loss: 0.0006347631590906531\n",
      "  batch [200/330] loss: 0.0006695833016128745\n",
      "  batch [250/330] loss: 0.0003687274293770315\n",
      "  batch [300/330] loss: 0.0004184195437410381\n",
      "=> epoch 162, training loss: 0.0004184195437410381, training accuracy: 99.89047241210938\n",
      "  batch [50/330] loss: 0.00028467188276408705\n",
      "  batch [100/330] loss: 0.0004096100999813643\n",
      "  batch [150/330] loss: 0.0003018768117763102\n",
      "  batch [200/330] loss: 0.00032673610376514263\n",
      "  batch [250/330] loss: 0.000326122451624542\n",
      "  batch [300/330] loss: 0.00017736487207002937\n",
      "=> epoch 163, training loss: 0.00017736487207002937, training accuracy: 99.63809204101562\n",
      "  batch [50/330] loss: 0.00028506574497077965\n",
      "  batch [100/330] loss: 0.0002501337978501397\n",
      "  batch [150/330] loss: 0.000129231531136611\n",
      "  batch [200/330] loss: 8.936608253134181e-05\n",
      "  batch [250/330] loss: 0.0002545494498917833\n",
      "  batch [300/330] loss: 0.0002964645165920956\n",
      "=> epoch 164, training loss: 0.0002964645165920956, training accuracy: 99.76190185546875\n",
      "  batch [50/330] loss: 0.0015746877063866124\n",
      "  batch [100/330] loss: 0.0012107287736034778\n",
      "  batch [150/330] loss: 0.0009168289364752127\n",
      "  batch [200/330] loss: 0.0006045687145233387\n",
      "  batch [250/330] loss: 0.00094136163208168\n",
      "  batch [300/330] loss: 0.0005061536198700196\n",
      "=> epoch 165, training loss: 0.0005061536198700196, training accuracy: 99.67619323730469\n",
      "  batch [50/330] loss: 0.003135220506461337\n",
      "  batch [100/330] loss: 0.001486943377647549\n",
      "  batch [150/330] loss: 0.001147146330709802\n",
      "  batch [200/330] loss: 0.001417700047197286\n",
      "  batch [250/330] loss: 0.0009665698351454921\n",
      "  batch [300/330] loss: 0.000503697579697473\n",
      "=> epoch 166, training loss: 0.000503697579697473, training accuracy: 99.92857360839844\n",
      "  batch [50/330] loss: 0.00023884804516274017\n",
      "  batch [100/330] loss: 0.00024245159564452478\n",
      "  batch [150/330] loss: 0.00021953554743231506\n",
      "  batch [200/330] loss: 0.00035960484712268224\n",
      "  batch [250/330] loss: 0.00046049599327670877\n",
      "  batch [300/330] loss: 0.0004081391871586675\n",
      "=> epoch 167, training loss: 0.0004081391871586675, training accuracy: 99.86666870117188\n",
      "  batch [50/330] loss: 0.00409422382327466\n",
      "  batch [100/330] loss: 0.004240062705008313\n",
      "  batch [150/330] loss: 0.004730534338857979\n",
      "  batch [200/330] loss: 0.003160161668783985\n",
      "  batch [250/330] loss: 0.0014033437527250499\n",
      "  batch [300/330] loss: 0.001187613130430691\n",
      "=> epoch 168, training loss: 0.001187613130430691, training accuracy: 99.84285736083984\n",
      "  batch [50/330] loss: 0.0003469926965772174\n",
      "  batch [100/330] loss: 0.00033995441656588807\n",
      "  batch [150/330] loss: 0.0007313120170438196\n",
      "  batch [200/330] loss: 0.00029931551987829154\n",
      "  batch [250/330] loss: 0.00025270672017359177\n",
      "  batch [300/330] loss: 0.0006852159770933213\n",
      "=> epoch 169, training loss: 0.0006852159770933213, training accuracy: 99.9190444946289\n",
      "  batch [50/330] loss: 0.000328055971491267\n",
      "  batch [100/330] loss: 0.00013744100844633066\n",
      "  batch [150/330] loss: 0.00022217901694239118\n",
      "  batch [200/330] loss: 0.00014621527550480096\n",
      "  batch [250/330] loss: 0.00022900255236163502\n",
      "  batch [300/330] loss: 0.0008164107017946663\n",
      "=> epoch 170, training loss: 0.0008164107017946663, training accuracy: 99.95237731933594\n",
      "  batch [50/330] loss: 0.00023310108907026005\n",
      "  batch [100/330] loss: 9.190586340264417e-05\n",
      "  batch [150/330] loss: 5.315065545255493e-05\n",
      "  batch [200/330] loss: 0.00021038279900312772\n",
      "  batch [250/330] loss: 0.00012821070793870602\n",
      "  batch [300/330] loss: 0.0002897399305766157\n",
      "=> epoch 171, training loss: 0.0002897399305766157, training accuracy: 99.80476379394531\n",
      "  batch [50/330] loss: 0.004465630825841799\n",
      "  batch [100/330] loss: 0.002591507907549385\n",
      "  batch [150/330] loss: 0.0008319518994539977\n",
      "  batch [200/330] loss: 0.0009518186283530667\n",
      "  batch [250/330] loss: 0.0009290116043848684\n",
      "  batch [300/330] loss: 0.0012582617208827286\n",
      "=> epoch 172, training loss: 0.0012582617208827286, training accuracy: 99.81904602050781\n",
      "  batch [50/330] loss: 0.0002523718136508251\n",
      "  batch [100/330] loss: 0.00031699764155928276\n",
      "  batch [150/330] loss: 0.0002970209071208956\n",
      "  batch [200/330] loss: 0.0002630041134543717\n",
      "  batch [250/330] loss: 0.00010577879918855616\n",
      "  batch [300/330] loss: 0.00011218400527286576\n",
      "=> epoch 173, training loss: 0.00011218400527286576, training accuracy: 99.97618865966797\n",
      "  batch [50/330] loss: 0.00010957438028344768\n",
      "  batch [100/330] loss: 3.701845095929457e-05\n",
      "  batch [150/330] loss: 5.311011532830889e-05\n",
      "  batch [200/330] loss: 3.578313640173292e-05\n",
      "  batch [250/330] loss: 0.00013170805827030562\n",
      "  batch [300/330] loss: 0.00015099782392280758\n",
      "=> epoch 174, training loss: 0.00015099782392280758, training accuracy: 99.990478515625\n",
      "  batch [50/330] loss: 0.0002859104501876573\n",
      "  batch [100/330] loss: 9.816800367116229e-05\n",
      "  batch [150/330] loss: 0.00014311224573066282\n",
      "  batch [200/330] loss: 0.00010115661836607615\n",
      "  batch [250/330] loss: 7.304741458574427e-05\n",
      "  batch [300/330] loss: 0.0005654809272782586\n",
      "=> epoch 175, training loss: 0.0005654809272782586, training accuracy: 99.88095092773438\n",
      "  batch [50/330] loss: 0.0002586888687474129\n",
      "  batch [100/330] loss: 0.00025621540937936516\n",
      "  batch [150/330] loss: 0.0012809475113244844\n",
      "  batch [200/330] loss: 0.0011840576754475478\n",
      "  batch [250/330] loss: 0.00066440861193405\n",
      "  batch [300/330] loss: 0.0008309800878923853\n",
      "=> epoch 176, training loss: 0.0008309800878923853, training accuracy: 99.57142639160156\n",
      "  batch [50/330] loss: 0.0015674527057708474\n",
      "  batch [100/330] loss: 0.000670874089730205\n",
      "  batch [150/330] loss: 0.0008621673633460887\n",
      "  batch [200/330] loss: 0.0012492100431700237\n",
      "  batch [250/330] loss: 0.0004902760327822761\n",
      "  batch [300/330] loss: 0.0004337816627230495\n",
      "=> epoch 177, training loss: 0.0004337816627230495, training accuracy: 99.87142944335938\n",
      "  batch [50/330] loss: 0.0003044981668062974\n",
      "  batch [100/330] loss: 0.00029718178621988047\n",
      "  batch [150/330] loss: 0.0003476794071757467\n",
      "  batch [200/330] loss: 0.0007951781220908742\n",
      "  batch [250/330] loss: 0.0005445043146828539\n",
      "  batch [300/330] loss: 0.0005983321619496564\n",
      "=> epoch 178, training loss: 0.0005983321619496564, training accuracy: 99.74761962890625\n",
      "  batch [50/330] loss: 0.0018190939067280851\n",
      "  batch [100/330] loss: 0.0009092074376239907\n",
      "  batch [150/330] loss: 0.0009717253578492091\n",
      "  batch [200/330] loss: 0.0003669282998162089\n",
      "  batch [250/330] loss: 0.0010701172659028089\n",
      "  batch [300/330] loss: 0.0007013053146365564\n",
      "=> epoch 179, training loss: 0.0007013053146365564, training accuracy: 99.87619018554688\n",
      "  batch [50/330] loss: 0.0002059136919087905\n",
      "  batch [100/330] loss: 0.0005259544826403726\n",
      "  batch [150/330] loss: 0.00038174219967913815\n",
      "  batch [200/330] loss: 0.00030223298037890343\n",
      "  batch [250/330] loss: 0.00038731002174608874\n",
      "  batch [300/330] loss: 0.0013611056541267318\n",
      "=> epoch 180, training loss: 0.0013611056541267318, training accuracy: 99.32857513427734\n",
      "  batch [50/330] loss: 0.0006933844355444307\n",
      "  batch [100/330] loss: 0.00032587268765928455\n",
      "  batch [150/330] loss: 0.00025388098879193424\n",
      "  batch [200/330] loss: 0.00016734323059790769\n",
      "  batch [250/330] loss: 0.00018733788174722576\n",
      "  batch [300/330] loss: 0.00024668716631276764\n",
      "=> epoch 181, training loss: 0.00024668716631276764, training accuracy: 99.92857360839844\n",
      "  batch [50/330] loss: 0.00023055416737042834\n",
      "  batch [100/330] loss: 0.00022011022014157788\n",
      "  batch [150/330] loss: 0.0002341039753955556\n",
      "  batch [200/330] loss: 0.00034086928467331746\n",
      "  batch [250/330] loss: 0.00024248286799775088\n",
      "  batch [300/330] loss: 0.0002701458819283289\n",
      "=> epoch 182, training loss: 0.0002701458819283289, training accuracy: 99.88571166992188\n",
      "  batch [50/330] loss: 0.0001811399049984175\n",
      "  batch [100/330] loss: 0.0003512389018378599\n",
      "  batch [150/330] loss: 0.0005083387281410979\n",
      "  batch [200/330] loss: 0.0004656487366191868\n",
      "  batch [250/330] loss: 0.0002813585962758225\n",
      "  batch [300/330] loss: 0.00023147054670698709\n",
      "=> epoch 183, training loss: 0.00023147054670698709, training accuracy: 99.42857360839844\n",
      "  batch [50/330] loss: 0.0009003925369252101\n",
      "  batch [100/330] loss: 0.0006291981830145232\n",
      "  batch [150/330] loss: 0.0010777155983923876\n",
      "  batch [200/330] loss: 0.00044971099609210797\n",
      "  batch [250/330] loss: 0.0005981446863588644\n",
      "  batch [300/330] loss: 0.000707111051895481\n",
      "=> epoch 184, training loss: 0.000707111051895481, training accuracy: 99.76667022705078\n",
      "  batch [50/330] loss: 0.001825253382907249\n",
      "  batch [100/330] loss: 0.0009376259785494767\n",
      "  batch [150/330] loss: 0.0006661996835755417\n",
      "  batch [200/330] loss: 0.0012178449467319297\n",
      "  batch [250/330] loss: 0.0007516061540372902\n",
      "  batch [300/330] loss: 0.0006463149898627307\n",
      "=> epoch 185, training loss: 0.0006463149898627307, training accuracy: 99.85237884521484\n",
      "  batch [50/330] loss: 0.0002717567684740061\n",
      "  batch [100/330] loss: 0.0003589184614902479\n",
      "  batch [150/330] loss: 0.0002715926286764443\n",
      "  batch [200/330] loss: 0.0003991064490037388\n",
      "  batch [250/330] loss: 0.0003825648736092262\n",
      "  batch [300/330] loss: 0.0003936033456484438\n",
      "=> epoch 186, training loss: 0.0003936033456484438, training accuracy: 99.9095230102539\n",
      "  batch [50/330] loss: 0.00040953723664279094\n",
      "  batch [100/330] loss: 0.0003144372518145246\n",
      "  batch [150/330] loss: 0.00029850362535216847\n",
      "  batch [200/330] loss: 0.00022307589422052842\n",
      "  batch [250/330] loss: 0.0005185034769929189\n",
      "  batch [300/330] loss: 0.0008288848027368658\n",
      "=> epoch 187, training loss: 0.0008288848027368658, training accuracy: 99.76667022705078\n",
      "  batch [50/330] loss: 0.0004531283874130168\n",
      "  batch [100/330] loss: 0.00019976208690059138\n",
      "  batch [150/330] loss: 0.00026763244341418613\n",
      "  batch [200/330] loss: 0.00029283319733622195\n",
      "  batch [250/330] loss: 0.00045790759685587545\n",
      "  batch [300/330] loss: 0.0005195830077045684\n",
      "=> epoch 188, training loss: 0.0005195830077045684, training accuracy: 99.57142639160156\n",
      "  batch [50/330] loss: 0.0009943296015553643\n",
      "  batch [100/330] loss: 0.0004979545345486258\n",
      "  batch [150/330] loss: 0.0005881102548883064\n",
      "  batch [200/330] loss: 0.0005443893992051016\n",
      "  batch [250/330] loss: 0.0004883050206335611\n",
      "  batch [300/330] loss: 0.0007004070999246324\n",
      "=> epoch 189, training loss: 0.0007004070999246324, training accuracy: 99.77619171142578\n",
      "  batch [50/330] loss: 0.00042640381581441033\n",
      "  batch [100/330] loss: 0.00047116673627169804\n",
      "  batch [150/330] loss: 0.00035290502796124203\n",
      "  batch [200/330] loss: 0.0007238212057563942\n",
      "  batch [250/330] loss: 0.00045654662690503754\n",
      "  batch [300/330] loss: 0.00034819294577027903\n",
      "=> epoch 190, training loss: 0.00034819294577027903, training accuracy: 99.74761962890625\n",
      "  batch [50/330] loss: 0.0002558904784964398\n",
      "  batch [100/330] loss: 0.0004391153438045876\n",
      "  batch [150/330] loss: 0.0006149255995696876\n",
      "  batch [200/330] loss: 0.00029826253881947193\n",
      "  batch [250/330] loss: 0.0012601907605785526\n",
      "  batch [300/330] loss: 0.0016369146962242668\n",
      "=> epoch 191, training loss: 0.0016369146962242668, training accuracy: 99.60952758789062\n",
      "  batch [50/330] loss: 0.002750976490206085\n",
      "  batch [100/330] loss: 0.0028507290744455532\n",
      "  batch [150/330] loss: 0.0019470760494004935\n",
      "  batch [200/330] loss: 0.0008573873629211448\n",
      "  batch [250/330] loss: 0.0008013198639528127\n",
      "  batch [300/330] loss: 0.0005483381572375947\n",
      "=> epoch 192, training loss: 0.0005483381572375947, training accuracy: 99.87619018554688\n",
      "  batch [50/330] loss: 0.0003208473564736778\n",
      "  batch [100/330] loss: 0.00016701286324314425\n",
      "  batch [150/330] loss: 0.00014005495043238625\n",
      "  batch [200/330] loss: 0.00020234999033709756\n",
      "  batch [250/330] loss: 0.0001174793260433944\n",
      "  batch [300/330] loss: 0.00030138458418878147\n",
      "=> epoch 193, training loss: 0.00030138458418878147, training accuracy: 99.93333435058594\n",
      "  batch [50/330] loss: 0.004720309993936098\n",
      "  batch [100/330] loss: 0.0014973931568092666\n",
      "  batch [150/330] loss: 0.0009426657469011843\n",
      "  batch [200/330] loss: 0.0008645653688872699\n",
      "  batch [250/330] loss: 0.0016096725320385303\n",
      "  batch [300/330] loss: 0.0005164128369360697\n",
      "=> epoch 194, training loss: 0.0005164128369360697, training accuracy: 99.92857360839844\n",
      "  batch [50/330] loss: 0.00047060542366671144\n",
      "  batch [100/330] loss: 0.000247086719609797\n",
      "  batch [150/330] loss: 0.00010995737675693818\n",
      "  batch [200/330] loss: 0.00012560944063807255\n",
      "  batch [250/330] loss: 8.987625055669923e-05\n",
      "  batch [300/330] loss: 0.00016232126189061092\n",
      "=> epoch 195, training loss: 0.00016232126189061092, training accuracy: 99.990478515625\n",
      "  batch [50/330] loss: 0.00017912345529839512\n",
      "  batch [100/330] loss: 5.465548890424543e-05\n",
      "  batch [150/330] loss: 5.230626054435561e-05\n",
      "  batch [200/330] loss: 4.17977538490959e-05\n",
      "  batch [250/330] loss: 0.00010239248484958807\n",
      "  batch [300/330] loss: 5.954713086430274e-05\n",
      "=> epoch 196, training loss: 5.954713086430274e-05, training accuracy: 90.33333587646484\n",
      "  batch [50/330] loss: 0.0016213571334665175\n",
      "  batch [100/330] loss: 0.0008504353447642643\n",
      "  batch [150/330] loss: 0.0007180323813809081\n",
      "  batch [200/330] loss: 0.001960432696690987\n",
      "  batch [250/330] loss: 0.0009963145777146564\n",
      "  batch [300/330] loss: 0.0005161025737179443\n",
      "=> epoch 197, training loss: 0.0005161025737179443, training accuracy: 99.79523468017578\n",
      "  batch [50/330] loss: 0.00030820002914697396\n",
      "  batch [100/330] loss: 0.00018214099821489072\n",
      "  batch [150/330] loss: 0.0002481370595451153\n",
      "  batch [200/330] loss: 0.00014402455419985926\n",
      "  batch [250/330] loss: 0.0002031708814101876\n",
      "  batch [300/330] loss: 0.0002543613633388304\n",
      "=> epoch 198, training loss: 0.0002543613633388304, training accuracy: 99.9047622680664\n",
      "  batch [50/330] loss: 0.0005767176717899929\n",
      "  batch [100/330] loss: 0.0002185975581778621\n",
      "  batch [150/330] loss: 7.490236609373823e-05\n",
      "  batch [200/330] loss: 3.6974049035052306e-05\n",
      "  batch [250/330] loss: 5.230077581290971e-05\n",
      "  batch [300/330] loss: 0.0001750839718606585\n",
      "=> epoch 199, training loss: 0.0001750839718606585, training accuracy: 99.5999984741211\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training\")\n",
    "for epoch in range(EPOCH + 1):\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"=> epoch {epoch}, training loss: {train_loss}, training accuracy: {get_acc(model, train_dataset):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a714b157-c479-4095-881c-68ab969a83d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"model_resnet_cifar10_200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c95b93b4-18b7-497b-ae76-857a04d491f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shadow_model = torchvision.models.resnet34(num_classes=10).to(device)\n",
    "state_dict = torch.load(\"model_resnet_cifar10_200\", map_location=device)\n",
    "shadow_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc479f-852e-43e1-8f86-06ac5988e49a",
   "metadata": {},
   "source": [
    "For the sake of comparability, let's examine the accuracy that target and shadow model obtain on a test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70fcb455-39ee-4961-83e6-cd6d6be0fbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target model accuracy after 200 epoch: 0.00 %\n",
      "Shadow model accuracy after 200 epoch: 0.00 %\n"
     ]
    }
   ],
   "source": [
    "# TODO: RETRAINING, did not set seed when creating test set. But was approx. 67 - 69 % accuracy, not 90.\n",
    "\n",
    "print(f\"Target model accuracy after {EPOCH + 1} epoch: {get_acc(target_model, test_dataset):.2f} %\")\n",
    "print(f\"Shadow model accuracy after {EPOCH + 1} epoch: {get_acc(shadow_model, test_dataset):.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f5604-54a3-49d9-9616-15c1eb57967e",
   "metadata": {},
   "source": [
    "As we can see, the model performance is very similar, making our trained model a good proxy for the attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd56e13d-3c1b-4b93-b4f4-71351a682f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_attack_data(dataset, shadow_model):\n",
    "    attack_data = [dataset[row][0] for row in range(len(dataset))]\n",
    "    dataloader = torch.utils.data.DataLoader(attack_data, batch_size=64, shuffle=False, num_workers=2)\n",
    "    \n",
    "    result = torch.tensor([]).to(device)\n",
    "    for images in dataloader:\n",
    "        images = images.to(device)\n",
    "        outputs = shadow_model(images)\n",
    "        result = torch.cat((result, outputs), dim=0)\n",
    "    \n",
    "    probs = torch.softmax(result, dim=1).to(\"cpu\")\n",
    "    sorted_probs, _indices = torch.sort(probs, descending=True, dim=1)\n",
    "    \n",
    "    return sorted_probs[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1bf0a37-c719-41f1-ad05-5c7b23ff0bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30000, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_members = create_attack_data(train_dataset, shadow_model)\n",
    "attack_no_members = create_attack_data(test_dataset, shadow_model)\n",
    "attack_train = torch.cat((attack_members, attack_no_members), dim=0)\n",
    "attack_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bfffcdb-8857-477f-ada7-2bb04596e0f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attack_labels = torch.cat((torch.tensor([1]).repeat(len(train_dataset)), torch.tensor([0]).repeat(len(test_dataset))), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7df9e298-c540-484e-ac44-7399a21c45bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lg = LogisticRegression(max_iter=1000, class_weight={1:1, 0:2.3})\n",
    "lg_fitted = lg.fit(attack_train.to(\"cpu\").detach().numpy(), attack_labels.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa947594-d087-41c9-9350-84fe33bad30a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6938333333333333"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_fitted.score(attack_train.to(\"cpu\").detach().numpy(), attack_labels.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9687bef6-1b9a-4349-ab6a-32c64b527e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_fitted.predict(attack_train.to(\"cpu\").detach().numpy()[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b30e636-b672-41a4-9e90-0f870678b628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'pickle/cifar10/resnet34/eval.p'\n",
    "\n",
    "with open(DATA_PATH, \"rb\") as f:\n",
    "    eval_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc19ce67-0d97-4358-b9f3-da8d376a9817",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 18.05 GB, other allocations: 81.94 MB, max allowed: 18.13 GB). Tried to allocate 256 bytes on shared pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m eval_data \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_attack_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshadow_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m, in \u001b[0;36mcreate_attack_data\u001b[0;34m(dataset, shadow_model)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      7\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mshadow_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((result, outputs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     11\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(result, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torchvision/models/resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m--> 275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[1;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torchvision/models/resnet.py:93\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     92\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m---> 93\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m     96\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:151\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches_tracked \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_batches_tracked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# use cumulative moving average\u001b[39;00m\n\u001b[1;32m    153\u001b[0m             exponential_average_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches_tracked)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 18.05 GB, other allocations: 81.94 MB, max allowed: 18.13 GB). Tried to allocate 256 bytes on shared pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "eval_data = create_attack_data(eval_dataset, shadow_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e8f77f-ac62-43e1-ba9e-b736f6191ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_labels = [eval_dataset[row][2] for row in range(len(eval_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d1562-359f-4e86-a202-e9e4863ff5cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted = lg_fitted.predict(eval_data.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "66b73577-eb8a-487c-9d76-3ad49909e739",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4b141fe1-bf92-42f1-a7b6-2a4ee6d47f92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.06      0.50      0.11        12\n",
      "           1       0.94      0.50      0.65       188\n",
      "\n",
      "    accuracy                           0.50       200\n",
      "   macro avg       0.50      0.50      0.38       200\n",
      "weighted avg       0.89      0.50      0.62       200\n",
      "\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(classification_report(predicted, eval_labels))\n",
    "print(accuracy_score(predicted, eval_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d3d6b3db-d25e-401f-843f-c3951bb58d7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-e528dc1a32ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0059168-ae7f-43bd-a762-71f72e7b2e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
