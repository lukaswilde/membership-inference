{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f5f9f45-39b7-492b-ad2e-123b9b9040e6",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This project was created during the course of the \"Attacks Against Machine Learning Models\" lecture at Saarland University. The goal of this project is to attack a target model and find out, whether certain images where used as training data. This constitutes a Privacy Attack called Membership Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "163c1680-ceab-45f4-8faa-ea7a819e9e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device mps\n",
      "Shadow dataset contains 30000 images\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "DATA_PATH = 'pickle/cifar10/resnet34/shadow.p'\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "with open(DATA_PATH, \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "print(f\"Shadow dataset contains {len(dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "18ee2551-49d5-44db-9ca3-7c63340a3757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target model resnet34 on cifar10 was trained for 200 epochs with final accuracy of 68.49%\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "model_str = \"resnet34\"\n",
    "task_dataset = \"cifar10\"\n",
    "\n",
    "MODEL_PATH = f'models/{model_str}_{task_dataset}.pth'\n",
    "\n",
    "target_model = models.resnet34(num_classes=10).to(device)\n",
    "# Change num_classes to 200 when you use the Tiny ImageNet dataset\n",
    "\n",
    "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "target_model.load_state_dict(state_dict['net'])\n",
    "# Test accuracy\n",
    "ACC = state_dict['acc']\n",
    "# Training epoch (start from 0)\n",
    "EPOCH = state_dict['epoch']\n",
    "\n",
    "print(f\"Target model {model_str} on {task_dataset} was trained for {epoch + 1} epochs with final accuracy of {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002415db-4401-45e6-a3b5-a7bc048a5b08",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "As we want to replicate the target model with our own model (therefore called shadow model), we train it in a similar way to the target model. Fortunately, we have access to the number of epochs the training model was trained, so we have two possibilities moving forward:\n",
    "\n",
    "1. We can use the number of epochs obtained to train our shadow model for the same amount\n",
    "2. We can train our shadow model until it converges\n",
    "\n",
    "We will do both in the following and compare the attack performance on `eval.p`.\n",
    "\n",
    "First, we will need to split our shadow dataset, to obtain a portion of the data that will not be used for training. This test data contains only non-members of the shadow model and will later be used for training the attack model.\n",
    "\n",
    "For the first approach, we just need the train-test-split because we know the number of epochs.\n",
    "\n",
    "For the second approach, we add an additional validation-split to determine when the training converges on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2041d4b6-9e7e-4cc2-800b-92c666b95565",
   "metadata": {},
   "source": [
    "## 1. Fixed number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43eb5441-5c3a-4216-b8a4-aa175b6e9cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train-test-split with training containing 21000 samples and test containing 9000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "shuffled_data = random.sample(dataset, len(dataset))\n",
    "split = int(0.7 * len(shuffled_data))\n",
    "train_dataset = shuffled_data[:split]\n",
    "test_dataset = shuffled_data[split:]\n",
    "\n",
    "print(f\"Created train-test-split with training containing {len(train_dataset)} samples and test containing {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af1325-82ea-4307-b5d6-999253f4fb42",
   "metadata": {},
   "source": [
    "## Data Inspection\n",
    "\n",
    "Before going to the training, let's analyse the data a bit. Especially important is the distribution of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c992a477-38ce-4a55-8eec-96fdcd456afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[tensor(0.9294), tensor(0.9216), tensor(0.91...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[tensor(0.8863), tensor(0.8706), tensor(0.87...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[tensor(1.), tensor(1.), tensor(1.), tensor(...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[tensor(0.9804), tensor(0.9647), tensor(0.96...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[tensor(0.5137), tensor(0.4706), tensor(0.52...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  label\n",
       "0  [[[tensor(0.9294), tensor(0.9216), tensor(0.91...      0\n",
       "1  [[[tensor(0.8863), tensor(0.8706), tensor(0.87...      8\n",
       "2  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...      8\n",
       "3  [[[tensor(0.9804), tensor(0.9647), tensor(0.96...      8\n",
       "4  [[[tensor(0.5137), tensor(0.4706), tensor(0.52...      7"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df = pd.DataFrame(train_dataset, columns=[\"image\", \"label\"])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "94d10172-bcb4-4704-a510-2ea7eb6dd82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df[\"label\"].value_counts().loc[range(10)].plot(kind='bar')\n",
    "plt.title(\"Distribution of Classes in Training Dataset\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d967eb-8f02-4981-994d-988a39420fbe",
   "metadata": {},
   "source": [
    "We can see that the distribution across the classes is pretty even, so we continue with the training of the shadow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ed8da7fe-29ca-42f7-bf03-fea8c747ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "89dc224d-7e73-40dc-b225-15313d81d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet34(num_classes=10).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "65fdbc30-8777-40e3-9b10-375fa4b9d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for batch_idx, data in enumerate(train_dataloader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 50 == 49:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch [{}/{}] loss: {}'.format(batch_idx + 1, len(train_dataloader) + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "18470af3-6151-4c32-8235-b5348b33289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(model, dataset):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "    correct = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(dataloader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            correct += (predicted == labels).float().sum()\n",
    "    model.train()\n",
    "    return 100 * correct/len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4ff453c6-ceed-422c-805f-75791711152d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "  batch [50/330] loss: 0.1068658356666565\n",
      "  batch [100/330] loss: 0.08759368586540223\n",
      "  batch [150/330] loss: 0.08197230792045593\n",
      "  batch [200/330] loss: 0.07861152398586273\n",
      "  batch [250/330] loss: 0.07530254852771759\n",
      "  batch [300/330] loss: 0.07363667953014373\n",
      "=> epoch 0, training loss: 0.07363667953014373, training accuracy: 48.11428451538086\n",
      "  batch [50/330] loss: 0.06933662116527557\n",
      "  batch [100/330] loss: 0.06879744338989258\n",
      "  batch [150/330] loss: 0.06835883331298828\n",
      "  batch [200/330] loss: 0.06506826114654542\n",
      "  batch [250/330] loss: 0.0622162309885025\n",
      "  batch [300/330] loss: 0.06394648241996766\n",
      "=> epoch 1, training loss: 0.06394648241996766, training accuracy: 49.652381896972656\n",
      "  batch [50/330] loss: 0.05879727125167847\n",
      "  batch [100/330] loss: 0.06049016612768173\n",
      "  batch [150/330] loss: 0.056367367327213286\n",
      "  batch [200/330] loss: 0.05578539550304413\n",
      "  batch [250/330] loss: 0.05530743956565857\n",
      "  batch [300/330] loss: 0.05693060773611069\n",
      "=> epoch 2, training loss: 0.05693060773611069, training accuracy: 48.53809356689453\n",
      "  batch [50/330] loss: 0.05138816624879837\n",
      "  batch [100/330] loss: 0.0490004261136055\n",
      "  batch [150/330] loss: 0.04934092497825623\n",
      "  batch [200/330] loss: 0.050534088313579556\n",
      "  batch [250/330] loss: 0.04946613323688507\n",
      "  batch [300/330] loss: 0.04618237471580505\n",
      "=> epoch 3, training loss: 0.04618237471580505, training accuracy: 61.82381057739258\n",
      "  batch [50/330] loss: 0.06359453839063645\n",
      "  batch [100/330] loss: 0.052779612123966216\n",
      "  batch [150/330] loss: 0.0478846914768219\n",
      "  batch [200/330] loss: 0.04389829248189926\n",
      "  batch [250/330] loss: 0.044836941242218015\n",
      "  batch [300/330] loss: 0.04477424049377442\n",
      "=> epoch 4, training loss: 0.04477424049377442, training accuracy: 68.83809661865234\n",
      "  batch [50/330] loss: 0.04396295535564423\n",
      "  batch [100/330] loss: 0.039438320338726046\n",
      "  batch [150/330] loss: 0.041137365341186526\n",
      "  batch [200/330] loss: 0.04040877431631088\n",
      "  batch [250/330] loss: 0.040551473140716554\n",
      "  batch [300/330] loss: 0.03989031171798706\n",
      "=> epoch 5, training loss: 0.03989031171798706, training accuracy: 66.67142486572266\n",
      "  batch [50/330] loss: 0.037703517764806745\n",
      "  batch [100/330] loss: 0.03453251785039902\n",
      "  batch [150/330] loss: 0.03576798474788666\n",
      "  batch [200/330] loss: 0.032742463201284405\n",
      "  batch [250/330] loss: 0.03465905743837357\n",
      "  batch [300/330] loss: 0.03629852735996247\n",
      "=> epoch 6, training loss: 0.03629852735996247, training accuracy: 70.86190795898438\n",
      "  batch [50/330] loss: 0.033709623157978055\n",
      "  batch [100/330] loss: 0.03230057549476623\n",
      "  batch [150/330] loss: 0.030574890553951264\n",
      "  batch [200/330] loss: 0.03191977715492249\n",
      "  batch [250/330] loss: 0.032054860562086104\n",
      "  batch [300/330] loss: 0.02870264646410942\n",
      "=> epoch 7, training loss: 0.02870264646410942, training accuracy: 76.19999694824219\n",
      "  batch [50/330] loss: 0.028597040832042696\n",
      "  batch [100/330] loss: 0.02552327737212181\n",
      "  batch [150/330] loss: 0.026907452642917634\n",
      "  batch [200/330] loss: 0.028003647685050963\n",
      "  batch [250/330] loss: 0.02831116518378258\n",
      "  batch [300/330] loss: 0.024923843324184417\n",
      "=> epoch 8, training loss: 0.024923843324184417, training accuracy: 83.88571166992188\n",
      "  batch [50/330] loss: 0.02108789899945259\n",
      "  batch [100/330] loss: 0.019965058743953705\n",
      "  batch [150/330] loss: 0.022355159878730775\n",
      "  batch [200/330] loss: 0.020340451419353485\n",
      "  batch [250/330] loss: 0.02063937497138977\n",
      "  batch [300/330] loss: 0.024233102440834046\n",
      "=> epoch 9, training loss: 0.024233102440834046, training accuracy: 86.12857055664062\n",
      "  batch [50/330] loss: 0.018888762071728706\n",
      "  batch [100/330] loss: 0.016688043117523192\n",
      "  batch [150/330] loss: 0.01847303380072117\n",
      "  batch [200/330] loss: 0.01866377992928028\n",
      "  batch [250/330] loss: 0.0206982903778553\n",
      "  batch [300/330] loss: 0.020351059705018996\n",
      "=> epoch 10, training loss: 0.020351059705018996, training accuracy: 79.61904907226562\n",
      "  batch [50/330] loss: 0.012509813264012336\n",
      "  batch [100/330] loss: 0.013110561482608318\n",
      "  batch [150/330] loss: 0.014071465909481048\n",
      "  batch [200/330] loss: 0.015619287326931953\n",
      "  batch [250/330] loss: 0.01518468114733696\n",
      "  batch [300/330] loss: 0.015569860875606537\n",
      "=> epoch 11, training loss: 0.015569860875606537, training accuracy: 86.79523468017578\n",
      "  batch [50/330] loss: 0.011734618663787841\n",
      "  batch [100/330] loss: 0.011966076612472534\n",
      "  batch [150/330] loss: 0.015589226588606835\n",
      "  batch [200/330] loss: 0.014250730976462364\n",
      "  batch [250/330] loss: 0.012454441480338573\n",
      "  batch [300/330] loss: 0.013527733728289604\n",
      "=> epoch 12, training loss: 0.013527733728289604, training accuracy: 93.13333129882812\n",
      "  batch [50/330] loss: 0.007306047270074487\n",
      "  batch [100/330] loss: 0.010297546237707138\n",
      "  batch [150/330] loss: 0.011166468907147646\n",
      "  batch [200/330] loss: 0.011597271248698235\n",
      "  batch [250/330] loss: 0.011669527687132359\n",
      "  batch [300/330] loss: 0.013534554176032543\n",
      "=> epoch 13, training loss: 0.013534554176032543, training accuracy: 93.57618713378906\n",
      "  batch [50/330] loss: 0.009484473705291749\n",
      "  batch [100/330] loss: 0.008032210510224104\n",
      "  batch [150/330] loss: 0.008829221427440644\n",
      "  batch [200/330] loss: 0.008019113548099994\n",
      "  batch [250/330] loss: 0.010035360980778932\n",
      "  batch [300/330] loss: 0.010876055728644132\n",
      "=> epoch 14, training loss: 0.010876055728644132, training accuracy: 88.0142822265625\n",
      "  batch [50/330] loss: 0.0094937851652503\n",
      "  batch [100/330] loss: 0.022126705810427666\n",
      "  batch [150/330] loss: 0.02012892486155033\n",
      "  batch [200/330] loss: 0.013679249197244643\n",
      "  batch [250/330] loss: 0.011479354977607726\n",
      "  batch [300/330] loss: 0.008637605175375939\n",
      "=> epoch 15, training loss: 0.008637605175375939, training accuracy: 94.88095092773438\n",
      "  batch [50/330] loss: 0.01347693607956171\n",
      "  batch [100/330] loss: 0.007905754096806049\n",
      "  batch [150/330] loss: 0.0070462141297757625\n",
      "  batch [200/330] loss: 0.00746669302508235\n",
      "  batch [250/330] loss: 0.0062078734338283535\n",
      "  batch [300/330] loss: 0.010229815863072872\n",
      "=> epoch 16, training loss: 0.010229815863072872, training accuracy: 95.66666412353516\n",
      "  batch [50/330] loss: 0.006584717556834221\n",
      "  batch [100/330] loss: 0.005743696466088295\n",
      "  batch [150/330] loss: 0.004614121423102915\n",
      "  batch [200/330] loss: 0.005607094099745155\n",
      "  batch [250/330] loss: 0.006464300986379385\n",
      "  batch [300/330] loss: 0.00681824279949069\n",
      "=> epoch 17, training loss: 0.00681824279949069, training accuracy: 97.14286041259766\n",
      "  batch [50/330] loss: 0.005607432117685675\n",
      "  batch [100/330] loss: 0.004899291796144098\n",
      "  batch [150/330] loss: 0.005035794982686639\n",
      "  batch [200/330] loss: 0.006779435534030199\n",
      "  batch [250/330] loss: 0.008022978778928519\n",
      "  batch [300/330] loss: 0.006551807675510645\n",
      "=> epoch 18, training loss: 0.006551807675510645, training accuracy: 94.48094940185547\n",
      "  batch [50/330] loss: 0.003546012019738555\n",
      "  batch [100/330] loss: 0.004808508172631264\n",
      "  batch [150/330] loss: 0.003829120517708361\n",
      "  batch [200/330] loss: 0.00476404383033514\n",
      "  batch [250/330] loss: 0.004477831498719752\n",
      "  batch [300/330] loss: 0.011134718112647534\n",
      "=> epoch 19, training loss: 0.011134718112647534, training accuracy: 89.21428680419922\n",
      "  batch [50/330] loss: 0.010491737201809882\n",
      "  batch [100/330] loss: 0.006014281179755926\n",
      "  batch [150/330] loss: 0.006258222753182054\n",
      "  batch [200/330] loss: 0.006492763213813305\n",
      "  batch [250/330] loss: 0.004571698807179928\n",
      "  batch [300/330] loss: 0.006835734952241182\n",
      "=> epoch 20, training loss: 0.006835734952241182, training accuracy: 91.21904754638672\n",
      "  batch [50/330] loss: 0.015039225876331328\n",
      "  batch [100/330] loss: 0.008769127402454615\n",
      "  batch [150/330] loss: 0.005189692234620452\n",
      "  batch [200/330] loss: 0.005317723784595728\n",
      "  batch [250/330] loss: 0.0068278321661055085\n",
      "  batch [300/330] loss: 0.005365192249417305\n",
      "=> epoch 21, training loss: 0.005365192249417305, training accuracy: 96.509521484375\n",
      "  batch [50/330] loss: 0.006069052617996931\n",
      "  batch [100/330] loss: 0.0032887900797650216\n",
      "  batch [150/330] loss: 0.0029532416965812446\n",
      "  batch [200/330] loss: 0.0034454249106347563\n",
      "  batch [250/330] loss: 0.004265726431272924\n",
      "  batch [300/330] loss: 0.00446393215842545\n",
      "=> epoch 22, training loss: 0.00446393215842545, training accuracy: 94.5142822265625\n",
      "  batch [50/330] loss: 0.002829887719359249\n",
      "  batch [100/330] loss: 0.0021835880475118757\n",
      "  batch [150/330] loss: 0.002965758586768061\n",
      "  batch [200/330] loss: 0.0030933451633900406\n",
      "  batch [250/330] loss: 0.003950256848707795\n",
      "  batch [300/330] loss: 0.003111949955113232\n",
      "=> epoch 23, training loss: 0.003111949955113232, training accuracy: 95.64286041259766\n",
      "  batch [50/330] loss: 0.003655091875232756\n",
      "  batch [100/330] loss: 0.0033922966215759517\n",
      "  batch [150/330] loss: 0.003720174180343747\n",
      "  batch [200/330] loss: 0.0036162828528322278\n",
      "  batch [250/330] loss: 0.003821400315500796\n",
      "  batch [300/330] loss: 0.0050577730583027\n",
      "=> epoch 24, training loss: 0.0050577730583027, training accuracy: 94.78571319580078\n",
      "  batch [50/330] loss: 0.00945577908307314\n",
      "  batch [100/330] loss: 0.007027962097898126\n",
      "  batch [150/330] loss: 0.009507956862449646\n",
      "  batch [200/330] loss: 0.011686986215412616\n",
      "  batch [250/330] loss: 0.015014622509479523\n",
      "  batch [300/330] loss: 0.0071828761100769046\n",
      "=> epoch 25, training loss: 0.0071828761100769046, training accuracy: 71.93809509277344\n",
      "  batch [50/330] loss: 0.0682521307617426\n",
      "  batch [100/330] loss: 0.04178256124258042\n",
      "  batch [150/330] loss: 0.02818274062871933\n",
      "  batch [200/330] loss: 0.02136816483736038\n",
      "  batch [250/330] loss: 0.016729588344693184\n",
      "  batch [300/330] loss: 0.012652731209993362\n",
      "=> epoch 26, training loss: 0.012652731209993362, training accuracy: 94.24285888671875\n",
      "  batch [50/330] loss: 0.006918908715248108\n",
      "  batch [100/330] loss: 0.0049151411857455965\n",
      "  batch [150/330] loss: 0.005052358612418175\n",
      "  batch [200/330] loss: 0.00524057499691844\n",
      "  batch [250/330] loss: 0.005560717405751347\n",
      "  batch [300/330] loss: 0.004926783565431833\n",
      "=> epoch 27, training loss: 0.004926783565431833, training accuracy: 98.24285888671875\n",
      "  batch [50/330] loss: 0.004033873639069497\n",
      "  batch [100/330] loss: 0.002233353864401579\n",
      "  batch [150/330] loss: 0.0020569491509813817\n",
      "  batch [200/330] loss: 0.002408091572346166\n",
      "  batch [250/330] loss: 0.0028872651825658977\n",
      "  batch [300/330] loss: 0.0031082605081610383\n",
      "=> epoch 28, training loss: 0.0031082605081610383, training accuracy: 98.47142791748047\n",
      "  batch [50/330] loss: 0.002507049816194922\n",
      "  batch [100/330] loss: 0.0023441079584881662\n",
      "  batch [150/330] loss: 0.0025985207261983306\n",
      "  batch [200/330] loss: 0.0018856782610528172\n",
      "  batch [250/330] loss: 0.0020597255425527693\n",
      "  batch [300/330] loss: 0.0031319829113781454\n",
      "=> epoch 29, training loss: 0.0031319829113781454, training accuracy: 97.28571319580078\n",
      "  batch [50/330] loss: 0.00284370387904346\n",
      "  batch [100/330] loss: 0.0022909946490544827\n",
      "  batch [150/330] loss: 0.0029841051178518684\n",
      "  batch [200/330] loss: 0.0029107087501324715\n",
      "  batch [250/330] loss: 0.0020342694432474674\n",
      "  batch [300/330] loss: 0.002119807165348902\n",
      "=> epoch 30, training loss: 0.002119807165348902, training accuracy: 99.33809661865234\n",
      "  batch [50/330] loss: 0.006572709208354354\n",
      "  batch [100/330] loss: 0.00373558396846056\n",
      "  batch [150/330] loss: 0.004968596022576093\n",
      "  batch [200/330] loss: 0.0036069871354848146\n",
      "  batch [250/330] loss: 0.0030547293424606323\n",
      "  batch [300/330] loss: 0.0031065269401296973\n",
      "=> epoch 31, training loss: 0.0031065269401296973, training accuracy: 98.70952606201172\n",
      "  batch [50/330] loss: 0.004465340764261782\n",
      "  batch [100/330] loss: 0.002421722520608455\n",
      "  batch [150/330] loss: 0.002378082883078605\n",
      "  batch [200/330] loss: 0.002749831959139556\n",
      "  batch [250/330] loss: 0.00233195785805583\n",
      "  batch [300/330] loss: 0.002367912301328033\n",
      "=> epoch 32, training loss: 0.002367912301328033, training accuracy: 97.92857360839844\n",
      "  batch [50/330] loss: 0.007751646351069212\n",
      "  batch [100/330] loss: 0.0043624722277745605\n",
      "  batch [150/330] loss: 0.003178980654105544\n",
      "  batch [200/330] loss: 0.002081324426457286\n",
      "  batch [250/330] loss: 0.0024163020998239516\n",
      "  batch [300/330] loss: 0.002585888607427478\n",
      "=> epoch 33, training loss: 0.002585888607427478, training accuracy: 97.76190185546875\n",
      "  batch [50/330] loss: 0.004226043497212231\n",
      "  batch [100/330] loss: 0.004752209606580436\n",
      "  batch [150/330] loss: 0.003394491285085678\n",
      "  batch [200/330] loss: 0.0025788262858986856\n",
      "  batch [250/330] loss: 0.0018298223677556962\n",
      "  batch [300/330] loss: 0.0026387329921126364\n",
      "=> epoch 34, training loss: 0.0026387329921126364, training accuracy: 96.42381286621094\n",
      "  batch [50/330] loss: 0.0017424547316040844\n",
      "  batch [100/330] loss: 0.0015834452679846435\n",
      "  batch [150/330] loss: 0.0017678858216386287\n",
      "  batch [200/330] loss: 0.001812621854711324\n",
      "  batch [250/330] loss: 0.00282151795970276\n",
      "  batch [300/330] loss: 0.0023535399900283666\n",
      "=> epoch 35, training loss: 0.0023535399900283666, training accuracy: 96.85237884521484\n",
      "  batch [50/330] loss: 0.0026416849615052344\n",
      "  batch [100/330] loss: 0.0020767738056601957\n",
      "  batch [150/330] loss: 0.0011607038556830957\n",
      "  batch [200/330] loss: 0.00198103828006424\n",
      "  batch [250/330] loss: 0.0018441424327902496\n",
      "  batch [300/330] loss: 0.0020533403144218027\n",
      "=> epoch 36, training loss: 0.0020533403144218027, training accuracy: 98.88095092773438\n",
      "  batch [50/330] loss: 0.004625035649165511\n",
      "  batch [100/330] loss: 0.0028994836402125657\n",
      "  batch [150/330] loss: 0.0027573818538803607\n",
      "  batch [200/330] loss: 0.003893324631266296\n",
      "  batch [250/330] loss: 0.003834719629958272\n",
      "  batch [300/330] loss: 0.004176603294909\n",
      "=> epoch 37, training loss: 0.004176603294909, training accuracy: 88.70952606201172\n",
      "  batch [50/330] loss: 0.005210862895473838\n",
      "  batch [100/330] loss: 0.0026464329631999137\n",
      "  batch [150/330] loss: 0.001780247096437961\n",
      "  batch [200/330] loss: 0.0020522647676989437\n",
      "  batch [250/330] loss: 0.002392956501338631\n",
      "  batch [300/330] loss: 0.002438034411869012\n",
      "=> epoch 38, training loss: 0.002438034411869012, training accuracy: 97.18095397949219\n",
      "  batch [50/330] loss: 0.0025076040237327108\n",
      "  batch [100/330] loss: 0.0016974081587977708\n",
      "  batch [150/330] loss: 0.0014640179226407782\n",
      "  batch [200/330] loss: 0.0014692590312333777\n",
      "  batch [250/330] loss: 0.002048501542885788\n",
      "  batch [300/330] loss: 0.0024860223605064676\n",
      "=> epoch 39, training loss: 0.0024860223605064676, training accuracy: 97.8952407836914\n",
      "  batch [50/330] loss: 0.008159696540795267\n",
      "  batch [100/330] loss: 0.004064383201301098\n",
      "  batch [150/330] loss: 0.003069250841625035\n",
      "  batch [200/330] loss: 0.0022912798966281114\n",
      "  batch [250/330] loss: 0.0026245924392715096\n",
      "  batch [300/330] loss: 0.0032316863844171167\n",
      "=> epoch 40, training loss: 0.0032316863844171167, training accuracy: 97.63333129882812\n",
      "  batch [50/330] loss: 0.001780652389745228\n",
      "  batch [100/330] loss: 0.0010631203112425284\n",
      "  batch [150/330] loss: 0.001829521250561811\n",
      "  batch [200/330] loss: 0.0022699299613013865\n",
      "  batch [250/330] loss: 0.0027810078526381405\n",
      "  batch [300/330] loss: 0.0027232181578874587\n",
      "=> epoch 41, training loss: 0.0027232181578874587, training accuracy: 98.9000015258789\n",
      "  batch [50/330] loss: 0.0015342740959022195\n",
      "  batch [100/330] loss: 0.0014949278079438954\n",
      "  batch [150/330] loss: 0.0016803474050248043\n",
      "  batch [200/330] loss: 0.0031003452339209616\n",
      "  batch [250/330] loss: 0.003135752192698419\n",
      "  batch [300/330] loss: 0.0021710391973610967\n",
      "=> epoch 42, training loss: 0.0021710391973610967, training accuracy: 98.26190185546875\n",
      "  batch [50/330] loss: 0.002399658486363478\n",
      "  batch [100/330] loss: 0.0019071154303383082\n",
      "  batch [150/330] loss: 0.003000176284229383\n",
      "  batch [200/330] loss: 0.003231816722545773\n",
      "  batch [250/330] loss: 0.0031863515274599195\n",
      "  batch [300/330] loss: 0.002830772242275998\n",
      "=> epoch 43, training loss: 0.002830772242275998, training accuracy: 98.69999694824219\n",
      "  batch [50/330] loss: 0.0021224486569408328\n",
      "  batch [100/330] loss: 0.001397406010190025\n",
      "  batch [150/330] loss: 0.0016584411058574915\n",
      "  batch [200/330] loss: 0.0013313912641024217\n",
      "  batch [250/330] loss: 0.002168934141052887\n",
      "  batch [300/330] loss: 0.0022542253094725313\n",
      "=> epoch 44, training loss: 0.0022542253094725313, training accuracy: 96.0952377319336\n",
      "  batch [50/330] loss: 0.00817171373963356\n",
      "  batch [100/330] loss: 0.005689126715064049\n",
      "  batch [150/330] loss: 0.004197895322926343\n",
      "  batch [200/330] loss: 0.0035142947239801287\n",
      "  batch [250/330] loss: 0.0024207565225660803\n",
      "  batch [300/330] loss: 0.0019436845349846408\n",
      "=> epoch 45, training loss: 0.0019436845349846408, training accuracy: 99.02381134033203\n",
      "  batch [50/330] loss: 0.004224956031423062\n",
      "  batch [100/330] loss: 0.004358329738490284\n",
      "  batch [150/330] loss: 0.0032074384950101374\n",
      "  batch [200/330] loss: 0.0023069155712146312\n",
      "  batch [250/330] loss: 0.002174739340785891\n",
      "  batch [300/330] loss: 0.002096493787597865\n",
      "=> epoch 46, training loss: 0.002096493787597865, training accuracy: 98.61904907226562\n",
      "  batch [50/330] loss: 0.003515187081415206\n",
      "  batch [100/330] loss: 0.0023634870022069664\n",
      "  batch [150/330] loss: 0.0018725011525675655\n",
      "  batch [200/330] loss: 0.0021694674679310993\n",
      "  batch [250/330] loss: 0.0020594324889825657\n",
      "  batch [300/330] loss: 0.0015234012540895493\n",
      "=> epoch 47, training loss: 0.0015234012540895493, training accuracy: 98.55238342285156\n",
      "  batch [50/330] loss: 0.004884824972134084\n",
      "  batch [100/330] loss: 0.0034685931843705473\n",
      "  batch [150/330] loss: 0.0017208130325889214\n",
      "  batch [200/330] loss: 0.002599804265424609\n",
      "  batch [250/330] loss: 0.007084481002762913\n",
      "  batch [300/330] loss: 0.007459332421422005\n",
      "=> epoch 48, training loss: 0.007459332421422005, training accuracy: 96.9047622680664\n",
      "  batch [50/330] loss: 0.005029535876587033\n",
      "  batch [100/330] loss: 0.002070495740976185\n",
      "  batch [150/330] loss: 0.001868723139166832\n",
      "  batch [200/330] loss: 0.0013550588546786457\n",
      "  batch [250/330] loss: 0.0013474612577701918\n",
      "  batch [300/330] loss: 0.0020350448851240797\n",
      "=> epoch 49, training loss: 0.0020350448851240797, training accuracy: 98.22856903076172\n",
      "  batch [50/330] loss: 0.0025380930779501797\n",
      "  batch [100/330] loss: 0.0016535458972211927\n",
      "  batch [150/330] loss: 0.0017960704010911285\n",
      "  batch [200/330] loss: 0.0015869650347158312\n",
      "  batch [250/330] loss: 0.0014400110074784607\n",
      "  batch [300/330] loss: 0.001115419679088518\n",
      "=> epoch 50, training loss: 0.001115419679088518, training accuracy: 98.20952606201172\n",
      "  batch [50/330] loss: 0.009481573726981878\n",
      "  batch [100/330] loss: 0.003300030724145472\n",
      "  batch [150/330] loss: 0.003017258538864553\n",
      "  batch [200/330] loss: 0.0019463374332990497\n",
      "  batch [250/330] loss: 0.0020708599602803587\n",
      "  batch [300/330] loss: 0.002385911616962403\n",
      "=> epoch 51, training loss: 0.002385911616962403, training accuracy: 97.64286041259766\n",
      "  batch [50/330] loss: 0.013282940881326795\n",
      "  batch [100/330] loss: 0.005986835774034261\n",
      "  batch [150/330] loss: 0.003896306292619556\n",
      "  batch [200/330] loss: 0.0022488636025227607\n",
      "  batch [250/330] loss: 0.0017566905128769577\n",
      "  batch [300/330] loss: 0.0014320025413762777\n",
      "=> epoch 52, training loss: 0.0014320025413762777, training accuracy: 98.24285888671875\n",
      "  batch [50/330] loss: 0.004014007972436957\n",
      "  batch [100/330] loss: 0.001886365389218554\n",
      "  batch [150/330] loss: 0.0020202977678272873\n",
      "  batch [200/330] loss: 0.011932221722090616\n",
      "  batch [250/330] loss: 0.009923344824463129\n",
      "  batch [300/330] loss: 0.0043586853640154005\n",
      "=> epoch 53, training loss: 0.0043586853640154005, training accuracy: 99.25714111328125\n",
      "  batch [50/330] loss: 0.0011662146360613406\n",
      "  batch [100/330] loss: 0.0009390599072212353\n",
      "  batch [150/330] loss: 0.0007216783041367307\n",
      "  batch [200/330] loss: 0.001099569218873512\n",
      "  batch [250/330] loss: 0.0009362112933886238\n",
      "  batch [300/330] loss: 0.000681293690111488\n",
      "=> epoch 54, training loss: 0.000681293690111488, training accuracy: 99.80476379394531\n",
      "  batch [50/330] loss: 0.0003614515707886312\n",
      "  batch [100/330] loss: 0.0004353806613944471\n",
      "  batch [150/330] loss: 0.0005818055578274652\n",
      "  batch [200/330] loss: 0.0005097631733224262\n",
      "  batch [250/330] loss: 0.0007781556755435304\n",
      "  batch [300/330] loss: 0.0012057157280505634\n",
      "=> epoch 55, training loss: 0.0012057157280505634, training accuracy: 99.17619323730469\n",
      "  batch [50/330] loss: 0.0012558656201581472\n",
      "  batch [100/330] loss: 0.001654505893267924\n",
      "  batch [150/330] loss: 0.001061454948503524\n",
      "  batch [200/330] loss: 0.0011550678812200204\n",
      "  batch [250/330] loss: 0.0012989773413573857\n",
      "  batch [300/330] loss: 0.0015158152042422444\n",
      "=> epoch 56, training loss: 0.0015158152042422444, training accuracy: 99.21428680419922\n",
      "  batch [50/330] loss: 0.0019468987950240263\n",
      "  batch [100/330] loss: 0.001248397598043084\n",
      "  batch [150/330] loss: 0.0011964373383671046\n",
      "  batch [200/330] loss: 0.0017574016983853654\n",
      "  batch [250/330] loss: 0.001738380187889561\n",
      "  batch [300/330] loss: 0.001364336423925124\n",
      "=> epoch 57, training loss: 0.001364336423925124, training accuracy: 99.37619018554688\n",
      "  batch [50/330] loss: 0.0007145001656317617\n",
      "  batch [100/330] loss: 0.0009458526100788731\n",
      "  batch [150/330] loss: 0.0014440536290640012\n",
      "  batch [200/330] loss: 0.001798681520158425\n",
      "  batch [250/330] loss: 0.0013265382973477245\n",
      "  batch [300/330] loss: 0.0015477468585595488\n",
      "=> epoch 58, training loss: 0.0015477468585595488, training accuracy: 99.54762268066406\n",
      "  batch [50/330] loss: 0.0017010202602250502\n",
      "  batch [100/330] loss: 0.0018621052247472108\n",
      "  batch [150/330] loss: 0.0017655035740463062\n",
      "  batch [200/330] loss: 0.0016666048200568184\n",
      "  batch [250/330] loss: 0.001500687616528012\n",
      "  batch [300/330] loss: 0.00266254425549414\n",
      "=> epoch 59, training loss: 0.00266254425549414, training accuracy: 97.11904907226562\n",
      "  batch [50/330] loss: 0.008707614608108998\n",
      "  batch [100/330] loss: 0.0037788487384095787\n",
      "  batch [150/330] loss: 0.0024877079750876873\n",
      "  batch [200/330] loss: 0.0014400940951891244\n",
      "  batch [250/330] loss: 0.0011146337273530663\n",
      "  batch [300/330] loss: 0.001485673470655456\n",
      "=> epoch 60, training loss: 0.001485673470655456, training accuracy: 99.19999694824219\n",
      "  batch [50/330] loss: 0.002993270789505914\n",
      "  batch [100/330] loss: 0.0019316122836899013\n",
      "  batch [150/330] loss: 0.0013901038595940918\n",
      "  batch [200/330] loss: 0.0011877762014046312\n",
      "  batch [250/330] loss: 0.0011322584877489134\n",
      "  batch [300/330] loss: 0.0016496671591885387\n",
      "=> epoch 61, training loss: 0.0016496671591885387, training accuracy: 99.15238189697266\n",
      "  batch [50/330] loss: 0.0009059828364406712\n",
      "  batch [100/330] loss: 0.0011136979375150986\n",
      "  batch [150/330] loss: 0.0011926120666903444\n",
      "  batch [200/330] loss: 0.0009142550169490278\n",
      "  batch [250/330] loss: 0.0013520000965800136\n",
      "  batch [300/330] loss: 0.001286052291863598\n",
      "=> epoch 62, training loss: 0.001286052291863598, training accuracy: 98.97142791748047\n",
      "  batch [50/330] loss: 0.006743125919718295\n",
      "  batch [100/330] loss: 0.0074180048797279595\n",
      "  batch [150/330] loss: 0.0031864054407924414\n",
      "  batch [200/330] loss: 0.0026742044957354665\n",
      "  batch [250/330] loss: 0.0013366830018348992\n",
      "  batch [300/330] loss: 0.0015998670966364443\n",
      "=> epoch 63, training loss: 0.0015998670966364443, training accuracy: 99.65714263916016\n",
      "  batch [50/330] loss: 0.0008821608644211665\n",
      "  batch [100/330] loss: 0.0009743389254435896\n",
      "  batch [150/330] loss: 0.0009322330408613197\n",
      "  batch [200/330] loss: 0.0008271526138996705\n",
      "  batch [250/330] loss: 0.0005074956095195375\n",
      "  batch [300/330] loss: 0.0005918498444370925\n",
      "=> epoch 64, training loss: 0.0005918498444370925, training accuracy: 99.93333435058594\n",
      "  batch [50/330] loss: 0.0003697795952757588\n",
      "  batch [100/330] loss: 0.0004026512437849306\n",
      "  batch [150/330] loss: 0.0007047556900215568\n",
      "  batch [200/330] loss: 0.0006080623189627658\n",
      "  batch [250/330] loss: 0.0007190591730322921\n",
      "  batch [300/330] loss: 0.001546727578228456\n",
      "=> epoch 65, training loss: 0.001546727578228456, training accuracy: 97.66190338134766\n",
      "  batch [50/330] loss: 0.0031595442284597084\n",
      "  batch [100/330] loss: 0.0028137128793168814\n",
      "  batch [150/330] loss: 0.001926350940950215\n",
      "  batch [200/330] loss: 0.001822140010073781\n",
      "  batch [250/330] loss: 0.0011206199614098296\n",
      "  batch [300/330] loss: 0.0018017873247154056\n",
      "=> epoch 66, training loss: 0.0018017873247154056, training accuracy: 97.93333435058594\n",
      "  batch [50/330] loss: 0.002160288042854518\n",
      "  batch [100/330] loss: 0.0010997504352708348\n",
      "  batch [150/330] loss: 0.0013993739961297251\n",
      "  batch [200/330] loss: 0.0013592736831633374\n",
      "  batch [250/330] loss: 0.0013663161190925167\n",
      "  batch [300/330] loss: 0.002229471306549385\n",
      "=> epoch 67, training loss: 0.002229471306549385, training accuracy: 99.16666412353516\n",
      "  batch [50/330] loss: 0.0026032689425628634\n",
      "  batch [100/330] loss: 0.0012616578403394669\n",
      "  batch [150/330] loss: 0.002117937953909859\n",
      "  batch [200/330] loss: 0.0053108006431721155\n",
      "  batch [250/330] loss: 0.002770206743385643\n",
      "  batch [300/330] loss: 0.005616678100195714\n",
      "=> epoch 68, training loss: 0.005616678100195714, training accuracy: 96.53333282470703\n",
      "  batch [50/330] loss: 0.0038627976402640343\n",
      "  batch [100/330] loss: 0.0013161147674545645\n",
      "  batch [150/330] loss: 0.001335071542998776\n",
      "  batch [200/330] loss: 0.0008386821238673293\n",
      "  batch [250/330] loss: 0.0009625026754802093\n",
      "  batch [300/330] loss: 0.001545577054959722\n",
      "=> epoch 69, training loss: 0.001545577054959722, training accuracy: 99.18095397949219\n",
      "  batch [50/330] loss: 0.0007675844387849792\n",
      "  batch [100/330] loss: 0.0009197872466174885\n",
      "  batch [150/330] loss: 0.0007645914071472362\n",
      "  batch [200/330] loss: 0.0008091296838829294\n",
      "  batch [250/330] loss: 0.00115747451983043\n",
      "  batch [300/330] loss: 0.0010334563897922634\n",
      "=> epoch 70, training loss: 0.0010334563897922634, training accuracy: 99.28571319580078\n",
      "  batch [50/330] loss: 0.0006252868046285584\n",
      "  batch [100/330] loss: 0.00033326808056153824\n",
      "  batch [150/330] loss: 0.0007412602718104608\n",
      "  batch [200/330] loss: 0.0006068227060895879\n",
      "  batch [250/330] loss: 0.0012175863837182988\n",
      "  batch [300/330] loss: 0.0008918355263449484\n",
      "=> epoch 71, training loss: 0.0008918355263449484, training accuracy: 99.06666564941406\n",
      "  batch [50/330] loss: 0.004864887590520084\n",
      "  batch [100/330] loss: 0.0021796593475155532\n",
      "  batch [150/330] loss: 0.0019541293112561105\n",
      "  batch [200/330] loss: 0.0012369577911449596\n",
      "  batch [250/330] loss: 0.001243347575655207\n",
      "  batch [300/330] loss: 0.0013383436902659014\n",
      "=> epoch 72, training loss: 0.0013383436902659014, training accuracy: 99.4000015258789\n",
      "  batch [50/330] loss: 0.0029710585884749887\n",
      "  batch [100/330] loss: 0.0017757929890649393\n",
      "  batch [150/330] loss: 0.001867117666406557\n",
      "  batch [200/330] loss: 0.001067609837395139\n",
      "  batch [250/330] loss: 0.0012098641432821751\n",
      "  batch [300/330] loss: 0.0010846209303708746\n",
      "=> epoch 73, training loss: 0.0010846209303708746, training accuracy: 99.31904602050781\n",
      "  batch [50/330] loss: 0.0009102993989363313\n",
      "  batch [100/330] loss: 0.0008782968408195302\n",
      "  batch [150/330] loss: 0.000740793934717658\n",
      "  batch [200/330] loss: 0.0006400395961245522\n",
      "  batch [250/330] loss: 0.0006425977494072867\n",
      "  batch [300/330] loss: 0.0008585203496622853\n",
      "=> epoch 74, training loss: 0.0008585203496622853, training accuracy: 98.78095245361328\n",
      "  batch [50/330] loss: 0.001472517447982682\n",
      "  batch [100/330] loss: 0.0014316055952222086\n",
      "  batch [150/330] loss: 0.001225676070112968\n",
      "  batch [200/330] loss: 0.0020253443817491645\n",
      "  batch [250/330] loss: 0.0026901070361491294\n",
      "  batch [300/330] loss: 0.0027094546970911322\n",
      "=> epoch 75, training loss: 0.0027094546970911322, training accuracy: 99.33333587646484\n",
      "  batch [50/330] loss: 0.001193612761911936\n",
      "  batch [100/330] loss: 0.001043388359452365\n",
      "  batch [150/330] loss: 0.0008247765076812356\n",
      "  batch [200/330] loss: 0.0012009858943638392\n",
      "  batch [250/330] loss: 0.0010162824694416486\n",
      "  batch [300/330] loss: 0.0009203393954085186\n",
      "=> epoch 76, training loss: 0.0009203393954085186, training accuracy: 99.56666564941406\n",
      "  batch [50/330] loss: 0.0008282523712841794\n",
      "  batch [100/330] loss: 0.0010159008020418697\n",
      "  batch [150/330] loss: 0.0005571675489190966\n",
      "  batch [200/330] loss: 0.001053836729901377\n",
      "  batch [250/330] loss: 0.001103022764174966\n",
      "  batch [300/330] loss: 0.001489249982056208\n",
      "=> epoch 77, training loss: 0.001489249982056208, training accuracy: 99.74761962890625\n",
      "  batch [50/330] loss: 0.0011676081859041006\n",
      "  batch [100/330] loss: 0.0011067238458781503\n",
      "  batch [150/330] loss: 0.0010486378573114053\n",
      "  batch [200/330] loss: 0.0009682465493388009\n",
      "  batch [250/330] loss: 0.0018083089931751602\n",
      "  batch [300/330] loss: 0.0008579697526874952\n",
      "=> epoch 78, training loss: 0.0008579697526874952, training accuracy: 98.14762115478516\n",
      "  batch [50/330] loss: 0.0009408959557767958\n",
      "  batch [100/330] loss: 0.0012532000144710764\n",
      "  batch [150/330] loss: 0.0016394653966417536\n",
      "  batch [200/330] loss: 0.0015544867244898341\n",
      "  batch [250/330] loss: 0.002341193795495201\n",
      "  batch [300/330] loss: 0.0028204980525188147\n",
      "=> epoch 79, training loss: 0.0028204980525188147, training accuracy: 99.29523468017578\n",
      "  batch [50/330] loss: 0.0009016939852735959\n",
      "  batch [100/330] loss: 0.0007073385971598327\n",
      "  batch [150/330] loss: 0.0007381865865900181\n",
      "  batch [200/330] loss: 0.0011418098153662868\n",
      "  batch [250/330] loss: 0.0009452899415337015\n",
      "  batch [300/330] loss: 0.0009810017383133527\n",
      "=> epoch 80, training loss: 0.0009810017383133527, training accuracy: 99.28571319580078\n",
      "  batch [50/330] loss: 0.001169810019840952\n",
      "  batch [100/330] loss: 0.0007044430321257096\n",
      "  batch [150/330] loss: 0.0006676217588828877\n",
      "  batch [200/330] loss: 0.0007649097237444948\n",
      "  batch [250/330] loss: 0.0012593801697657909\n",
      "  batch [300/330] loss: 0.0013966832245932892\n",
      "=> epoch 81, training loss: 0.0013966832245932892, training accuracy: 97.27143096923828\n",
      "  batch [50/330] loss: 0.006589070523157716\n",
      "  batch [100/330] loss: 0.003226822213502601\n",
      "  batch [150/330] loss: 0.0035773283925373107\n",
      "  batch [200/330] loss: 0.001951928341994062\n",
      "  batch [250/330] loss: 0.001860408281441778\n",
      "  batch [300/330] loss: 0.004827047587838024\n",
      "=> epoch 82, training loss: 0.004827047587838024, training accuracy: 95.86190795898438\n",
      "  batch [50/330] loss: 0.004815606890246272\n",
      "  batch [100/330] loss: 0.0022733555731829255\n",
      "  batch [150/330] loss: 0.002098707067780197\n",
      "  batch [200/330] loss: 0.001469305400038138\n",
      "  batch [250/330] loss: 0.0018742046495899558\n",
      "  batch [300/330] loss: 0.001218035715748556\n",
      "=> epoch 83, training loss: 0.001218035715748556, training accuracy: 99.80000305175781\n",
      "  batch [50/330] loss: 0.000870636148378253\n",
      "  batch [100/330] loss: 0.000576802135357866\n",
      "  batch [150/330] loss: 0.0008553171702078544\n",
      "  batch [200/330] loss: 0.0007101036258973181\n",
      "  batch [250/330] loss: 0.00048589629742491525\n",
      "  batch [300/330] loss: 0.0006274645570956636\n",
      "=> epoch 84, training loss: 0.0006274645570956636, training accuracy: 99.80000305175781\n",
      "  batch [50/330] loss: 0.0037761160280788317\n",
      "  batch [100/330] loss: 0.0011068764328374526\n",
      "  batch [150/330] loss: 0.0012261490120436065\n",
      "  batch [200/330] loss: 0.0012965318255737657\n",
      "  batch [250/330] loss: 0.0012477831557334866\n",
      "  batch [300/330] loss: 0.0011180254660430363\n",
      "=> epoch 85, training loss: 0.0011180254660430363, training accuracy: 99.12857055664062\n",
      "  batch [50/330] loss: 0.003043668414640706\n",
      "  batch [100/330] loss: 0.0017567546747159213\n",
      "  batch [150/330] loss: 0.0008987712058005855\n",
      "  batch [200/330] loss: 0.0006006864994415082\n",
      "  batch [250/330] loss: 0.0006022149133204949\n",
      "  batch [300/330] loss: 0.000760071033728309\n",
      "=> epoch 86, training loss: 0.000760071033728309, training accuracy: 99.84761810302734\n",
      "  batch [50/330] loss: 0.0002738605499325786\n",
      "  batch [100/330] loss: 0.00031466497985093155\n",
      "  batch [150/330] loss: 0.00013893820668454282\n",
      "  batch [200/330] loss: 0.0009858257339074043\n",
      "  batch [250/330] loss: 0.01176741613075137\n",
      "  batch [300/330] loss: 0.005921488264575601\n",
      "=> epoch 87, training loss: 0.005921488264575601, training accuracy: 99.0952377319336\n",
      "  batch [50/330] loss: 0.0050489202523604036\n",
      "  batch [100/330] loss: 0.0025293541974388063\n",
      "  batch [150/330] loss: 0.002202414468396455\n",
      "  batch [200/330] loss: 0.0011735104331746697\n",
      "  batch [250/330] loss: 0.0011886358760530129\n",
      "  batch [300/330] loss: 0.0014311471534892916\n",
      "=> epoch 88, training loss: 0.0014311471534892916, training accuracy: 99.9047622680664\n",
      "  batch [50/330] loss: 0.0004338165868248325\n",
      "  batch [100/330] loss: 0.0005340113732963801\n",
      "  batch [150/330] loss: 0.00023529925849288702\n",
      "  batch [200/330] loss: 0.00011022898546070791\n",
      "  batch [250/330] loss: 0.00012879073061048985\n",
      "  batch [300/330] loss: 0.00040541588394262364\n",
      "=> epoch 89, training loss: 0.00040541588394262364, training accuracy: 99.990478515625\n",
      "  batch [50/330] loss: 0.00013404805873869918\n",
      "  batch [100/330] loss: 0.00014354998756607528\n",
      "  batch [150/330] loss: 0.0002179489595000632\n",
      "  batch [200/330] loss: 0.0005518164663444623\n",
      "  batch [250/330] loss: 0.0008189538904116489\n",
      "  batch [300/330] loss: 0.0004578104800893925\n",
      "=> epoch 90, training loss: 0.0004578104800893925, training accuracy: 99.89047241210938\n",
      "  batch [50/330] loss: 0.0020502597882295958\n",
      "  batch [100/330] loss: 0.0016366137514123693\n",
      "  batch [150/330] loss: 0.0013342960504814983\n",
      "  batch [200/330] loss: 0.0012475829292670825\n",
      "  batch [250/330] loss: 0.000835430096834898\n",
      "  batch [300/330] loss: 0.0008232224179664626\n",
      "=> epoch 91, training loss: 0.0008232224179664626, training accuracy: 99.71904754638672\n",
      "  batch [50/330] loss: 0.00046568702776858116\n",
      "  batch [100/330] loss: 0.0004587686164304614\n",
      "  batch [150/330] loss: 0.0003552099204971455\n",
      "  batch [200/330] loss: 0.00041448500809201504\n",
      "  batch [250/330] loss: 0.0009420929236803203\n",
      "  batch [300/330] loss: 0.0008024190120340791\n",
      "=> epoch 92, training loss: 0.0008024190120340791, training accuracy: 99.63809204101562\n",
      "  batch [50/330] loss: 0.003753795584430918\n",
      "  batch [100/330] loss: 0.0018053173472872004\n",
      "  batch [150/330] loss: 0.0016849670362425967\n",
      "  batch [200/330] loss: 0.0014223419686313719\n",
      "  batch [250/330] loss: 0.000954925203230232\n",
      "  batch [300/330] loss: 0.001294822003808804\n",
      "=> epoch 93, training loss: 0.001294822003808804, training accuracy: 99.66666412353516\n",
      "  batch [50/330] loss: 0.0016029183660866693\n",
      "  batch [100/330] loss: 0.0014748948737978934\n",
      "  batch [150/330] loss: 0.0008495015782827977\n",
      "  batch [200/330] loss: 0.000643102818983607\n",
      "  batch [250/330] loss: 0.0007623988286359236\n",
      "  batch [300/330] loss: 0.0006183129946002737\n",
      "=> epoch 94, training loss: 0.0006183129946002737, training accuracy: 99.5142822265625\n",
      "  batch [50/330] loss: 0.003197992372326553\n",
      "  batch [100/330] loss: 0.0013153844062471763\n",
      "  batch [150/330] loss: 0.0009352261751191691\n",
      "  batch [200/330] loss: 0.0010115508248563856\n",
      "  batch [250/330] loss: 0.000702400096168276\n",
      "  batch [300/330] loss: 0.0007917690825706813\n",
      "=> epoch 95, training loss: 0.0007917690825706813, training accuracy: 99.56666564941406\n",
      "  batch [50/330] loss: 0.0006130213539581746\n",
      "  batch [100/330] loss: 0.0003270765386841958\n",
      "  batch [150/330] loss: 0.0005906991699594073\n",
      "  batch [200/330] loss: 0.0006368349225376733\n",
      "  batch [250/330] loss: 0.0004954627907063695\n",
      "  batch [300/330] loss: 0.0007717914357053815\n",
      "=> epoch 96, training loss: 0.0007717914357053815, training accuracy: 99.81904602050781\n",
      "  batch [50/330] loss: 0.00039276517734106165\n",
      "  batch [100/330] loss: 0.00045366406909306536\n",
      "  batch [150/330] loss: 0.0002587419411720475\n",
      "  batch [200/330] loss: 0.000451804396128864\n",
      "  batch [250/330] loss: 0.0005426718378075747\n",
      "  batch [300/330] loss: 0.0008930941344078747\n",
      "=> epoch 97, training loss: 0.0008930941344078747, training accuracy: 99.70476531982422\n",
      "  batch [50/330] loss: 0.007274139190092683\n",
      "  batch [100/330] loss: 0.002297937859548256\n",
      "  batch [150/330] loss: 0.006391171363182366\n",
      "  batch [200/330] loss: 0.0027359479567967354\n",
      "  batch [250/330] loss: 0.0014972092140233145\n",
      "  batch [300/330] loss: 0.001728698020800948\n",
      "=> epoch 98, training loss: 0.001728698020800948, training accuracy: 99.81428527832031\n",
      "  batch [50/330] loss: 0.0003576887761882972\n",
      "  batch [100/330] loss: 0.00042334377707447856\n",
      "  batch [150/330] loss: 0.0006926994837122038\n",
      "  batch [200/330] loss: 0.00044448120100423695\n",
      "  batch [250/330] loss: 0.000494783642119728\n",
      "  batch [300/330] loss: 0.0003731769782898482\n",
      "=> epoch 99, training loss: 0.0003731769782898482, training accuracy: 99.68095397949219\n",
      "  batch [50/330] loss: 0.002642061845166609\n",
      "  batch [100/330] loss: 0.0007919619627064094\n",
      "  batch [150/330] loss: 0.0008086924636154435\n",
      "  batch [200/330] loss: 0.0008310998554807156\n",
      "  batch [250/330] loss: 0.0008575540416059085\n",
      "  batch [300/330] loss: 0.000507152108475566\n",
      "=> epoch 100, training loss: 0.000507152108475566, training accuracy: 99.78571319580078\n",
      "  batch [50/330] loss: 0.006465865449281409\n",
      "  batch [100/330] loss: 0.0031908558355644347\n",
      "  batch [150/330] loss: 0.0011673588952980935\n",
      "  batch [200/330] loss: 0.0008320263121277093\n",
      "  batch [250/330] loss: 0.0006478171249618753\n",
      "  batch [300/330] loss: 0.00036213329317979516\n",
      "=> epoch 101, training loss: 0.00036213329317979516, training accuracy: 99.94761657714844\n",
      "  batch [50/330] loss: 0.0002470345489709871\n",
      "  batch [100/330] loss: 0.000244828076189151\n",
      "  batch [150/330] loss: 0.00012757294622133485\n",
      "  batch [200/330] loss: 0.00031061058201885315\n",
      "  batch [250/330] loss: 0.0003206173455255339\n",
      "  batch [300/330] loss: 0.00013890984592580935\n",
      "=> epoch 102, training loss: 0.00013890984592580935, training accuracy: 99.97142791748047\n",
      "  batch [50/330] loss: 0.0003009921239354298\n",
      "  batch [100/330] loss: 0.00014787728265218902\n",
      "  batch [150/330] loss: 0.0002984925592572836\n",
      "  batch [200/330] loss: 0.0008119526652517379\n",
      "  batch [250/330] loss: 0.0009845926398847952\n",
      "  batch [300/330] loss: 0.000954918145405827\n",
      "=> epoch 103, training loss: 0.000954918145405827, training accuracy: 99.05238342285156\n",
      "  batch [50/330] loss: 0.007975089858751745\n",
      "  batch [100/330] loss: 0.00353136295103468\n",
      "  batch [150/330] loss: 0.0018574168661143631\n",
      "  batch [200/330] loss: 0.0014439144738717005\n",
      "  batch [250/330] loss: 0.0012207211265922524\n",
      "  batch [300/330] loss: 0.001654876881861128\n",
      "=> epoch 104, training loss: 0.001654876881861128, training accuracy: 99.80000305175781\n",
      "  batch [50/330] loss: 0.0005818031841772609\n",
      "  batch [100/330] loss: 0.0005559910107986071\n",
      "  batch [150/330] loss: 0.00039661073428578673\n",
      "  batch [200/330] loss: 0.0002889142649946734\n",
      "  batch [250/330] loss: 0.0002527049279888161\n",
      "  batch [300/330] loss: 0.00022508697013836354\n",
      "=> epoch 105, training loss: 0.00022508697013836354, training accuracy: 99.95714569091797\n",
      "  batch [50/330] loss: 0.00024563699030841235\n",
      "  batch [100/330] loss: 0.00021702610312058823\n",
      "  batch [150/330] loss: 0.00021143343809671932\n",
      "  batch [200/330] loss: 7.570212714199443e-05\n",
      "  batch [250/330] loss: 0.0007046913985541323\n",
      "  batch [300/330] loss: 0.0007695032057308709\n",
      "=> epoch 106, training loss: 0.0007695032057308709, training accuracy: 99.72380828857422\n",
      "  batch [50/330] loss: 0.00031291695345862536\n",
      "  batch [100/330] loss: 0.00045973982669238466\n",
      "  batch [150/330] loss: 0.0005760863739560591\n",
      "  batch [200/330] loss: 0.0006042701506521553\n",
      "  batch [250/330] loss: 0.000935273354902165\n",
      "  batch [300/330] loss: 0.0009862134918221272\n",
      "=> epoch 107, training loss: 0.0009862134918221272, training accuracy: 98.21904754638672\n",
      "  batch [50/330] loss: 0.006966651436756365\n",
      "  batch [100/330] loss: 0.002811925797490403\n",
      "  batch [150/330] loss: 0.0021334794922731816\n",
      "  batch [200/330] loss: 0.0011271599375177175\n",
      "  batch [250/330] loss: 0.0011419291514903308\n",
      "  batch [300/330] loss: 0.001261545445478987\n",
      "=> epoch 108, training loss: 0.001261545445478987, training accuracy: 99.80952453613281\n",
      "  batch [50/330] loss: 0.0013340229811146856\n",
      "  batch [100/330] loss: 0.0007781569435901474\n",
      "  batch [150/330] loss: 0.0008259218584280461\n",
      "  batch [200/330] loss: 0.0006528238950413652\n",
      "  batch [250/330] loss: 0.00041420840207138097\n",
      "  batch [300/330] loss: 0.0007234643980045803\n",
      "=> epoch 109, training loss: 0.0007234643980045803, training accuracy: 99.79047393798828\n",
      "  batch [50/330] loss: 0.0014701311978860758\n",
      "  batch [100/330] loss: 0.0005910159819322871\n",
      "  batch [150/330] loss: 0.0005303345303982497\n",
      "  batch [200/330] loss: 0.0017465001873497386\n",
      "  batch [250/330] loss: 0.0014935883813595864\n",
      "  batch [300/330] loss: 0.0011073931207647547\n",
      "=> epoch 110, training loss: 0.0011073931207647547, training accuracy: 99.35237884521484\n",
      "  batch [50/330] loss: 0.02632403766736388\n",
      "  batch [100/330] loss: 0.009875307010486722\n",
      "  batch [150/330] loss: 0.005612274570390582\n",
      "  batch [200/330] loss: 0.003998267884366215\n",
      "  batch [250/330] loss: 0.003213088784366846\n",
      "  batch [300/330] loss: 0.002565135981887579\n",
      "=> epoch 111, training loss: 0.002565135981887579, training accuracy: 99.66666412353516\n",
      "  batch [50/330] loss: 0.002701636300422251\n",
      "  batch [100/330] loss: 0.0021443314899224786\n",
      "  batch [150/330] loss: 0.0016333538730395958\n",
      "  batch [200/330] loss: 0.0008706726444652304\n",
      "  batch [250/330] loss: 0.0007161125945276581\n",
      "  batch [300/330] loss: 0.0014637793654110282\n",
      "=> epoch 112, training loss: 0.0014637793654110282, training accuracy: 99.94285583496094\n",
      "  batch [50/330] loss: 0.0002491546881792601\n",
      "  batch [100/330] loss: 0.0003033795374503825\n",
      "  batch [150/330] loss: 0.0003550299529451877\n",
      "  batch [200/330] loss: 0.0001556635680317413\n",
      "  batch [250/330] loss: 0.0002324561826972058\n",
      "  batch [300/330] loss: 0.00045317981104017236\n",
      "=> epoch 113, training loss: 0.00045317981104017236, training accuracy: 99.9857177734375\n",
      "  batch [50/330] loss: 0.0001976698286671308\n",
      "  batch [100/330] loss: 0.0002135672892909497\n",
      "  batch [150/330] loss: 8.486382186674746e-05\n",
      "  batch [200/330] loss: 0.0004433460144282435\n",
      "  batch [250/330] loss: 0.00043322805899515514\n",
      "  batch [300/330] loss: 0.00010249863000353798\n",
      "=> epoch 114, training loss: 0.00010249863000353798, training accuracy: 99.98094940185547\n",
      "  batch [50/330] loss: 0.00011061422625425621\n",
      "  batch [100/330] loss: 4.0032515564234926e-05\n",
      "  batch [150/330] loss: 5.027309206343489e-05\n",
      "  batch [200/330] loss: 0.00012868980401253794\n",
      "  batch [250/330] loss: 7.803062823586515e-05\n",
      "  batch [300/330] loss: 0.00011443246445560362\n",
      "=> epoch 115, training loss: 0.00011443246445560362, training accuracy: 99.9857177734375\n",
      "  batch [50/330] loss: 8.077546895947307e-05\n",
      "  batch [100/330] loss: 7.22171757806791e-05\n",
      "  batch [150/330] loss: 0.00018055879367602756\n",
      "  batch [200/330] loss: 0.0001568229721087846\n",
      "  batch [250/330] loss: 0.0002129673921954236\n",
      "  batch [300/330] loss: 9.296953821467469e-05\n",
      "=> epoch 116, training loss: 9.296953821467469e-05, training accuracy: 100.0\n",
      "  batch [50/330] loss: 0.0001645642076182412\n",
      "  batch [100/330] loss: 6.973581216152524e-05\n",
      "  batch [150/330] loss: 5.708045705978293e-05\n",
      "  batch [200/330] loss: 0.00010835696854337584\n",
      "  batch [250/330] loss: 0.0001287435195154103\n",
      "  batch [300/330] loss: 0.0002162419129126647\n",
      "=> epoch 117, training loss: 0.0002162419129126647, training accuracy: 99.85713958740234\n",
      "  batch [50/330] loss: 0.002070131636703081\n",
      "  batch [100/330] loss: 0.0018268054135260172\n",
      "  batch [150/330] loss: 0.0011195447803474964\n",
      "  batch [200/330] loss: 0.000914149060336058\n",
      "  batch [250/330] loss: 0.0010290356582554523\n",
      "  batch [300/330] loss: 0.001318322720879223\n",
      "=> epoch 118, training loss: 0.001318322720879223, training accuracy: 98.84761810302734\n",
      "  batch [50/330] loss: 0.004011035418952816\n",
      "  batch [100/330] loss: 0.0014933508191024884\n",
      "  batch [150/330] loss: 0.0011325014849426225\n",
      "  batch [200/330] loss: 0.0010189944439189275\n",
      "  batch [250/330] loss: 0.0008418567729968345\n",
      "  batch [300/330] loss: 0.0008488261077436619\n",
      "=> epoch 119, training loss: 0.0008488261077436619, training accuracy: 99.71428680419922\n",
      "  batch [50/330] loss: 0.0005668954608845524\n",
      "  batch [100/330] loss: 0.0002646273735444993\n",
      "  batch [150/330] loss: 0.0005674990504630841\n",
      "  batch [200/330] loss: 0.00029027017028420234\n",
      "  batch [250/330] loss: 0.0005831932142245932\n",
      "  batch [300/330] loss: 0.0005943978232462541\n",
      "=> epoch 120, training loss: 0.0005943978232462541, training accuracy: 99.82857513427734\n",
      "  batch [50/330] loss: 0.0002865092034844565\n",
      "  batch [100/330] loss: 0.00019573914563807192\n",
      "  batch [150/330] loss: 0.00017114626043621684\n",
      "  batch [200/330] loss: 6.167943612672389e-05\n",
      "  batch [250/330] loss: 7.394234113235143e-05\n",
      "  batch [300/330] loss: 0.00018087515246952536\n",
      "=> epoch 121, training loss: 0.00018087515246952536, training accuracy: 99.95714569091797\n",
      "  batch [50/330] loss: 9.965779744379688e-05\n",
      "  batch [100/330] loss: 0.00017114284170020256\n",
      "  batch [150/330] loss: 0.00030301664519720363\n",
      "  batch [200/330] loss: 5.179007200968044e-05\n",
      "  batch [250/330] loss: 0.00022660923275179813\n",
      "  batch [300/330] loss: 0.0009414370173726638\n",
      "=> epoch 122, training loss: 0.0009414370173726638, training accuracy: 94.53809356689453\n",
      "  batch [50/330] loss: 0.0062250382844358685\n",
      "  batch [100/330] loss: 0.003380429410841316\n",
      "  batch [150/330] loss: 0.0021379654083866625\n",
      "  batch [200/330] loss: 0.0014280821605352685\n",
      "  batch [250/330] loss: 0.0009432309629046358\n",
      "  batch [300/330] loss: 0.0009103096096077934\n",
      "=> epoch 123, training loss: 0.0009103096096077934, training accuracy: 99.46666717529297\n",
      "  batch [50/330] loss: 0.001214377993310336\n",
      "  batch [100/330] loss: 0.0007139246704464312\n",
      "  batch [150/330] loss: 0.000604367299209116\n",
      "  batch [200/330] loss: 0.0004705879702814855\n",
      "  batch [250/330] loss: 0.0005514326562260976\n",
      "  batch [300/330] loss: 0.0002947390479384921\n",
      "=> epoch 124, training loss: 0.0002947390479384921, training accuracy: 99.89047241210938\n",
      "  batch [50/330] loss: 0.00019235213422507514\n",
      "  batch [100/330] loss: 0.00034248692404071333\n",
      "  batch [150/330] loss: 0.00038818980089126855\n",
      "  batch [200/330] loss: 0.0006101945228438126\n",
      "  batch [250/330] loss: 0.00039259084365039597\n",
      "  batch [300/330] loss: 0.0003621878809062764\n",
      "=> epoch 125, training loss: 0.0003621878809062764, training accuracy: 99.87142944335938\n",
      "  batch [50/330] loss: 0.00047918042493256505\n",
      "  batch [100/330] loss: 0.0002688782974291826\n",
      "  batch [150/330] loss: 0.0003127507957706257\n",
      "  batch [200/330] loss: 0.0003011687265789078\n",
      "  batch [250/330] loss: 0.0012196745380570065\n",
      "  batch [300/330] loss: 0.0005897713932208717\n",
      "=> epoch 126, training loss: 0.0005897713932208717, training accuracy: 99.88571166992188\n",
      "  batch [50/330] loss: 0.00021191126445046393\n",
      "  batch [100/330] loss: 0.0004714830642860761\n",
      "  batch [150/330] loss: 0.0006010016910586273\n",
      "  batch [200/330] loss: 0.0009871300574668567\n",
      "  batch [250/330] loss: 0.0015531814954883884\n",
      "  batch [300/330] loss: 0.0013809060860075987\n",
      "=> epoch 127, training loss: 0.0013809060860075987, training accuracy: 99.20476531982422\n",
      "  batch [50/330] loss: 0.0008437544065382098\n",
      "  batch [100/330] loss: 0.001487358975748066\n",
      "  batch [150/330] loss: 0.0009834204861545004\n",
      "  batch [200/330] loss: 0.0005006844694435131\n",
      "  batch [250/330] loss: 0.000804829324624734\n",
      "  batch [300/330] loss: 0.0010280062365345657\n",
      "=> epoch 128, training loss: 0.0010280062365345657, training accuracy: 98.30000305175781\n",
      "  batch [50/330] loss: 0.015258517479989677\n",
      "  batch [100/330] loss: 0.004683951018378139\n",
      "  batch [150/330] loss: 0.0030396654368378224\n",
      "  batch [200/330] loss: 0.0020501379172783345\n",
      "  batch [250/330] loss: 0.0022766293139429763\n",
      "  batch [300/330] loss: 0.0016542054950259627\n",
      "=> epoch 129, training loss: 0.0016542054950259627, training accuracy: 99.9000015258789\n",
      "  batch [50/330] loss: 0.0003152288380661048\n",
      "  batch [100/330] loss: 0.00036391151329735294\n",
      "  batch [150/330] loss: 0.00027040666405810045\n",
      "  batch [200/330] loss: 0.000391547161154449\n",
      "  batch [250/330] loss: 0.00024555684404913334\n",
      "  batch [300/330] loss: 0.00017890207027085125\n",
      "=> epoch 130, training loss: 0.00017890207027085125, training accuracy: 100.0\n",
      "  batch [50/330] loss: 0.00031837291263218503\n",
      "  batch [100/330] loss: 0.00024427704777917823\n",
      "  batch [150/330] loss: 0.000295101813680958\n",
      "  batch [200/330] loss: 8.579301563440822e-05\n",
      "  batch [250/330] loss: 0.0001683332119791885\n",
      "  batch [300/330] loss: 8.205793683009687e-05\n",
      "=> epoch 131, training loss: 8.205793683009687e-05, training accuracy: 99.97618865966797\n",
      "  batch [50/330] loss: 0.00021868182346224786\n",
      "  batch [100/330] loss: 0.00022507667092577322\n",
      "  batch [150/330] loss: 0.0003027038930922572\n",
      "  batch [200/330] loss: 0.00041614335175836457\n",
      "  batch [250/330] loss: 0.00039450531142938414\n",
      "  batch [300/330] loss: 0.00028869881713035285\n",
      "=> epoch 132, training loss: 0.00028869881713035285, training accuracy: 99.94761657714844\n",
      "  batch [50/330] loss: 0.00108561945004476\n",
      "  batch [100/330] loss: 0.0008519787726108916\n",
      "  batch [150/330] loss: 0.000691370424348861\n",
      "  batch [200/330] loss: 0.0007459626851632492\n",
      "  batch [250/330] loss: 0.0009612744953192305\n",
      "  batch [300/330] loss: 0.0005290007227449678\n",
      "=> epoch 133, training loss: 0.0005290007227449678, training accuracy: 99.80952453613281\n",
      "  batch [50/330] loss: 0.0002750420365773607\n",
      "  batch [100/330] loss: 0.00022034662844816922\n",
      "  batch [150/330] loss: 0.0005103385507609346\n",
      "  batch [200/330] loss: 0.0006003969107259763\n",
      "  batch [250/330] loss: 0.0005546855692373356\n",
      "  batch [300/330] loss: 0.0006271346136636567\n",
      "=> epoch 134, training loss: 0.0006271346136636567, training accuracy: 99.69523620605469\n",
      "  batch [50/330] loss: 0.000571718872837664\n",
      "  batch [100/330] loss: 0.0005516327700388501\n",
      "  batch [150/330] loss: 0.0006655796132981777\n",
      "  batch [200/330] loss: 0.0011492583919098251\n",
      "  batch [250/330] loss: 0.001405583524290705\n",
      "  batch [300/330] loss: 0.0007390059565368574\n",
      "=> epoch 135, training loss: 0.0007390059565368574, training accuracy: 99.60952758789062\n",
      "  batch [50/330] loss: 0.0006640335877018514\n",
      "  batch [100/330] loss: 0.0007833297277538804\n",
      "  batch [150/330] loss: 0.0008830625080736354\n",
      "  batch [200/330] loss: 0.00044180602001142686\n",
      "  batch [250/330] loss: 0.0009847438072029036\n",
      "  batch [300/330] loss: 0.0007160149952687789\n",
      "=> epoch 136, training loss: 0.0007160149952687789, training accuracy: 99.4190444946289\n",
      "  batch [50/330] loss: 0.00043871116594527846\n",
      "  batch [100/330] loss: 0.0004428483044321183\n",
      "  batch [150/330] loss: 0.0003605239512980916\n",
      "  batch [200/330] loss: 0.00047725789828109553\n",
      "  batch [250/330] loss: 0.00029495363626119795\n",
      "  batch [300/330] loss: 0.0010365542166255182\n",
      "=> epoch 137, training loss: 0.0010365542166255182, training accuracy: 99.78571319580078\n",
      "  batch [50/330] loss: 0.0004660003775425139\n",
      "  batch [100/330] loss: 0.0003094899854659161\n",
      "  batch [150/330] loss: 0.00043821877367736304\n",
      "  batch [200/330] loss: 0.000582109208957263\n",
      "  batch [250/330] loss: 0.0006067686116221012\n",
      "  batch [300/330] loss: 0.0006911108982458245\n",
      "=> epoch 138, training loss: 0.0006911108982458245, training accuracy: 99.67619323730469\n",
      "  batch [50/330] loss: 0.0004185354014225595\n",
      "  batch [100/330] loss: 0.00013763450329133775\n",
      "  batch [150/330] loss: 0.0006762671192736888\n",
      "  batch [200/330] loss: 0.0008328354150726227\n",
      "  batch [250/330] loss: 0.0003692975871017552\n",
      "  batch [300/330] loss: 0.0006412055076361867\n",
      "=> epoch 139, training loss: 0.0006412055076361867, training accuracy: 99.76190185546875\n",
      "  batch [50/330] loss: 0.0020581752643338406\n",
      "  batch [100/330] loss: 0.0009583645344973774\n",
      "  batch [150/330] loss: 0.0007361764480156125\n",
      "  batch [200/330] loss: 0.0004965234404808143\n",
      "  batch [250/330] loss: 0.0008114127578446642\n",
      "  batch [300/330] loss: 0.000548775831295643\n",
      "=> epoch 140, training loss: 0.000548775831295643, training accuracy: 99.81904602050781\n",
      "  batch [50/330] loss: 0.00046477630178560505\n",
      "  batch [100/330] loss: 0.0003153769836644642\n",
      "  batch [150/330] loss: 0.000308407518259628\n",
      "  batch [200/330] loss: 0.0003449954570241971\n",
      "  batch [250/330] loss: 0.0004853573747095652\n",
      "  batch [300/330] loss: 0.0006898485240526498\n",
      "=> epoch 141, training loss: 0.0006898485240526498, training accuracy: 97.51905059814453\n",
      "  batch [50/330] loss: 0.00592640656628646\n",
      "  batch [100/330] loss: 0.0017365511177340523\n",
      "  batch [150/330] loss: 0.0016294997166842221\n",
      "  batch [200/330] loss: 0.002623942847363651\n",
      "  batch [250/330] loss: 0.004122824450023472\n",
      "  batch [300/330] loss: 0.0012876000972464681\n",
      "=> epoch 142, training loss: 0.0012876000972464681, training accuracy: 99.5809555053711\n",
      "  batch [50/330] loss: 0.0012536751619772986\n",
      "  batch [100/330] loss: 0.0009601290000136941\n",
      "  batch [150/330] loss: 0.0004293104503012728\n",
      "  batch [200/330] loss: 0.00036220591886376496\n",
      "  batch [250/330] loss: 0.00045907757338136437\n",
      "  batch [300/330] loss: 0.0005024106995842885\n",
      "=> epoch 143, training loss: 0.0005024106995842885, training accuracy: 99.96190643310547\n",
      "  batch [50/330] loss: 0.0002714918015517469\n",
      "  batch [100/330] loss: 0.00016753319971030578\n",
      "  batch [150/330] loss: 0.0001684227694713627\n",
      "  batch [200/330] loss: 0.00014576912486518267\n",
      "  batch [250/330] loss: 0.0001169490634565591\n",
      "  batch [300/330] loss: 0.00019954993260034825\n",
      "=> epoch 144, training loss: 0.00019954993260034825, training accuracy: 99.98094940185547\n",
      "  batch [50/330] loss: 0.00015323246105981524\n",
      "  batch [100/330] loss: 0.0001662308292943635\n",
      "  batch [150/330] loss: 0.00011139892826759024\n",
      "  batch [200/330] loss: 0.00022220786546677117\n",
      "  batch [250/330] loss: 0.0001011785119844717\n",
      "  batch [300/330] loss: 0.0001530534091507434\n",
      "=> epoch 145, training loss: 0.0001530534091507434, training accuracy: 99.95714569091797\n",
      "  batch [50/330] loss: 0.0032623663218109868\n",
      "  batch [100/330] loss: 0.001448550439963583\n",
      "  batch [150/330] loss: 0.0014221683717914857\n",
      "  batch [200/330] loss: 0.0010632236681412906\n",
      "  batch [250/330] loss: 0.0007824475436354987\n",
      "  batch [300/330] loss: 0.000748367116379086\n",
      "=> epoch 146, training loss: 0.000748367116379086, training accuracy: 99.63809204101562\n",
      "  batch [50/330] loss: 0.002805602937238291\n",
      "  batch [100/330] loss: 0.0014606240353314205\n",
      "  batch [150/330] loss: 0.001061702572624199\n",
      "  batch [200/330] loss: 0.0009187116235261783\n",
      "  batch [250/330] loss: 0.0006797741962654982\n",
      "  batch [300/330] loss: 0.0003485738764720736\n",
      "=> epoch 147, training loss: 0.0003485738764720736, training accuracy: 99.88571166992188\n",
      "  batch [50/330] loss: 0.00022339831083081663\n",
      "  batch [100/330] loss: 0.00030406631268851925\n",
      "  batch [150/330] loss: 0.000301689335530682\n",
      "  batch [200/330] loss: 0.00016516220175981288\n",
      "  batch [250/330] loss: 0.00010877722671284573\n",
      "  batch [300/330] loss: 0.00022440769067179646\n",
      "=> epoch 148, training loss: 0.00022440769067179646, training accuracy: 99.9952392578125\n",
      "  batch [50/330] loss: 9.692874673783081e-05\n",
      "  batch [100/330] loss: 0.00012446063785318985\n",
      "  batch [150/330] loss: 0.0006630253480470855\n",
      "  batch [200/330] loss: 0.00042676347748783885\n",
      "  batch [250/330] loss: 0.0002692590963051771\n",
      "  batch [300/330] loss: 0.00028548276406218065\n",
      "=> epoch 149, training loss: 0.00028548276406218065, training accuracy: 99.93333435058594\n",
      "  batch [50/330] loss: 0.00025158941479458006\n",
      "  batch [100/330] loss: 0.0007553310154617066\n",
      "  batch [150/330] loss: 0.0009008071491480223\n",
      "  batch [200/330] loss: 0.0007474796146561858\n",
      "  batch [250/330] loss: 0.000684694346273318\n",
      "  batch [300/330] loss: 0.0006135385853849584\n",
      "=> epoch 150, training loss: 0.0006135385853849584, training accuracy: 99.74285888671875\n",
      "  batch [50/330] loss: 0.000828251970859128\n",
      "  batch [100/330] loss: 0.0005689151987753575\n",
      "  batch [150/330] loss: 0.0005441091798275011\n",
      "  batch [200/330] loss: 0.00035304515379539224\n",
      "  batch [250/330] loss: 0.0007477810511263669\n",
      "  batch [300/330] loss: 0.0011287399945504148\n",
      "=> epoch 151, training loss: 0.0011287399945504148, training accuracy: 98.6047592163086\n",
      "  batch [50/330] loss: 0.005310002009151504\n",
      "  batch [100/330] loss: inf\n",
      "  batch [150/330] loss: 0.0015040454387199133\n",
      "  batch [200/330] loss: 0.00412711764650885\n",
      "  batch [250/330] loss: 0.0017852587935049088\n",
      "  batch [300/330] loss: 0.0011347664800705388\n",
      "=> epoch 152, training loss: 0.0011347664800705388, training accuracy: 98.95714569091797\n",
      "  batch [50/330] loss: 0.0009053631597780623\n",
      "  batch [100/330] loss: 0.00040392876451369374\n",
      "  batch [150/330] loss: 0.0005707957900012844\n",
      "  batch [200/330] loss: 0.00035043351080094\n",
      "  batch [250/330] loss: 0.000455528703489108\n",
      "  batch [300/330] loss: 0.0004925513917041826\n",
      "=> epoch 153, training loss: 0.0004925513917041826, training accuracy: 99.990478515625\n",
      "  batch [50/330] loss: 0.00025318000707193276\n",
      "  batch [100/330] loss: 5.969854402792407e-05\n",
      "  batch [150/330] loss: 8.525758179166587e-05\n",
      "  batch [200/330] loss: 0.00015188279779977166\n",
      "  batch [250/330] loss: 0.00012117298783596198\n",
      "  batch [300/330] loss: 8.194412705779541e-05\n",
      "=> epoch 154, training loss: 8.194412705779541e-05, training accuracy: 99.9952392578125\n",
      "  batch [50/330] loss: 8.189566127839499e-05\n",
      "  batch [100/330] loss: 3.720801378949545e-05\n",
      "  batch [150/330] loss: 0.0002007542827486759\n",
      "  batch [200/330] loss: 8.230360813467996e-05\n",
      "  batch [250/330] loss: 3.799221207736991e-05\n",
      "  batch [300/330] loss: 4.4412523155187954e-05\n",
      "=> epoch 155, training loss: 4.4412523155187954e-05, training accuracy: 99.98094940185547\n",
      "  batch [50/330] loss: 1.824590008800442e-05\n",
      "  batch [100/330] loss: 2.1107856859089226e-05\n",
      "  batch [150/330] loss: 9.763418402326351e-05\n",
      "  batch [200/330] loss: 5.0773234324879016e-05\n",
      "  batch [250/330] loss: 4.2592690409946954e-05\n",
      "  batch [300/330] loss: 0.0005340036388151929\n",
      "=> epoch 156, training loss: 0.0005340036388151929, training accuracy: 99.69047546386719\n",
      "  batch [50/330] loss: 0.005967470138013596\n",
      "  batch [100/330] loss: 0.002829689856036566\n",
      "  batch [150/330] loss: 0.0020989414660143664\n",
      "  batch [200/330] loss: 0.0012542550785001366\n",
      "  batch [250/330] loss: 0.0009534546056820545\n",
      "  batch [300/330] loss: 0.001030333313945448\n",
      "=> epoch 157, training loss: 0.001030333313945448, training accuracy: 99.81428527832031\n",
      "  batch [50/330] loss: 0.001103018425143091\n",
      "  batch [100/330] loss: 0.0008367046685307287\n",
      "  batch [150/330] loss: 0.0006575652378087398\n",
      "  batch [200/330] loss: 0.00038271378744684625\n",
      "  batch [250/330] loss: 0.0004017193731924635\n",
      "  batch [300/330] loss: 0.0004018058232686599\n",
      "=> epoch 158, training loss: 0.0004018058232686599, training accuracy: 99.95237731933594\n",
      "  batch [50/330] loss: 7.002196665780503e-05\n",
      "  batch [100/330] loss: 0.00015164473951153923\n",
      "  batch [150/330] loss: 0.0002603880555834621\n",
      "  batch [200/330] loss: 0.000313330589873658\n",
      "  batch [250/330] loss: 0.00016362793078587856\n",
      "  batch [300/330] loss: 0.0002160862225937308\n",
      "=> epoch 159, training loss: 0.0002160862225937308, training accuracy: 99.87619018554688\n",
      "  batch [50/330] loss: 7.563415286131203e-05\n",
      "  batch [100/330] loss: 7.592419732372946e-05\n",
      "  batch [150/330] loss: 5.22715176721249e-05\n",
      "  batch [200/330] loss: 0.00018492605704886956\n",
      "  batch [250/330] loss: 5.7109199231490495e-05\n",
      "  batch [300/330] loss: 0.00010746498682783567\n",
      "=> epoch 160, training loss: 0.00010746498682783567, training accuracy: 99.98094940185547\n",
      "  batch [50/330] loss: 0.0008307627618632978\n",
      "  batch [100/330] loss: 0.001874745136243291\n",
      "  batch [150/330] loss: 0.0012625427034654421\n",
      "  batch [200/330] loss: 0.0025902557946683373\n",
      "  batch [250/330] loss: 0.0024361255781841463\n",
      "  batch [300/330] loss: 0.0014279701861960347\n",
      "=> epoch 161, training loss: 0.0014279701861960347, training accuracy: 99.52381134033203\n",
      "  batch [50/330] loss: 0.00036888068707776256\n",
      "  batch [100/330] loss: 0.0006928628727255273\n",
      "  batch [150/330] loss: 0.0006347631590906531\n",
      "  batch [200/330] loss: 0.0006695833016128745\n",
      "  batch [250/330] loss: 0.0003687274293770315\n",
      "  batch [300/330] loss: 0.0004184195437410381\n",
      "=> epoch 162, training loss: 0.0004184195437410381, training accuracy: 99.89047241210938\n",
      "  batch [50/330] loss: 0.00028467188276408705\n",
      "  batch [100/330] loss: 0.0004096100999813643\n",
      "  batch [150/330] loss: 0.0003018768117763102\n",
      "  batch [200/330] loss: 0.00032673610376514263\n",
      "  batch [250/330] loss: 0.000326122451624542\n",
      "  batch [300/330] loss: 0.00017736487207002937\n",
      "=> epoch 163, training loss: 0.00017736487207002937, training accuracy: 99.63809204101562\n",
      "  batch [50/330] loss: 0.00028506574497077965\n",
      "  batch [100/330] loss: 0.0002501337978501397\n",
      "  batch [150/330] loss: 0.000129231531136611\n",
      "  batch [200/330] loss: 8.936608253134181e-05\n",
      "  batch [250/330] loss: 0.0002545494498917833\n",
      "  batch [300/330] loss: 0.0002964645165920956\n",
      "=> epoch 164, training loss: 0.0002964645165920956, training accuracy: 99.76190185546875\n",
      "  batch [50/330] loss: 0.0015746877063866124\n",
      "  batch [100/330] loss: 0.0012107287736034778\n",
      "  batch [150/330] loss: 0.0009168289364752127\n",
      "  batch [200/330] loss: 0.0006045687145233387\n",
      "  batch [250/330] loss: 0.00094136163208168\n",
      "  batch [300/330] loss: 0.0005061536198700196\n",
      "=> epoch 165, training loss: 0.0005061536198700196, training accuracy: 99.67619323730469\n",
      "  batch [50/330] loss: 0.003135220506461337\n",
      "  batch [100/330] loss: 0.001486943377647549\n",
      "  batch [150/330] loss: 0.001147146330709802\n",
      "  batch [200/330] loss: 0.001417700047197286\n",
      "  batch [250/330] loss: 0.0009665698351454921\n",
      "  batch [300/330] loss: 0.000503697579697473\n",
      "=> epoch 166, training loss: 0.000503697579697473, training accuracy: 99.92857360839844\n",
      "  batch [50/330] loss: 0.00023884804516274017\n",
      "  batch [100/330] loss: 0.00024245159564452478\n",
      "  batch [150/330] loss: 0.00021953554743231506\n",
      "  batch [200/330] loss: 0.00035960484712268224\n",
      "  batch [250/330] loss: 0.00046049599327670877\n",
      "  batch [300/330] loss: 0.0004081391871586675\n",
      "=> epoch 167, training loss: 0.0004081391871586675, training accuracy: 99.86666870117188\n",
      "  batch [50/330] loss: 0.00409422382327466\n",
      "  batch [100/330] loss: 0.004240062705008313\n",
      "  batch [150/330] loss: 0.004730534338857979\n",
      "  batch [200/330] loss: 0.003160161668783985\n",
      "  batch [250/330] loss: 0.0014033437527250499\n",
      "  batch [300/330] loss: 0.001187613130430691\n",
      "=> epoch 168, training loss: 0.001187613130430691, training accuracy: 99.84285736083984\n",
      "  batch [50/330] loss: 0.0003469926965772174\n",
      "  batch [100/330] loss: 0.00033995441656588807\n",
      "  batch [150/330] loss: 0.0007313120170438196\n",
      "  batch [200/330] loss: 0.00029931551987829154\n",
      "  batch [250/330] loss: 0.00025270672017359177\n",
      "  batch [300/330] loss: 0.0006852159770933213\n",
      "=> epoch 169, training loss: 0.0006852159770933213, training accuracy: 99.9190444946289\n",
      "  batch [50/330] loss: 0.000328055971491267\n",
      "  batch [100/330] loss: 0.00013744100844633066\n",
      "  batch [150/330] loss: 0.00022217901694239118\n",
      "  batch [200/330] loss: 0.00014621527550480096\n",
      "  batch [250/330] loss: 0.00022900255236163502\n",
      "  batch [300/330] loss: 0.0008164107017946663\n",
      "=> epoch 170, training loss: 0.0008164107017946663, training accuracy: 99.95237731933594\n",
      "  batch [50/330] loss: 0.00023310108907026005\n",
      "  batch [100/330] loss: 9.190586340264417e-05\n",
      "  batch [150/330] loss: 5.315065545255493e-05\n",
      "  batch [200/330] loss: 0.00021038279900312772\n",
      "  batch [250/330] loss: 0.00012821070793870602\n",
      "  batch [300/330] loss: 0.0002897399305766157\n",
      "=> epoch 171, training loss: 0.0002897399305766157, training accuracy: 99.80476379394531\n",
      "  batch [50/330] loss: 0.004465630825841799\n",
      "  batch [100/330] loss: 0.002591507907549385\n",
      "  batch [150/330] loss: 0.0008319518994539977\n",
      "  batch [200/330] loss: 0.0009518186283530667\n",
      "  batch [250/330] loss: 0.0009290116043848684\n",
      "  batch [300/330] loss: 0.0012582617208827286\n",
      "=> epoch 172, training loss: 0.0012582617208827286, training accuracy: 99.81904602050781\n",
      "  batch [50/330] loss: 0.0002523718136508251\n",
      "  batch [100/330] loss: 0.00031699764155928276\n",
      "  batch [150/330] loss: 0.0002970209071208956\n",
      "  batch [200/330] loss: 0.0002630041134543717\n",
      "  batch [250/330] loss: 0.00010577879918855616\n",
      "  batch [300/330] loss: 0.00011218400527286576\n",
      "=> epoch 173, training loss: 0.00011218400527286576, training accuracy: 99.97618865966797\n",
      "  batch [50/330] loss: 0.00010957438028344768\n",
      "  batch [100/330] loss: 3.701845095929457e-05\n",
      "  batch [150/330] loss: 5.311011532830889e-05\n",
      "  batch [200/330] loss: 3.578313640173292e-05\n",
      "  batch [250/330] loss: 0.00013170805827030562\n",
      "  batch [300/330] loss: 0.00015099782392280758\n",
      "=> epoch 174, training loss: 0.00015099782392280758, training accuracy: 99.990478515625\n",
      "  batch [50/330] loss: 0.0002859104501876573\n",
      "  batch [100/330] loss: 9.816800367116229e-05\n",
      "  batch [150/330] loss: 0.00014311224573066282\n",
      "  batch [200/330] loss: 0.00010115661836607615\n",
      "  batch [250/330] loss: 7.304741458574427e-05\n",
      "  batch [300/330] loss: 0.0005654809272782586\n",
      "=> epoch 175, training loss: 0.0005654809272782586, training accuracy: 99.88095092773438\n",
      "  batch [50/330] loss: 0.0002586888687474129\n",
      "  batch [100/330] loss: 0.00025621540937936516\n",
      "  batch [150/330] loss: 0.0012809475113244844\n",
      "  batch [200/330] loss: 0.0011840576754475478\n",
      "  batch [250/330] loss: 0.00066440861193405\n",
      "  batch [300/330] loss: 0.0008309800878923853\n",
      "=> epoch 176, training loss: 0.0008309800878923853, training accuracy: 99.57142639160156\n",
      "  batch [50/330] loss: 0.0015674527057708474\n",
      "  batch [100/330] loss: 0.000670874089730205\n",
      "  batch [150/330] loss: 0.0008621673633460887\n",
      "  batch [200/330] loss: 0.0012492100431700237\n",
      "  batch [250/330] loss: 0.0004902760327822761\n",
      "  batch [300/330] loss: 0.0004337816627230495\n",
      "=> epoch 177, training loss: 0.0004337816627230495, training accuracy: 99.87142944335938\n",
      "  batch [50/330] loss: 0.0003044981668062974\n",
      "  batch [100/330] loss: 0.00029718178621988047\n",
      "  batch [150/330] loss: 0.0003476794071757467\n",
      "  batch [200/330] loss: 0.0007951781220908742\n",
      "  batch [250/330] loss: 0.0005445043146828539\n",
      "  batch [300/330] loss: 0.0005983321619496564\n",
      "=> epoch 178, training loss: 0.0005983321619496564, training accuracy: 99.74761962890625\n",
      "  batch [50/330] loss: 0.0018190939067280851\n",
      "  batch [100/330] loss: 0.0009092074376239907\n",
      "  batch [150/330] loss: 0.0009717253578492091\n",
      "  batch [200/330] loss: 0.0003669282998162089\n",
      "  batch [250/330] loss: 0.0010701172659028089\n",
      "  batch [300/330] loss: 0.0007013053146365564\n",
      "=> epoch 179, training loss: 0.0007013053146365564, training accuracy: 99.87619018554688\n",
      "  batch [50/330] loss: 0.0002059136919087905\n",
      "  batch [100/330] loss: 0.0005259544826403726\n",
      "  batch [150/330] loss: 0.00038174219967913815\n",
      "  batch [200/330] loss: 0.00030223298037890343\n",
      "  batch [250/330] loss: 0.00038731002174608874\n",
      "  batch [300/330] loss: 0.0013611056541267318\n",
      "=> epoch 180, training loss: 0.0013611056541267318, training accuracy: 99.32857513427734\n",
      "  batch [50/330] loss: 0.0006933844355444307\n",
      "  batch [100/330] loss: 0.00032587268765928455\n",
      "  batch [150/330] loss: 0.00025388098879193424\n",
      "  batch [200/330] loss: 0.00016734323059790769\n",
      "  batch [250/330] loss: 0.00018733788174722576\n",
      "  batch [300/330] loss: 0.00024668716631276764\n",
      "=> epoch 181, training loss: 0.00024668716631276764, training accuracy: 99.92857360839844\n",
      "  batch [50/330] loss: 0.00023055416737042834\n",
      "  batch [100/330] loss: 0.00022011022014157788\n",
      "  batch [150/330] loss: 0.0002341039753955556\n",
      "  batch [200/330] loss: 0.00034086928467331746\n",
      "  batch [250/330] loss: 0.00024248286799775088\n",
      "  batch [300/330] loss: 0.0002701458819283289\n",
      "=> epoch 182, training loss: 0.0002701458819283289, training accuracy: 99.88571166992188\n",
      "  batch [50/330] loss: 0.0001811399049984175\n",
      "  batch [100/330] loss: 0.0003512389018378599\n",
      "  batch [150/330] loss: 0.0005083387281410979\n",
      "  batch [200/330] loss: 0.0004656487366191868\n",
      "  batch [250/330] loss: 0.0002813585962758225\n",
      "  batch [300/330] loss: 0.00023147054670698709\n",
      "=> epoch 183, training loss: 0.00023147054670698709, training accuracy: 99.42857360839844\n",
      "  batch [50/330] loss: 0.0009003925369252101\n",
      "  batch [100/330] loss: 0.0006291981830145232\n",
      "  batch [150/330] loss: 0.0010777155983923876\n",
      "  batch [200/330] loss: 0.00044971099609210797\n",
      "  batch [250/330] loss: 0.0005981446863588644\n",
      "  batch [300/330] loss: 0.000707111051895481\n",
      "=> epoch 184, training loss: 0.000707111051895481, training accuracy: 99.76667022705078\n",
      "  batch [50/330] loss: 0.001825253382907249\n",
      "  batch [100/330] loss: 0.0009376259785494767\n",
      "  batch [150/330] loss: 0.0006661996835755417\n",
      "  batch [200/330] loss: 0.0012178449467319297\n",
      "  batch [250/330] loss: 0.0007516061540372902\n",
      "  batch [300/330] loss: 0.0006463149898627307\n",
      "=> epoch 185, training loss: 0.0006463149898627307, training accuracy: 99.85237884521484\n",
      "  batch [50/330] loss: 0.0002717567684740061\n",
      "  batch [100/330] loss: 0.0003589184614902479\n",
      "  batch [150/330] loss: 0.0002715926286764443\n",
      "  batch [200/330] loss: 0.0003991064490037388\n",
      "  batch [250/330] loss: 0.0003825648736092262\n",
      "  batch [300/330] loss: 0.0003936033456484438\n",
      "=> epoch 186, training loss: 0.0003936033456484438, training accuracy: 99.9095230102539\n",
      "  batch [50/330] loss: 0.00040953723664279094\n",
      "  batch [100/330] loss: 0.0003144372518145246\n",
      "  batch [150/330] loss: 0.00029850362535216847\n",
      "  batch [200/330] loss: 0.00022307589422052842\n",
      "  batch [250/330] loss: 0.0005185034769929189\n",
      "  batch [300/330] loss: 0.0008288848027368658\n",
      "=> epoch 187, training loss: 0.0008288848027368658, training accuracy: 99.76667022705078\n",
      "  batch [50/330] loss: 0.0004531283874130168\n",
      "  batch [100/330] loss: 0.00019976208690059138\n",
      "  batch [150/330] loss: 0.00026763244341418613\n",
      "  batch [200/330] loss: 0.00029283319733622195\n",
      "  batch [250/330] loss: 0.00045790759685587545\n",
      "  batch [300/330] loss: 0.0005195830077045684\n",
      "=> epoch 188, training loss: 0.0005195830077045684, training accuracy: 99.57142639160156\n",
      "  batch [50/330] loss: 0.0009943296015553643\n",
      "  batch [100/330] loss: 0.0004979545345486258\n",
      "  batch [150/330] loss: 0.0005881102548883064\n",
      "  batch [200/330] loss: 0.0005443893992051016\n",
      "  batch [250/330] loss: 0.0004883050206335611\n",
      "  batch [300/330] loss: 0.0007004070999246324\n",
      "=> epoch 189, training loss: 0.0007004070999246324, training accuracy: 99.77619171142578\n",
      "  batch [50/330] loss: 0.00042640381581441033\n",
      "  batch [100/330] loss: 0.00047116673627169804\n",
      "  batch [150/330] loss: 0.00035290502796124203\n",
      "  batch [200/330] loss: 0.0007238212057563942\n",
      "  batch [250/330] loss: 0.00045654662690503754\n",
      "  batch [300/330] loss: 0.00034819294577027903\n",
      "=> epoch 190, training loss: 0.00034819294577027903, training accuracy: 99.74761962890625\n",
      "  batch [50/330] loss: 0.0002558904784964398\n",
      "  batch [100/330] loss: 0.0004391153438045876\n",
      "  batch [150/330] loss: 0.0006149255995696876\n",
      "  batch [200/330] loss: 0.00029826253881947193\n",
      "  batch [250/330] loss: 0.0012601907605785526\n",
      "  batch [300/330] loss: 0.0016369146962242668\n",
      "=> epoch 191, training loss: 0.0016369146962242668, training accuracy: 99.60952758789062\n",
      "  batch [50/330] loss: 0.002750976490206085\n",
      "  batch [100/330] loss: 0.0028507290744455532\n",
      "  batch [150/330] loss: 0.0019470760494004935\n",
      "  batch [200/330] loss: 0.0008573873629211448\n",
      "  batch [250/330] loss: 0.0008013198639528127\n",
      "  batch [300/330] loss: 0.0005483381572375947\n",
      "=> epoch 192, training loss: 0.0005483381572375947, training accuracy: 99.87619018554688\n",
      "  batch [50/330] loss: 0.0003208473564736778\n",
      "  batch [100/330] loss: 0.00016701286324314425\n",
      "  batch [150/330] loss: 0.00014005495043238625\n",
      "  batch [200/330] loss: 0.00020234999033709756\n",
      "  batch [250/330] loss: 0.0001174793260433944\n",
      "  batch [300/330] loss: 0.00030138458418878147\n",
      "=> epoch 193, training loss: 0.00030138458418878147, training accuracy: 99.93333435058594\n",
      "  batch [50/330] loss: 0.004720309993936098\n",
      "  batch [100/330] loss: 0.0014973931568092666\n",
      "  batch [150/330] loss: 0.0009426657469011843\n",
      "  batch [200/330] loss: 0.0008645653688872699\n",
      "  batch [250/330] loss: 0.0016096725320385303\n",
      "  batch [300/330] loss: 0.0005164128369360697\n",
      "=> epoch 194, training loss: 0.0005164128369360697, training accuracy: 99.92857360839844\n",
      "  batch [50/330] loss: 0.00047060542366671144\n",
      "  batch [100/330] loss: 0.000247086719609797\n",
      "  batch [150/330] loss: 0.00010995737675693818\n",
      "  batch [200/330] loss: 0.00012560944063807255\n",
      "  batch [250/330] loss: 8.987625055669923e-05\n",
      "  batch [300/330] loss: 0.00016232126189061092\n",
      "=> epoch 195, training loss: 0.00016232126189061092, training accuracy: 99.990478515625\n",
      "  batch [50/330] loss: 0.00017912345529839512\n",
      "  batch [100/330] loss: 5.465548890424543e-05\n",
      "  batch [150/330] loss: 5.230626054435561e-05\n",
      "  batch [200/330] loss: 4.17977538490959e-05\n",
      "  batch [250/330] loss: 0.00010239248484958807\n",
      "  batch [300/330] loss: 5.954713086430274e-05\n",
      "=> epoch 196, training loss: 5.954713086430274e-05, training accuracy: 90.33333587646484\n",
      "  batch [50/330] loss: 0.0016213571334665175\n",
      "  batch [100/330] loss: 0.0008504353447642643\n",
      "  batch [150/330] loss: 0.0007180323813809081\n",
      "  batch [200/330] loss: 0.001960432696690987\n",
      "  batch [250/330] loss: 0.0009963145777146564\n",
      "  batch [300/330] loss: 0.0005161025737179443\n",
      "=> epoch 197, training loss: 0.0005161025737179443, training accuracy: 99.79523468017578\n",
      "  batch [50/330] loss: 0.00030820002914697396\n",
      "  batch [100/330] loss: 0.00018214099821489072\n",
      "  batch [150/330] loss: 0.0002481370595451153\n",
      "  batch [200/330] loss: 0.00014402455419985926\n",
      "  batch [250/330] loss: 0.0002031708814101876\n",
      "  batch [300/330] loss: 0.0002543613633388304\n",
      "=> epoch 198, training loss: 0.0002543613633388304, training accuracy: 99.9047622680664\n",
      "  batch [50/330] loss: 0.0005767176717899929\n",
      "  batch [100/330] loss: 0.0002185975581778621\n",
      "  batch [150/330] loss: 7.490236609373823e-05\n",
      "  batch [200/330] loss: 3.6974049035052306e-05\n",
      "  batch [250/330] loss: 5.230077581290971e-05\n",
      "  batch [300/330] loss: 0.0001750839718606585\n",
      "=> epoch 199, training loss: 0.0001750839718606585, training accuracy: 99.5999984741211\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training\")\n",
    "for epoch in range(EPOCH + 1):\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"=> epoch {epoch}, training loss: {train_loss}, training accuracy: {get_acc(model, train_dataset):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a714b157-c479-4095-881c-68ab969a83d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_resnet_cifar10_200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc479f-852e-43e1-8f86-06ac5988e49a",
   "metadata": {},
   "source": [
    "For the sake of comparability, let's examine the accuracy that target and shadow model obtain on a test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "70fcb455-39ee-4961-83e6-cd6d6be0fbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target model accuracy after 200 epoch: 67.57 %\n",
      "Shadow model accuracy after 200 epoch: 69.69 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Target model accuracy after {EPOCH + 1} epoch: {get_acc(target_model, test_dataset):.2f} %\")\n",
    "print(f\"Shadow model accuracy after {EPOCH + 1} epoch: {get_acc(model, test_dataset):.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f5604-54a3-49d9-9616-15c1eb57967e",
   "metadata": {},
   "source": [
    "As we can see, the model performance is very similar, making our trained model a good proxy for the attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b50f28-33c6-4f1d-ab9f-a12ea499a6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amlm",
   "language": "python",
   "name": "amlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
